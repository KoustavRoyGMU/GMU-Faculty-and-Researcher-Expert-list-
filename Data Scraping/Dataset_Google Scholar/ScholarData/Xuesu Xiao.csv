titles,authors,date,source,descriptions,citations
Uav assisted usv visual navigation for marine mass casualty incident response,"Xuesu Xiao, Jan Dufek, Tim Woodbury, Robin Murphy",2017/9/24,Conference 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"This research teams an Unmanned Surface Vehicle (USV) with an Unmanned Aerial Vehicle (UAV) to augment and automate marine mass casualty incident search and rescue in emergency response phase. The demand for real-time responsiveness of those missions requires fast and comprehensive situational awareness and precise operations, which are challenging to achieve because of the large area and the flat nature of the water field. The responders, drowning victims, and rescue vehicle are far apart and all located at the sea level. The long distances mean responders cannot clearly discern the rescue vehicle and victims from the surrounding water. Furthermore, being at the same elevation makes depth perception difficult. Rescue vehicle and victims at different distances from the responder will always appear to be close together. This makes it almost impossible for the responders to accurately drive the …",82
Motion planning and control for mobile robot navigation using machine learning: a survey,"Xuesu Xiao, Bo Liu, Garrett Warnell, Peter Stone",2022/6,Journal Autonomous Robots,"Moving in complex environments is an essential capability of intelligent mobile robots. Decades of research and engineering have been dedicated to developing sophisticated navigation systems to move mobile robots from one point to another. Despite their overall success, a recently emerging research thrust is devoted to developing machine learning techniques to address the same problem, based in large part on the success of deep learning. However, to date, there has not been much direct comparison between the classical and emerging paradigms to this problem. In this article, we survey recent works that apply machine learning for motion planning and control in mobile robot navigation, within the context of classical navigation systems. The surveyed works are classified into different categories, which delineate the relationship of the learning approaches to classical methods. Based on this classification, we …",54
A lifelong learning approach to mobile robot navigation,"Bo Liu, Xuesu Xiao, Peter Stone",2021/2/2,Journal IEEE Robotics and Automation Letters,"This letter presents a self-improving lifelong learning framework for a mobile robot navigating in different environments. Classical static navigation methods require environment-specific in-situ system adjustment, e.g., from human experts, or may repeat their mistakes regardless of how many times they have navigated in the same environment. Having the potential to improve with experience, learning-based navigation is highly dependent on access to training resources, e.g., sufficient memory and fast computation, and is prone to forgetting previously learned capability, especially when facing different environments. In this work, we propose Lifelong Learning for Navigation (LLfN) which (1) improves a mobile robot's navigation behavior purely based on its own experience, and (2) retains the robot's capability to navigate in previous environments after learning in new ones. LLfN is implemented and tested entirely …",47
Appld: Adaptive planner parameter learning from demonstration,"Xuesu Xiao, Bo Liu, Garrett Warnell, Jonathan Fink, Peter Stone",2020/6/15,Journal IEEE Robotics and Automation Letters,"Existing autonomous robot navigation systems allow robots to move from one point to another in a collision-free manner. However, when facing new environments, these systems generally require re-tuning by expert roboticists with a good understanding of the inner workings of the navigation system. In contrast, even users who are unversed in the details of robot navigation algorithms can generate desirable navigation behavior in new environments via teleoperation. In this letter, we introduce appld, Adaptive Planner Parameter Learning from Demonstration, that allows existing navigation systems to be successfully applied to new complex environments, given only a human-teleoperated demonstration of desirable navigation. appld is verified on two robots running different navigation systems in different environments. Experimental results show that appld can outperform navigation systems with the default and …",44
Locomotive reduction for snake robots,"Xuesu Xiao, Ellen Cappo, Weikun Zhen, Jin Dai, Ke Sun, Chaohui Gong, Matthew J Travers, Howie Choset",2015/5/26,Conference 2015 IEEE International Conference on Robotics and Automation (ICRA),"Limbless locomotion, evidenced by both biological and robotic snakes, capitalizes on these systems' redundant degrees of freedom to negotiate complicated environments. While the versatility of locomotion methods provided by a snake-like form is of great advantage, the difficulties in both representing the high dimensional workspace configuration and implementing the desired translations and orientations makes difficult further development of autonomous behaviors for snake robots. Based on a previously defined average body frame and set of motion primitives, this work proposes locomotive reduction, a simplifying methodology which reduces the complexity of controlling a redundant snake robot to that of navigating a differential-drive vehicle. We verify this technique by controlling a 16-DOF snake robot using locomotive reduction combined with a visual tracking system. The simplicity resulting from the …",35
Indoor uav localization using a tether,"Xuesu Xiao, Yiming Fan, Jan Dufek, Robin Murphy",2018/8/6,"Conference 2018 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","This paper presents an approach to localize a UAV in indoor environments using only a quasi-taut tether. In indoor GPS-denied environments, UAV localization usually depends on vision-based methods combined with inertial sensing, such as visual odometry or SLAM using 2D/3D cameras or laser range finders. This necessitates either heavy and sophisticated sensor payload mounted onto the UAV platform or computationally expensive algorithms running online. In this work, we investigate another indoor localization possibility for a tethered UAV: using the tether's sensory feedback, which is fed into a catenary-based mechanics model, to localize the UAV in an indoor global frame defined by the tether reel center. Our localization method is tested on a physical robot, Fotokite Pro. Our approach could reduce the error of the state-of-the-art tether-based indoor aerial vehicle localization by 31.12%. Since the UAV is …",34
Benchmarking metric ground navigation,"Daniel Perille, Abigail Truong, Xuesu Xiao, Peter Stone",2020/11/4,"Conference 2020 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","Metric ground navigation addresses the problem of autonomously moving a robot from one point to another in an obstacle-occupied planar environment in a collision-free manner. It is one of the most fundamental capabilities of intelligent mobile robots. This paper presents a standardized testbed with a set of environments and metrics to benchmark difficulty of different scenarios and performance of different systems of metric ground navigation. Current benchmarks focus on individual components of mobile robot navigation, such as perception and state estimation, but the navigation performance as a whole is rarely measured in a systematic and standardized fashion. As a result, navigation systems are usually tested and compared in an ad hoc manner, such as in one or two manually chosen environments. The introduced benchmark provides a general testbed for ground robot navigation in a metric world. The …",33
Learning inverse kinodynamics for accurate high-speed off-road navigation on unstructured terrain,"Xuesu Xiao, Joydeep Biswas, Peter Stone",2021/6/16,Journal IEEE Robotics and Automation Letters,"This letter presents a learning-based approach to consider the effect of unobservable world states in kinodynamic motion planning in order to enable accurate high-speed off-road navigation on unstructured terrain. Existing kinodynamic motion planners either operate in structured and homogeneous environments and thus do not need to explicitly account for terrain-vehicle interaction, or assume a set of discrete terrain classes. However, when operating on unstructured terrain, especially at high speeds, even small variations in the environment will be magnified and cause inaccurate plan execution. In this letter, to capture the complex kinodynamic model and mathematically unknown world state, we learn a kinodynamic planner in a data-driven manner with onboard inertial observations. Our approach is tested on a physical robot in different indoor and outdoor environments, enables fast and accurate off-road …",32
Two case studies and gaps analysis of flood assessment for emergency management with small unmanned aerial systems,"Robin Murphy, Jan Dufek, Traci Sarmiento, Grant Wilde, Xuesu Xiao, Jeff Braun, Lachlan Mullen, Richard Smith, Sam Allred, Justin Adams, Adam Wright, Jess Gingrich",2016/10/23,"Conference 2016 IEEE international symposium on safety, security, and rescue robotics (SSRR)","This paper documents the successful use of small unmanned aerial systems (SUAS) for two floods in Fort Bend County, Texas, and identifies gaps in informatics, manpower, human-robot interaction, and cost-benefit analysis. The case studies focus on how emergency managers can use SUAS for flood assessment including flood mapping and projection of impact, verification of flood inundation models, providing justification for publicly accountable decisions, and public information. The case studies are particularly informative because they were flown at the direction of County experts during two actual flood events and represent 21 flights over four days.",29
Appli: Adaptive planner parameter learning from interventions,"Zizhao Wang, Xuesu Xiao, Bo Liu, Garrett Warnell, Peter Stone",2021/5/30,Conference 2021 IEEE international conference on robotics and automation (ICRA),"While classical autonomous navigation systems can typically move robots from one point to another safely and in a collision-free manner, these systems may fail or produce suboptimal behavior in certain scenarios. The current practice in such scenarios is to manually re-tune the system’s parameters, e.g. max speed, sampling rate, inflation radius, to optimize performance. This practice requires expert knowledge and may jeopardize performance in the originally good scenarios. Meanwhile, it is relatively easy for a human to identify those failure or suboptimal cases and provide a teleoperated intervention to correct the failure or suboptimal behavior. In this work, we seek to learn from those human interventions to improve navigation performance. In particular, we propose Adaptive Planner Parameter Learning from Interventions (APPLI), in which multiple sets of navigation parameters are learned during training and …",24
Visual servoing for teleoperation using a tethered uav,"Xuesu Xiao, Jan Dufek, Robin Murphy",2017/10/11,"Conference 2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)","This paper presents a visual servoing approach for robotic teleoperation using a tethered unmanned aerial vehicle (UAV). When teleoperating a robot, human operator's perception of the remote situation is limited by the robot's onboard camera. This deteriorates situational awareness and poses challenges on operation precision and efficiency. Tele- operated visual assistants are used in practice. For example, in Fukushima Daiichi nuclear disaster decommissioning, a secondary ground robot is used to follow and watch the primary robot. However, this requires two robots and 2-4 operators to perform one task. Furthermore, it introduces more problems, such as extra teamwork demand, miscommunication risk, suboptimal viewpoints. This work proposes to use a tethered UAV to replace the extra ground robot and human operators. In order to visually assist the primary robot autonomously, a visual servoing algorithm …",24
Applr: Adaptive planner parameter learning from reinforcement,"Zifan Xu, Gauraang Dhamankar, Anirudh Nair, Xuesu Xiao, Garrett Warnell, Bo Liu, Zizhao Wang, Peter Stone",2021/5/30,Conference 2021 IEEE international conference on robotics and automation (ICRA),"Classical navigation systems typically operate using a fixed set of hand-picked parameters (e.g. maximum speed, sampling rate, inflation radius, etc.) and require heavy expert re-tuning in order to work in new environments. To mitigate this requirement, it has been proposed to learn parameters for different contexts in a new environment using human demonstrations collected via teleoperation. However, learning from human demonstration limits deployment to the training environment, and limits overall performance to that of a potentially-suboptimal demonstrator. In this paper, we introduce APPLR, Adaptive Planner Parameter Learning from Reinforcement, which allows existing navigation systems to adapt to new scenarios by using a parameter selection scheme discovered via reinforcement learning (RL) in a wide variety of simulation environments. We evaluate APPLR on a robot in both simulated and physical …",22
Toward agile maneuvers in highly constrained spaces: Learning from hallucination,"Xuesu Xiao, Bo Liu, Garrett Warnell, Peter Stone",2021/2/11,Journal IEEE Robotics and Automation Letters,"While classical approaches to autonomous robot navigation currently enable operation in certain environments, they break down in tightly constrained spaces, e.g., where the robot needs to engage in agile maneuvers to squeeze between obstacles. Recent machine learning techniques have the potential to address this shortcoming, but existing approaches require vast amounts of navigation experience for training, during which the robot must operate in close proximity to obstacles and risk collision. In this letter, we propose to side-step this requirement by introducing a new machine learning paradigm for autonomous navigation called learning from hallucination (LfH), which can use training data collected in completely safe environments to compute navigation controllers that result in fast, smooth, and safe navigation in highly constrained environments. Our experimental results show that the proposed LfH system …",22
Appl: Adaptive planner parameter learning,"Xuesu Xiao, Zizhao Wang, Zifan Xu, Bo Liu, Garrett Warnell, Gauraang Dhamankar, Anirudh Nair, Peter Stone",2022/8/1,Journal Robotics and Autonomous Systems,"While current autonomous navigation systems allow robots to successfully drive themselves from one point to another in specific environments, they typically require extensive manual parameter re-tuning by human robotics experts in order to function in new environments. Furthermore, even for just one complex environment, a single set of fine-tuned parameters may not work well in different regions of that environment. These problems prohibit reliable mobile robot deployment by non-expert users. As a remedy, we propose Adaptive Planner Parameter Learning (appl), a machine learning framework that can leverage non-expert human interaction via several modalities – including teleoperated demonstrations, corrective interventions, and evaluative feedback – and also unsupervised reinforcement learning to learn a parameter policy that can dynamically adjust the parameters of classical navigation systems in …",20
Voila: Visual-observation-only imitation learning for autonomous navigation,"Haresh Karnan, Garrett Warnell, Xuesu Xiao, Peter Stone",2022/5/23,Conference 2022 International Conference on Robotics and Automation (ICRA),"While imitation learning for vision-based au-tonomous mobile robot navigation has recently received a great deal of attention in the research community, existing approaches typically require state-action demonstrations that were gathered using the deployment platform. However, what if one cannot easily outfit their platform to record these demonstration signals or-worse yet-the demonstrator does not have access to the platform at all? Is imitation learning for vision-based autonomous navigation even possible in such scenarios? In this work, we hypothesize that the answer is yes and that recent ideas from the Imitation from Observation (IfO) literature can be brought to bear such that a robot can learn to navigate using only ego-centric video collected by a demonstrator, even in the presence of viewpoint mismatch. To this end, we introduce a new algorithm, Visual-Observation-only Imitation Learning for Autonomous …",19
Prevention and resolution of conflicts in social navigation–a survey,"Reuth Mirsky, Xuesu Xiao, Justin Hart, Peter Stone",2021/6,Journal arXiv preprint arXiv:2106.12113,"With the approaching goal of having robots collaborate in shared human-robot environments, navigation in this context becomes both crucial and desirable. Recent developments in robotics have encountered and tackled some of the challenges of navigating in mixed humanrobot environments, and in recent years we observe a surge of related work that specifically targets the question of how to handle conflicts between agents in social navigation. These contributions offer models, algorithms, and evaluation metrics, however as this research area is inherently interdisciplinary, many of the relevant papers are not comparable and there is no standard vocabulary between the researchers. The main goal of this survey is to bridge this gap by proposing such a common language, using it to survey existing work, and highlighting open problems. It starts by defining a conflict in social navigation, and offers a detailed taxonomy of its components. This survey then maps existing work while discussing papers using the framing of the proposed taxonomy. Finally, this paper propose some future directions and problems that are currently in the frontier of social navigation to help focus research efforts.",19
Agile robot navigation through hallucinated learning and sober deployment,"Xuesu Xiao, Bo Liu, Peter Stone",2021/5/30,Conference 2021 IEEE international conference on robotics and automation (ICRA),"Learning from Hallucination (LfH) is a recent machine learning paradigm for autonomous navigation, which uses training data collected in completely safe environments and adds numerous imaginary obstacles to make the environment densely constrained, to learn navigation planners that produce feasible navigation even in highly constrained (more dangerous) spaces. However, LfH requires hallucinating the robot perception during deployment to match with the hallucinated training data, which creates a need for sometimes-infeasible prior knowledge and tends to generate very conservative planning. In this work, we propose a new LfH paradigm that does not require runtime hallucination—a feature we call ""sober deployment""—and can therefore adapt to more realistic navigation scenarios. This novel Hallucinated Learning and Sober Deployment (HLSD) paradigm is tested in a benchmark testbed of 300 …",19
Motion planning for a uav with a straight or kinked tether,"Xuesu Xiao, Jan Dufek, Mohamed Suhail, Robin Murphy",2018/10/1,Conference 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"This paper develops and compares two motion planning algorithms for a tethered UAV with and without the possibility of the tether contacting the confined and cluttered environment. Tethered aerial vehicles have been studied due to their advantages such as power duration, stability, and safety. However, the disadvantages brought in by the extra tether have not been well investigated by the robotic locomotion community, especially when the tethered agent is locomoting in a non-free space occupied with obstacles. In this work, we propose two motion planning frameworks that (1) reduce the reachable configuration space by taking into account the tether and (2) deliberately plan (and relax) the contact point(s) of the tether with the environment and enable an equivalent reachable configuration space as the non-tethered counterpart would have. Both methods are tested on a physical robot, Fotokite Pro. With our …",18
Machine learning for placement-insensitive inertial motion capture,"Xuesu Xiao, Shuayb Zarar",2018/5/21,Conference 2018 IEEE International Conference on Robotics and Automation (ICRA),"Although existing inertial motion-capture systems work reasonably well (≤10° error in Euler angles), their accuracy suffers when sensor positions change relative to the associated body segments (±60° mean error and 120° standard deviation). We attribute this performance degradation to undermined calibration values, sensor movement latency and displacement offsets. The latter specifically leads to incongruent rotation matrices in kinematic algorithms that rely on rotational transformations. To overcome these limitations, we propose to employ machine-learning techniques. In particular, we use multi-layer perceptrons to learn sensor-displacement patterns based on 3 hours of motion data collected from 12 test subjects in the lab over 215 trials. Furthermore, to compensate for calibration and latency errors, we directly process sensor data with deep neural networks and estimate the joint angles. Based on these …",18
Apple: Adaptive planner parameter learning from evaluative feedback,"Zizhao Wang, Xuesu Xiao, Garrett Warnell, Peter Stone",2021/7/30,Journal IEEE Robotics and Automation Letters,"Classical autonomous navigation systems can control robots in a collision-free manner, oftentimes with verifiable safety and explainability. When facing new environments, however, fine-tuning of the system parameters by an expert is typically required before the system can navigate as expected. To alleviate this requirement, the recently-proposed Adaptive Planner Parameter Learning paradigm allows robots to learn how to dynamically adjust planner parameters using a teleoperated demonstration or corrective interventions from non-expert users. However, these interaction modalities require users to take full control of the moving robot, which requires the users to be familiar with robot teleoperation. As an alternative, we introduce apple, Adaptive Planner Parameter Learning from Evaluative Feedback (real-time, scalar-valued assessments of behavior), which represents a less-demanding modality of interaction …",17
Socially compliant navigation dataset (scand): A large-scale dataset of demonstrations for social navigation,"Haresh Karnan, Anirudh Nair, Xuesu Xiao, Garrett Warnell, Sören Pirk, Alexander Toshev, Justin Hart, Joydeep Biswas, Peter Stone",2022/6/17,Journal IEEE Robotics and Automation Letters,"Social navigation is the capability of an autonomous agent, such as a robot, to navigate in a “socially compliant” manner in the presence of other intelligent agents such as humans. With the emergence of autonomously navigating mobile robots in human-populated environments (e.g., domestic service robots in homes and restaurants and food delivery robots on public sidewalks), incorporating socially compliant navigation behaviors on these robots becomes critical to ensuring safe and comfortable human-robot coexistence. To address this challenge, imitation learning is a promising framework, since it is easier for humans to demonstrate the task of social navigation rather than to formulate reward functions that accurately capture the complex multi-objective setting of social navigation. The use of imitation learning and inverse reinforcement learning to social navigation for mobile robots, however, is currently …",16
A unified framework for operational range estimation of mobile robots operating on a single discharge to avoid complete immobilization,"Kshitij Tiwari, Xuesu Xiao, Ashish Malik, Nak Young Chong",2019/2/1,Journal Mechatronics,"Mobile robots are being increasingly deployed in fields where human intervention is deemed risky. However, in doing so, one of the prime concern is to prevent complete battery depletion which may in turn lead to immobilization of the robot during the mission. Thus, we need to carefully manage the energy available to explore as much of the unknown environment as feasible whilst guaranteeing a safe return journey to home base. For this, we need to identify the key components that draw energy and quantify their individual energy requirements. However, this problem is difficult due to the fact that most of the robots have different motion models, and the energy consumption usually also varies from mission to mission. It is desirable to have a generic framework that takes into account different locomotion models and possible mission profiles. This paper presents a methodology to unify the energy consumption …",16
A review on snake robot testbeds in granular and restricted maneuverability spaces,"Xuesu Xiao, Robin Murphy",2018/12/1,Source Robotics and Autonomous Systems,"This article reviews the state of the art in evaluating snake robots for small spaces such as a collapsed building where the snake is either locomoting in restricted maneuverability spaces, such as narrow pipes or tunnels, or pushing through granular regions, such as dirt and rubble. It makes recommendations on designing a testbed that can enable a comprehensive evaluation of a snake robot’s overall capability and an objective comparison of different snakes. A survey of 31 papers reveals that 20 testbeds were used to test snake robots in restricted maneuverability environments. All of those were built specifically to test a particular snake robot rather than for comparison with other snake robots, but each offers insights into designing comprehensive, comparative testbeds. The article analyzed these 20 testbeds in terms of how well they addressed the previously established disaster robotics metrics of scale (a …",16
Autonomous visual assistance for robot operations using a tethered uav,"Xuesu Xiao, Jan Dufek, Robin R Murphy",2021,Conference Field and Service Robotics: Results of the 12th International Conference,"This paper develops an autonomous tethered aerial visual assistant for robot operations in unstructured or confined environments. Robotic tele-operation in remote environments is difficult due to the lack of sufficient situational awareness, mostly caused by stationary and limited field-of-view and lack of depth perception from the robot’s onboard camera. The emerging state of the practice is to use two robots, a primary and a secondary that acts as a visual assistant to overcome the perceptual limitations of the onboard sensors by providing an external viewpoint. However, problems exist when using a tele-operated visual assistant: extra manpower, manually chosen suboptimal viewpoint, and extra teamwork demand between primary and secondary operators. In this work, we use an autonomous tethered aerial visual assistant to replace the secondary robot and operator, reducing the human-robot ratio from 2 …",14
A wearable system for articulated human pose tracking under uncertainty of sensor placement,"Xuesu Xiao, Shuayb Zarar",2018/8/26,Conference 2018 7th IEEE International Conference on Biomedical Robotics and Biomechatronics (Biorob),"To precisely track human motion, today's state-of-the-art employs either well-calibrated sensors tightly strapped to the body or high-speed cameras confined to a finite capture volume. These restrictions make such systems less mobile. In this paper, we aim to break this usability barrier around motion-capture technology through a wearable system that has sensors integrated directly into garments. We develop a pose-estimation approach based on classic kinematics and show that it is insufficient to analyze motion in such a system, leading to mean Euler angle errors of up to ±60° and standard deviations of 120°. Thus, we motivate the need for data-driven algorithms in this domain. Through a quantitative study, we attribute motion-estimation errors to the high-degree of sensor displacement (up to 118° standard deviation from the nominal value) with respect to the body segments that are present when human poses …",14
From agile ground to aerial navigation: Learning from learned hallucination,"Zizhao Wang, Xuesu Xiao, Alexander J Nettekoven, Kadhiravan Umasankar, Anika Singh, Sriram Bommakanti, Ufuk Topcu, Peter Stone",2021/9/27,Conference 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"This paper presents a self-supervised Learning from Learned Hallucination (LfLH) method to learn fast and reactive motion planners for ground and aerial robots to navigate through highly constrained environments. The recent Learning from Hallucination (LfH) paradigm for autonomous navigation executes motion plans by random exploration in completely safe obstacle-free spaces, uses hand-crafted hallucination techniques to add imaginary obstacles to the robot’s perception, and then learns motion planners to navigate in realistic, highly-constrained, dangerous spaces. However, current hand-crafted hallucination techniques need to be tailored for specific robot types (e.g., a differential drive ground vehicle), and use approximations heavily dependent on certain assumptions (e.g., a short planning horizon). In this work, instead of manually designing hallucination functions, LfLH learns to hallucinate obstacle …",13
Using human-inspired signals to disambiguate navigational intentions,"Justin Hart, Reuth Mirsky, Xuesu Xiao, Stone Tejeda, Bonny Mahajan, Jamin Goo, Kathryn Baldauf, Sydney Owen, Peter Stone",2020/11/6,"Book Social Robotics: 12th International Conference, ICSR 2020, Golden, CO, USA, November 14–18, 2020, Proceedings","People are proficient at communicating their intentions in order to avoid conflicts when navigating in narrow, crowded environments. Mobile robots, on the other hand, often lack both the ability to interpret human intentions and the ability to clearly communicate their own intentions to people sharing their space. This work addresses the second of these points, leveraging insights about how people implicitly communicate with each other through gaze to enable mobile robots to more clearly signal their navigational intention. We present a human study measuring the importance of gaze in coordinating people’s navigation. This study is followed by the development of a virtual agent head which is added to a mobile robot platform. Comparing the performance of a robot with a virtual agent head against one with an LED turn signal demonstrates its ability to impact people’s navigational choices, and that people more …",13
Benchmarking tether-based uav motion primitives,"Xuesu Xiao, Jan Dufek, Robin Murphy",2019/9/2,"Conference 2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","This paper proposes and benchmarks two tether-based motion primitives for tethered UAVs to execute autonomous flight with proprioception only. Tethered UAVs have been studied mainly due to power and safety considerations. Tether is either not included in the UAV motion (treated same as free-flying UAV) or only in terms of station-keeping and highspeed steady flight. However, feedback from and control over the tether configuration could be utilized as a set of navigational tools for autonomous flight, especially in GPS-denied environments and without vision-based exteroception. In this work, two tether-based motion primitives are proposed, which can enable autonomous flight of a tethered UAV. The proposed motion primitives are implemented on a physical tethered UAV for autonomous path execution with motion capture ground truth. The navigational performance is quantified and compared. The proposed …",13
Lifelong navigation,"Bo Liu, Xuesu Xiao, Peter Stone",2020,Journal arXiv preprint arXiv:2007.14486,,11
Explicit-risk-aware path planning with reward maximization,"Xuesu Xiao, Jan Dufek, Robin Murphy",2019/3/7,Journal arXiv preprint arXiv:1903.03187,"This paper develops a path planner that minimizes risk (e.g. motion execution) while maximizing accumulated reward (e.g., quality of sensor viewpoint) motivated by visual assistance or tracking scenarios in unstructured or confined environments. In these scenarios, the robot should maintain the best viewpoint as it moves to the goal. However, in unstructured or confined environments, some paths may increase the risk of collision; therefore there is a tradeoff between risk and reward. Conventional state-dependent risk or probabilistic uncertainty modeling do not consider path-level risk or is difficult to acquire. This risk-reward planner explicitly represents risk as a function of motion plans, i.e., paths. Without manual assignment of the negative impact to the planner caused by risk, this planner takes in a pre-established viewpoint quality map and plans target location and path leading to it simultaneously, in order to maximize overall reward along the entire path while minimizing risk. Exact and approximate algorithms are presented, whose solution is further demonstrated on a physical tethered aerial vehicle. Other than the visual assistance problem, the proposed framework also provides a new planning paradigm to address minimum-risk planning under dynamical risk and absence of substructure optimality and to balance the trade-off between reward and risk.",11
Estimating achievable range of ground robots operating on single battery discharge for operational efficacy amelioration,"Kshitij Tiwari, Xuesu Xiao, Nak Young Chong",2018/10/1,Conference 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"Mobile robots are increasingly being used to assist with active pursuit and law enforcement. One major limitation for such missions is the resource (battery) allocated to the robot. Factors like nature and agility of evader, terrain over which pursuit is being carried out, plausible traversal velocity and the amount of necessary data to be collected all influence how long the robot can last in the field and how far it can travel. In this paper, we develop an analytical model that analyzes the energy utilization for a variety of components mounted on a robot to estimate the maximum operational range achievable by the robot operating on a single battery discharge. We categorize the major consumers of energy as: 1.) ancillary robotic functions such as computation, communication, sensing etc., and 2.) maneuvering which involves propulsion, steering etc. Both these consumers draw power from the common power source but the …",11
Visual pose stabilization of tethered small unmanned aerial system to assist drowning victim recovery,"Jan Dufek, Xuesu Xiao, Robin Murphy",2017/10/11,"Conference 2017 IEEE International Symposium on Safety, Security and Rescue Robotics (SSRR)","This paper proposes a method for visual pose stabilization of Fotokite, a tethered small unmanned aerial system, using a forward facing monocular camera. Conventionally, Fotokite stabilizes itself only relative to its tether and not relative to the global frame. It is, therefore, susceptible to environmental disturbances (especially wind) or motion of its ground station. Related work proposed visual stabilization for unmanned aerial systems using a downward facing camera and homography estimation. The major disadvantage of this approach is that all the features used in the homography estimation must be in the same plane. The method proposed in this paper works for features in different planes and can be used with a forward-facing camera. This paper is the part of a bigger project on saving drowning victims using lifesaving unmanned surface vehicle visually servoed by Fotokite to reach the victims. Some of the used …",11
Exploration of planetary skylights and tunnels,"Red Whittaker, Uland Wong, Heather Jones, Steven Huber, Christopher Cunningham, Warren C Whittaker, Steve McGuire, Xuesu Xiao, Rick Shanor, Ander Solorzano, Tom Carlone, Wennie Tabib, Christopher Greve, Lauren Schneider, Nathan Otten",2014/12/8,Issue HQ-E-DAA-TN63106,"While planetary pits and caves have been fiction for a century, they have been seen from orbit only in the last few years. These discoveries exceed the fantasies in diversity, scale, and abundance. For pits and caves, this is the age of discovery, ranging from a few pits on the Moon and Mars in 2009 to hundreds within the time of this research, with many more to come. Pits with subsurface voids have been confirmed on the Moon and Mars and indicated on Venus, Phobos, Eros, Gaspra, Ida, Enceladus, and Europa. Compelling next steps are surface and subsurface exploration.Pits and caves are opportunistic study targets for unique origins, geology, and climate that will broadly impact planetary science. Holes on Mars are of particular interest because their interior caves are relatively protected from the harsh surface, making them good candidates to contain Martian life. Pits are prime targets for possible future spacecraft, robots, and even human interplanetary explorers. Caves and caverns could be ready-_made shelters for future Moon and Mars explorers and colonists. Discoveries to date look down from on high with satellites but cannot reveal the wonders of caves. They cannot enter, touch, or view pits up close. Genuine exploration is only achievable through surface missions. Robotic missions can assess suitability for safe entry and habitation, plus inform techniques for developing subsurface infrastructure.Missions into planetary voids redefine the future of exploration, science, and habitation beyond Earth. We can reach this future only by targeting specific technological advancement now. Prior missions and current roadmap priorities target …",10
Causal dynamics learning for task-independent state abstraction,"Zizhao Wang, Xuesu Xiao, Zifan Xu, Yuke Zhu, Peter Stone",2022/6/27,Journal arXiv preprint arXiv:2206.13452,"Learning dynamics models accurately is an important goal for Model-Based Reinforcement Learning (MBRL), but most MBRL methods learn a dense dynamics model which is vulnerable to spurious correlations and therefore generalizes poorly to unseen states. In this paper, we introduce Causal Dynamics Learning for Task-Independent State Abstraction (CDL), which first learns a theoretically proved causal dynamics model that removes unnecessary dependencies between state variables and the action, thus generalizing well to unseen states. A state abstraction can then be derived from the learned dynamics, which not only improves sample efficiency but also applies to a wider range of tasks than existing state abstraction methods. Evaluated on two simulated environments and downstream tasks, both the dynamics model and policies learned by the proposed method generalize well to unseen states and the derived state abstraction improves sample efficiency compared to learning without it.",9
Robot risk-awareness by formal risk reasoning and planning,"Xuesu Xiao, Jan Dufek, Robin R Murphy",2020/2/17,Journal IEEE Robotics and Automation Letters,"This letter proposes a formal robot motion risk reasoning framework and develops a risk-aware path planner that minimizes the proposed risk. While robots locomoting in unstructured or confined environments face a variety of risk, existing risk only focuses on collision with obstacles. Such risk is currently only addressed in ad hoc manners. Without a formal definition, ill-supported properties, e.g. additive or Markovian, are simply assumed. Relied on an incomplete and inaccurate representation of risk, risk-aware planners use ad hoc risk functions or chance constraints to minimize risk. The former inevitably has low fidelity when modeling risk, while the latter conservatively generates feasible path within a probability bound. Using propositional logic and probability theory, the proposed motion risk reasoning framework is formal. Building uponauniverse of risk elements of interest, three major risk categories, i.e. locale …",9
Vi-ikd: High-speed accurate off-road navigation using learned visual-inertial inverse kinodynamics,"Haresh Karnan, Kavan Singh Sikand, Pranav Atreya, Sadegh Rabiee, Xuesu Xiao, Garrett Warnell, Peter Stone, Joydeep Biswas",2022/10/23,Conference 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"One of the key challenges in high-speed off-road navigation on ground vehicles is that the kinodynamics of the vehicle-terrain interaction can differ dramatically depending on the terrain. Previous approaches to addressing this challenge have considered learning an inverse kinodynamics (IKD) model, conditioned on inertial information of the vehicle to sense the kinodynamic interactions. In this paper, we hypothesize that to enable accurate high-speed off-road navigation using a learned IKD model, in addition to inertial information from the past, one must also anticipate the kinodynamic interactions of the vehicle with the terrain in the future. To this end, we introduce Visual-Inertial Inverse Kinodynamics (VI-IKD), a novel learning based IKD model that is conditioned on visual information from a terrain patch ahead of the robot in addition to past inertial information, enabling it to anticipate kinodynamic interactions in …",8
Machine learning methods for local motion planning: A study of end-to-end vs. parameter learning,"Zifan Xu, Xuesu Xiao, Garrett Warnell, Anirudh Nair, Peter Stone",2021/10/25,"Conference 2021 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","While decades of research efforts have been devoted to developing classical autonomous navigation systems to move robots from one point to another in a collision-free manner, machine learning approaches to navigation have been recently proposed to learn navigation behaviors from data. Two representative paradigms are end-to-end learning (directly from perception to motion) and parameter learning (from perception to parameters used by a classical underlying planner). These two types of methods are believed to have complementary pros and cons: parameter learning is expected to be robust to different scenarios, have provable guarantees, and exhibit explainable behaviors; end-to-end learning does not require extensive engineering and has the potential to outperform approaches that rely on classical systems. However, these beliefs have not been verified through real-world experiments in a …",7
Best viewpoints for external robots or sensors assisting other robots,"Jan Dufek, Xuesu Xiao, Robin R Murphy",2021/7/13,Journal IEEE Transactions on Human-Machine Systems,"This work creates a model of the value of different external viewpoints of a robot performing tasks. The current state of the practice is to use a teleoperated assistant robot to provide a view of a task being performed by a primary robot; however, the choice of viewpoints is ad hoc and does not always lead to improved performance. This research applies a psychomotor approach to develop a model of the relative quality of external viewpoints using Gibsonian affordances. In this approach, viewpoints for the affordances are rated based on the psychomotor behavior of human operators and clustered into manifolds of viewpoints with the equivalent value. The value of 30 viewpoints is quantified in a study with 31 expert robot operators for four affordances (reachability, passability, manipulability, and traversability) using a computer-based simulator of two robots. The adjacent viewpoints with similar values are clustered …",6
Explicit motion risk representation,"Xuesu Xiao, Jan Dufek, Robin Murphy",2019/9/2,"Conference 2019 IEEE International Symposium on Safety, Security, and Rescue Robotics (SSRR)","This paper presents a formal definition and explicit representation of robot motion risk. Currently, robot motion risk has not been formally defined, but has already been used in motion and path planning. Risk is either implicitly represented as model uncertainty using probabilistic approaches, where the definition of risk is somewhat avoided, or explicitly modeled as a simple function of states, without a formal definition. In this work, we provide formal reasoning behind what risk is for robot motion and propose a formal definition of risk in terms of a sequence of motion, namely path. Mathematical approaches to represent motion risk are also presented, which is in accordance with our risk definition and properties. The definition and representation of risk provide a meaningful way to evaluate or construct robot motion or path plans. The understanding of risk is even of greater interest for the search and rescue community …",5
Packet loss concealment with recurrent neural networks for wireless inertial pose tracking,"Xuesu Xiao, Shuayb Zarar",2018/3/4,Conference 2018 IEEE 15th International Conference on Wearable and Implantable Body Sensor Networks (BSN),"Inertial sensing is a technology that enables motion capture outside of well-defined studio environments. Yet, there are several hurdles that have to be overcome in order to achieve a high-quality user experience. Among them is enabling robust wireless communication. Thanks to strict requirements on throughput and far-field operation along with existing issues of occlusion and client interference, packet-loss rates in wireless inertial-sensing systems can amplify pose-tracking errors by as much as 39%. In this paper, we develop a new type of sequence-predictors based on long short-term memory neural networks that can be used to significantly conceal packet losses for inertial pose-tracking. To lower computational overheads, we systematically exploit spatio-temporal correlations of data and distribute sensor loads among multiple predictors. Through experiments conducted with 3.5 hrs. of high-frequency inertial …",5
Energy considerations for wheeled mobile robots operating on a single battery discharge,"X Xiao, WR Whittaker",2014/8,"Publisher Technical Report CMURI-TR-14-16, Carnegie Mellon University Robotics Institute",,5
High-speed accurate robot control using learned forward kinodynamics and non-linear least squares optimization,"Pranav Atreya, Haresh Karnan, Kavan Singh Sikand, Xuesu Xiao, Sadegh Rabiee, Joydeep Biswas",2022/10/23,Conference 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"Accurate control of robots at high speeds requires a control system that can take into account the kinodynamic interactions of the robot with the environment. Prior works on learning inverse kinodynamic (IKD) models of robots have shown success in capturing the complex kinodynamic effects. However, the types of control problems these approaches can be applied to are limited only to that of following pre-computed kinodynamically feasible trajectories. In this paper we present Optim-FKD, a new formulation for accurate, high-speed robot control that makes use of a learned forward kinodynamic (FKD) model and non-linear least squares optimization. Optim-FKD can be used for accurate, high speed control on any control task specifiable by a non-linear least squares objective. Optim-FKD can solve for control objectives such as path following and time-optimal control in real time, without needing access to pre …",4
Team orienteering coverage planning with uncertain reward,"Bo Liu, Xuesu Xiao, Peter Stone",2021/9/27,Conference 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),"Many municipalities and large organizations have fleets of vehicles that need to be coordinated for tasks such as garbage collection or infrastructure inspection. Motivated by this need, this paper focuses on the common subproblem in which a team of vehicles needs to plan coordinated routes to patrol an area over iterations while minimizing temporally and spatially dependent costs. In particular, at a specific location (e.g., a vertex on a graph), we assume the cost accumulates over time and its growth rate is a random variable with a fixed but unknown mean, and the cost is reset to zero whenever any vehicle visits the vertex (representing the robot ""servicing"" the vertex). We formulate this problem in graph terminology and call it Team Orienteering Coverage Planning with Uncertain Reward (TOCPUR). We propose to solve TOCPUR by simultaneously estimating the accumulated cost at every vertex on the graph and …",4
Exploring applications for autonomous nonverbal human-robot interaction,"Justin W Hart, Nick DePalma, Mitchell W Pryor, Bradley Hayes, Karl Kruusamäe, Reuth Mirsky, Xuesu Xiao",2021/3/8,Book Companion of the 2021 ACM/IEEE International Conference on Human-Robot Interaction,"Non-verbal Human-Robot Interaction (nHRI) encompasses the study of the exchange of human-robot gaze, gesture, touch, body language, paralinguistic, facial and affect expression. nHRI has advanced beyond theoretical and computational contributions. Progress has been made through a variety of user studies and laboratory experiments as well as practical efforts such as integration of nonverbal inputs with other HRI modalities including domain specific implementations. This workshop seeks to promote collaboration between two threads of research: experimental nHRI, and application domains that can benefit from its use.",4
William (red) l. whittaker. energy considerations for wheeled mobile robots operating on a single battery discharge,Xuesu Xiao,2014/8,"Publisher Technical report, Technical Report CMU-RI-TR-14-16, Carnegie Mellon University Robotics Institute",,4
Learning Model Predictive Controllers with Real-Time Attention for Real-World Navigation,"Xuesu Xiao, Tingnan Zhang, Krzysztof Choromanski, Edward Lee, Anthony Francis, Jake Varley, Stephen Tu, Sumeet Singh, Peng Xu, Fei Xia, Sven Mikael Persson, Dmitry Kalashnikov, Leila Takayama, Roy Frostig, Jie Tan, Carolina Parada, Vikas Sindhwani",2022/9/22,Journal arXiv preprint arXiv:2209.10780,"Despite decades of research, existing navigation systems still face real-world challenges when deployed in the wild, e.g., in cluttered home environments or in human-occupied public spaces. To address this, we present a new class of implicit control policies combining the benefits of imitation learning with the robust handling of system constraints from Model Predictive Control (MPC). Our approach, called Performer-MPC, uses a learned cost function parameterized by vision context embeddings provided by Performers -- a low-rank implicit-attention Transformer. We jointly train the cost function and construct the controller relying on it, effectively solving end-to-end the corresponding bi-level optimization problem. We show that the resulting policy improves standard MPC performance by leveraging a few expert demonstrations of the desired navigation behavior in different challenging real-world scenarios. Compared with a standard MPC policy, Performer-MPC achieves >40% better goal reached in cluttered environments and >65% better on social metrics when navigating around humans.",3
Autonomous ground navigation in highly constrained spaces: Lessons learned from the barn challenge at icra 2022,"Xuesu Xiao, Zifan Xu, Zizhao Wang, Yunlong Song, Garrett Warnell, Peter Stone, Tingnan Zhang, Shravan Ravi, Gary Wang, Haresh Karnan, Joydeep Biswas, Nicholas Mohammad, Lauren Bramblett, Rahul Peddi, Nicola Bezzo, Zhanteng Xie, Philip Dames",2022/8/22,Journal arXiv preprint arXiv:2208.10473,"The BARN (Benchmark Autonomous Robot Navigation) Challenge took place at the 2022 IEEE International Conference on Robotics and Automation (ICRA 2022) in Philadelphia, PA. The aim of the challenge was to evaluate state-of-the-art autonomous ground navigation systems for moving robots through highly constrained environments in a safe and efficient manner. Specifically, the task was to navigate a standardized, differential-drive ground robot from a predefined start location to a goal location as quickly as possible without colliding with any obstacles, both in simulation and in the real world. Five teams from all over the world participated in the qualifying simulation competition, three of which were invited to compete with each other at a set of physical obstacle courses at the conference center in Philadelphia. The competition results suggest that autonomous ground navigation in highly constrained spaces, despite seeming ostensibly simple even for experienced roboticists, is actually far from being a solved problem. In this article, we discuss the challenge, the approaches used by the top three winning teams, and lessons learned to direct future research.",3
Visual representation learning for preference-aware path planning,"Kavan Singh Sikand, Sadegh Rabiee, Adam Uccello, Xuesu Xiao, Garrett Warnell, Joydeep Biswas",2022/5/23,Conference 2022 International Conference on Robotics and Automation (ICRA),"Autonomous mobile robots deployed in outdoor environments must reason about different types of terrain for both safety (e.g., prefer dirt over mud) and deployer preferences (e.g., prefer dirt path over flower beds). Most existing solutions to this preference-aware path planning problem use semantic segmentation to classify terrain types from camera images, and then ascribe costs to each type. Unfortunately, there are three key limitations of such approaches - they 1) require preenumeration of the discrete terrain types, 2) are unable to handle hybrid terrain types (e.g., grassy dirt), and 3) require expensive labelled data to train visual semantic segmentation. We introduce Visual Representation Learning for Preference-Aware Path Planning (VRL-PAP), an alternative approach that overcomes all three limitations: VRL-PAP leverages un-labelled human demonstrations of navigation to autonomously generate triplets for …",3
Task-independent causal state abstraction,"Zizhao Wang, Xuesu Xiao, Yuke Zhu, Peter Stone",2021,"Journal Proceedings of the 35th International Conference on Neural Information Processing Systems, Robot Learning workshop","Learning dynamics models accurately and learning policies sample-efficiently are two important challenges for Model-Based Reinforcement Learning (MBRL). Regarding dynamics accuracy, in contrast to the sparse dynamics exhibited in many real world environments, most MBRL methods learn a dense dynamics model which is vulnerable to spurious correlations and therefore generalizes poorly to unseen states. Meanwhile, existing state abstractions can improve sample efficiency, but their dependence on specific reward functions constrains their applications to limited tasks. In this paper, we introduce a novel state abstraction called Task-Independent Causal State Abstraction (TICSA). Exploiting sparsity exhibited in the real world, the proposed method first learns a causal dynamics model that generalizes to unexplored states. A state abstraction can then be derived from the learned dynamics, which not only improves sample efficiency but also applies to many tasks. Using a simulated manipulation environment and two different tasks, we observe that both the dynamics model and policies learned by the proposed method generalize well to unseen states and that TICSA also improves sample efficiency compared to learning without state abstraction.",3
Tethered aerial visual assistance,"Xuesu Xiao, Jan Dufek, Robin R Murphy",2020/1/15,Journal arXiv preprint arXiv:2001.06347,"In this paper, an autonomous tethered Unmanned Aerial Vehicle (UAV) is developed into a visual assistant in a marsupial co-robots team, collaborating with a tele-operated Unmanned Ground Vehicle (UGV) for robot operations in unstructured or confined environments. These environments pose extreme challenges to the remote tele-operator due to the lack of sufficient situational awareness, mostly caused by the unstructuredness and confinement, stationary and limited field-of-view and lack of depth perception from the robot's onboard cameras. To overcome these problems, a secondary tele-operated robot is used in current practices, who acts as a visual assistant and provides external viewpoints to overcome the perceptual limitations of the primary robot's onboard sensors. However, a second tele-operated robot requires extra manpower and teamwork demand between primary and secondary operators. The manually chosen viewpoints tend to be subjective and sub-optimal. Considering these intricacies, we develop an autonomous tethered aerial visual assistant in place of the secondary tele-operated robot and operator, to reduce human robot ratio from 2:2 to 1:2. Using a fundamental viewpoint quality theory, a formal risk reasoning framework, and a newly developed tethered motion suite, our visual assistant is able to autonomously navigate to good-quality viewpoints in a risk-aware manner through unstructured or confined spaces with a tether. The developed marsupial co-robots team could improve tele-operation efficiency in nuclear operations, bomb squad, disaster robots, and other domains with novel tasks or highly occluded …",3
Autonomous Ground Navigation in Highly Constrained Spaces: Lessons Learned From the Benchmark Autonomous Robot Navigation Challenge at ICRA 2022 [Competitions],"Xuesu Xiao, Zifan Xu, Zizhao Wang, Yunlong Song, Garrett Warnell, Peter Stone, Tingnan Zhang, Shravan Ravi, Gary Wang, Haresh Karnan, Joydeep Biswas, Nicholas Mohammad, Lauren Bramblett, Rahul Peddi, Nicola Bezzo, Zhanteng Xie, Philip Dames",2022/12/7,Journal IEEE Robotics & Automation Magazine,"The Benchmark Autonomous Robot Navigation (BARN) Challenge took place at the 2022 IEEE International Conference on Robotics and Automation (ICRA), in Philadelphia, PA, USA. The aim of the challenge was to evaluate state-of-the-art autonomous ground navigation systems for moving robots through highly constrained environments in a safe and efficient manner. Specifically, the task was to navigate a standardized differential drive ground robot from a predefined start location to a goal location as quickly as possible without colliding with any obstacles, both in simulation and in the real world. Five teams from all over the world participated in the qualifying simulation competition, three of which were invited to compete with one another at a set of physical obstacle courses at the conference center in Philadelphia. The competition results suggest that autonomous ground navigation in highly constrained spaces …",2
A protocol for validating social navigation policies,"Sören Pirk, Edward Lee, Xuesu Xiao, Leila Takayama, Anthony Francis, Alexander Toshev",2022/4/11,Journal arXiv preprint arXiv:2204.05443,"Enabling socially acceptable behavior for situated agents is a major goal of recent robotics research. Robots should not only operate safely around humans, but also abide by complex social norms. A key challenge for developing socially-compliant policies is measuring the quality of their behavior. Social behavior is enormously complex, making it difficult to create reliable metrics to gauge the performance of algorithms. In this paper, we propose a protocol for social navigation benchmarking that defines a set of canonical social navigation scenarios and an in-situ metric for evaluating performance on these scenarios using questionnaires. Our experiments show this protocol is realistic, scalable, and repeatable across runs and physical spaces. Our protocol can be replicated verbatim or it can be used to define a social navigation benchmark for novel scenarios. Our goal is to introduce a protocol for benchmarking social scenarios that is homogeneous and comparable.",2
From agile ground to aerial navigation: Learning from learned hallucination. In 2021 IEEE/RSJ international conference on intelligent robots and systems (IROS),"Z Wang, X Xiao, AJ Nettekoven, K Umasankar, A Singh, S Bommakanti, U Topcu, P Stone",2021,Publisher IEEE,,2
Energy utilization and energetic estimation of achievable range for wheeled mobile robots operating on a single battery discharge,"X Xiao, WRL Whittaker",2015/6,Book Tech. Rep. CMU-RI-TR-14–15,,2
Uav assisted usv visual navigation for marine mass casualty incident response. In 2017 IEEE,"Xuesu Xiao, Jan Dufek, Tim Woodbury, Robin Murphy",RSJ International Conference on Intelligent Robots and Systems (IROS),Pages 6105-6110,,2
Benchmarking reinforcement learning techniques for autonomous navigation,"Zifan Xu, Bo Liu, Xuesu Xiao, Anirudh Nair, Peter Stone",2022/10/10,Journal arXiv preprint arXiv:2210.04839,"Deep reinforcement learning (RL) has brought many successes for autonomous robot navigation. However, there still exists important limitations that prevent real-world use of RL-based navigation systems. For example, most learning approaches lack safety guarantees; and learned navigation systems may not generalize well to unseen environments. Despite a variety of recent learning techniques to tackle these challenges in general, a lack of an open-source benchmark and reproducible learning methods specifically for autonomous navigation makes it difficult for roboticists to choose what learning methods to use for their mobile robots and for learning researchers to identify current shortcomings of general learning methods for autonomous navigation. In this paper, we identify four major desiderata of applying deep RL approaches for autonomous navigation: (D1) reasoning under uncertainty, (D2) safety, (D3) learning from limited trial-and-error data, and (D4) generalization to diverse and novel environments. Then, we explore four major classes of learning techniques with the purpose of achieving one or more of the four desiderata: memory-based neural network architectures (D1), safe RL (D2), model-based RL (D2, D3), and domain randomization (D4). By deploying these learning techniques in a new open-source large-scale navigation benchmark and real-world environments, we perform a comprehensive study aimed at establishing to what extent can these techniques achieve these desiderata for RL-based navigation systems.",1
Camera-IMU Extrinsic Calibration Quality Monitoring for Autonomous Ground Vehicles,"Xuesu Xiao, Yulin Zhang, Haifeng Li, Hongpeng Wang, Binbin Li",2022/2/16,Journal IEEE Robotics and Automation Letters,"Highly accurate sensor extrinsic calibration is critical for data fusion from multiple sensors, such as camera and Inertial Measurement Unit (IMU) sensor suit. A pre-calibrated extrinsics, however, may no longer be accurate due to external disturbances, e.g., vehicle vibration, which will lead to significant performance deterioration of autonomous vehicles. Existing approaches rely on online recalibration at a fixed frequency regardless of whether the extrinsics have actually been changed or recalibration is needed, which is computationally inefficient. In this letter, we present an approach to monitor extrinsic calibration quality for camera-IMU sensor suite to determine when recalibration is actually necessary. We propose an efficient algorithm to detect robust road image features, utilize IMU data to capture the mismatches of those features, and quantify extrinsic calibration error through three commonly-used error metrics …",1
Learning Model Predictive Controllers with Real-Time Attention for Real-World Navigation,"Xuesu Xiao, Tingnan Zhang, Krzysztof Choromanski, Edward Lee, Anthony Francis, Jake Varley, Stephen Tu, Sumeet Singh, Peng Xu, Fei Xia, Sven Mikael Persson, Dmitry Kalashnikov, Leila Takayama, Roy Frostig, Jie Tan, Carolina Parada, Vikas Sindhwani",2022/9/22,Journal arXiv preprint arXiv:2209.10780,"Despite decades of research, existing navigation systems still face real-world challenges when deployed in the wild, e.g., in cluttered home environments or in human-occupied public spaces. To address this, we present a new class of implicit control policies combining the benefits of imitation learning with the robust handling of system constraints from Model Predictive Control (MPC). Our approach, called Performer-MPC, uses a learned cost function parameterized by vision context embeddings provided by Performers -- a low-rank implicit-attention Transformer. We jointly train the cost function and construct the controller relying on it, effectively solving end-to-end the corresponding bi-level optimization problem. We show that the resulting policy improves standard MPC performance by leveraging a few expert demonstrations of the desired navigation behavior in different challenging real-world scenarios. Compared with a standard MPC policy, Performer-MPC achieves >40% better goal reached in cluttered environments and >65% better on social metrics when navigating around humans.",3
Incorporating gaze into social navigation,"Justin Hart, Reuth Mirsky, Xuesu Xiao, Peter Stone",2021/7/8,Journal arXiv preprint arXiv:2107.04001,"Most current approaches to social navigation focus on the trajectory and position of participants in the interaction. Our current work on the topic focuses on integrating gaze into social navigation, both to cue nearby pedestrians as to the intended trajectory of the robot and to enable the robot to read the intentions of nearby pedestrians. This paper documents a series of experiments in our laboratory investigating the role of gaze in social navigation.",1
Conflict Avoidance in Social Navigation--a Survey,"Reuth Mirsky, Xuesu Xiao, Justin Hart, Peter Stone",2021/6/23,Journal arXiv preprint arXiv:2106.12113,"A major goal in robotics is to enable intelligent mobile robots to operate smoothly in shared human-robot environments. One of the most fundamental capabilities in service of this goal is competent navigation in this ``social"" context. As a result, there has been a recent surge of research on social navigation; and especially as it relates to the handling of conflicts between agents during social navigation. These developments introduce a variety of models and algorithms, however as this research area is inherently interdisciplinary, many of the relevant papers are not comparable and there is no shared standard vocabulary. This survey aims to bridge this gap by introducing such a common language, using it to survey existing work, and highlighting open problems. It starts by defining the boundaries of this survey to a limited, yet highly common type of social navigation - conflict avoidance. Within this proposed scope, this survey introduces a detailed taxonomy of the conflict avoidance components. This survey then maps existing work into this taxonomy, while discussing papers using its framing. Finally, this paper proposes some future research directions and open problems that are currently on the frontier of social navigation to aid ongoing and future research.",1
Using parallelized containers for reinforcement learning on large computer clusters,"Anirudh Nair, Zifan Xu, Gauraang Dhamankar, Xuesu Xiao",2021,"Description Reinforcement Learning (RL) robotics projects require massive amounts of computing power for both training and experimentation. Moreover, it is likely that large-scale computing clusters may not have the required software to run experiments for certain projects, eg Robot Operating System (ROS)(Quigley et al. 2009). To remedy this, containers, such as Docker (Merkel 2014) and Singularity (Kurtzer, Sochat, and Bauer 2017), have been used to package all the necessary software needed to run experiments for RL. In addition, the use of a container improves reproducability since the container can be built into an image file and shared very easily.While these containers can be built from scratch based on each case, the code to create the container, run experiments, and collect data is quite general. There are guides on how to build containers; however, there is no clear guide that exists for creating and implementing these containers for large scale RL robotics tasks on computer clusters. In this paper, we present our Parallelized Containers for Reinforcement Learning (PCRL) framework which provides a general foundation to carry out large scale RL robotics tasks. Moreover, we present an example of this framework being applied to a mobile robot navigation task. Our PCRL framework methodology is specific to the actor-critic RL algorithms as well as the HTCondor scheduler and Singularity containers; however, this framework can be easily extended to use other RL algorithms, container methods, and schedulers.","Reinforcement Learning (RL) robotics projects require massive amounts of computing power for both training and experimentation. Moreover, it is likely that large-scale computing clusters may not have the required software to run experiments for certain projects, eg Robot Operating System (ROS)(Quigley et al. 2009). To remedy this, containers, such as Docker (Merkel 2014) and Singularity (Kurtzer, Sochat, and Bauer 2017), have been used to package all the necessary software needed to run experiments for RL. In addition, the use of a container improves reproducability since the container can be built into an image file and shared very easily.",1
Risk-aware Path and Motion Planning for a Tethered Aerial Visual Assistant in Unstructured or Confined Environments,Xuesu Xiao,2020/7/19,Journal arXiv preprint arXiv:2007.09595,"This research aims at developing path and motion planning algorithms for a tethered Unmanned Aerial Vehicle (UAV) to visually assist a teleoperated primary robot in unstructured or confined environments. The emerging state of the practice for nuclear operations, bomb squad, disaster robots, and other domains with novel tasks or highly occluded environments is to use two robots, a primary and a secondary that acts as a visual assistant to overcome the perceptual limitations of the sensors by providing an external viewpoint. However, the benefits of using an assistant have been limited for at least three reasons: (1) users tend to choose suboptimal viewpoints, (2) only ground robot assistants are considered, ignoring the rapid evolution of small unmanned aerial systems for indoor flying, (3) introducing a whole crew for the second teleoperated robot is not cost effective, may introduce further teamwork demands, and therefore could lead to miscommunication. This dissertation proposes to use an autonomous tethered aerial visual assistant to replace the secondary robot and its operating crew. Along with a pre-established theory of viewpoint quality based on affordances, this dissertation aims at defining and representing robot motion risk in unstructured or confined environments. Based on those theories, a novel high level path planning algorithm is developed to enable risk-aware planning, which balances the tradeoff between viewpoint quality and motion risk in order to provide safe and trustworthy visual assistance flight. The planned flight trajectory is then realized on a tethered UAV platform. The perception and actuation are tailored to fit the …",1
Robot motion risk reasoning framework,"Xuesu Xiao, Jan Dufek, Robin R Murphy",2019/9/5,Journal arXiv preprint arXiv:1909.02531,"This paper presents a formal and comprehensive reasoning framework for robot motion risk, with a focus on locomotion in challenging unstructured or confined environments. Risk which locomoting robots face in physical spaces was not formally defined in the robotics literature. Safety or risk concerns were addressed in an ad hoc fashion, depending only on the specific application of interest. Without a formal definition, certain properties of risk were simply assumed but ill-supported, such as additivity or being Markovian. The only contributing adverse effect being considered is related with obstacles. This work proposes a formal definition of robot motion risk using propositional logic and probability theory. It presents a universe of risk elements within three major risk categories and unifies them into one single metric. True properties of risk are revealed with formal reasoning, such as non-additivity or history-dependency. Risk representation which encompasses risk effects from both temporal and spatial domain is presented. The resulted risk framework provides a formal approach to reason about robot motion risk. Safety of robot locomotion could be explicitly reasoned, quantified, and compared. It could be used for risk-aware planning and reasoning by both human and robotic agents.",1
Orange: Operational range estimation for mobile robot exploration on a single discharge cycle,"Kshitij Tiwari, Xuesu Xiao, Ville Kyrki, Nak Young Chong",2019/5/29,Journal arXiv preprint arXiv:1905.12559,"This paper presents an approach for estimating the operational range for mobile robot exploration on a single battery discharge. Deploying robots in the wild usually requires uninterrupted energy sources to maintain the robot's mobility throughout the entire mission. However, for most endeavors into the unknown environments, recharging is usually not an option, due to the lack of pre-installed recharging stations or other mission constraints. In these cases, the ability to model the on-board energy consumption and estimate the operational range is crucial to prevent running out of battery in the wild. To this end, this work describes our recent findings that quantitatively break down the robot's on-board energy consumption and predict the operational range to guarantee safe mission completion on a single battery discharge cycle. Two range estimators with different levels of generality and model fidelity are presented, whose performances were validated on physical robot platforms in both indoor and outdoor environments. Model performance metrics are also presented as benchmarks.",1
Packet Loss Concealment in Wireless Networks with LSTM Sequence Predictors for Inertial Pose Tracking,"Xuesu Xiao, Shuayb Zarar",2018,Journal IEEE Int. Conf. Wearable and Implantable Body Sensor Networks (BSN),"Inertial sensing is a technology that enables motion capture outside of well-defined studio environments. Yet, there are several hurdles that have be overcome in order to achieve a high-quality user experience. Among them is enabling robust wireless communication. Thanks to strict requirements on throughput and far-field operation along with existing issues of occlusion and client interference, packet-loss rates in wireless inertial-sensing systems can amplify pose-tracking errors by as much as 39%. In this paper, we develop a new type of sequencepredictors based on long short-term memory neural networks that can be used to significantly conceal packet losses in inertial pose-tracking systems. To lower computational overheads, we systematically exploit spatio-temporal correlations of data and distribute sensor loads among multiple predictors. Through experiments conducted with 3.5 hrs. of high-frequency inertial motion-capture data, we demonstrate that our approach is able to fully conceal packet losses at rates of up to 20%.",1
DynaBARN: Benchmarking Metric Ground Navigation in Dynamic Environments,"Anirudh Nair, Fulin Jiang, Kang Hou, Zifan Xu, Shuozhe Li, Xuesu Xiao, Peter Stone",navigation,Volume 7,"Safely avoiding dynamic obstacles while moving toward a goal is a fundamental capability of autonomous mobile robots. Current benchmarks for dynamic obstacle avoidance do not provide a way to alter how obstacles move and instead use only a single method to uniquely determine the movement of obstacles, eg, constant velocity, the social force model, or Optimal Reciprocal Collision Avoidance (ORCA). Using a single method in this way restricts the variety of scenarios in which the robot navigation system is trained and/or evaluated, thus limiting its robustness to dynamic obstacles of different speeds, trajectory smoothness, acceleration/deceleration, etc., which we call motion profiles. In this paper, we present a simulation testbed, DynaBARN, to evaluate a robot navigation system’s ability to navigate in environments with obstacles with different motion profiles, which are systematically generated by a set of difficulty metrics. Additionally, we provide a demonstration collection pipeline that records robot navigation trials controlled by human users to compare with autonomous navigation performance and to develop navigation systems using learning from demonstration. Finally, we provide results of four classical and learning-based navigation systems in DynaBARN, which can serve as baselines for future studies. We release DynaBARN open source as a standardized benchmark for future autonomous navigation research in environments with different dynamic obstacles. The code and environments are released at https://github. com/aninair1905/DynaBARN.",1
"Toward Wheeled Mobility on Vertically Challenging Terrain: Platforms, Datasets, and Algorithms","Aniket Datar, Chenhui Pan, Mohammad Nazeri, Xuesu Xiao",2023/3/2,Journal arXiv preprint arXiv:2303.00998,"Most conventional wheeled robots can only move in flat environments and simply divide their planar workspaces into free spaces and obstacles. Deeming obstacles as non-traversable significantly limits wheeled robots' mobility in real-world, non-flat, off-road environments, where part of the terrain (e.g., steep slopes, rugged boulders) will be treated as non-traversable obstacles. To improve wheeled mobility in those non-flat environments with vertically challenging terrain, we present two wheeled platforms with little hardware modification compared to conventional wheeled robots; we collect datasets of our wheeled robots crawling over previously non-traversable, vertically challenging terrain to facilitate data-driven mobility; we also present algorithms and their experimental results to show that conventional wheeled robots have previously unrealized potential of moving through vertically challenging terrain. We make our platforms, datasets, and algorithms publicly available to facilitate future research on wheeled mobility.",
Learning Real-world Autonomous Navigation by Self-Supervised Environment Synthesis,"Zifan Xu, Anirudh Nair, Xuesu Xiao, Peter Stone",2022/10/10,Journal arXiv preprint arXiv:2210.04852,"Machine learning approaches have recently enabled autonomous navigation for mobile robots in a data-driven manner. Since most existing learning-based navigation systems are trained with data generated in artificially created training environments, during real-world deployment at scale, it is inevitable that robots will encounter unseen scenarios, which are out of the training distribution and therefore lead to poor real-world performance. On the other hand, directly training in the real world is generally unsafe and inefficient. To address this issue, we introduce Self-supervised Environment Synthesis (SES), in which, after real-world deployment with safety and efficiency requirements, autonomous mobile robots can utilize experience from the real-world deployment, reconstruct navigation scenarios, and synthesize representative training environments in simulation. Training in these synthesized environments leads to improved future performance in the real world. The effectiveness of SES at synthesizing representative simulation environments and improving real-world navigation performance is evaluated via a large-scale deployment in a high-fidelity, realistic simulator and a small-scale deployment on a physical robot.",
Learning Perceptual Hallucination for Multi-Robot Navigation in Narrow Hallways,"Jin-Soo Park, Xuesu Xiao, Garrett Warnell, Harel Yedidsion, Peter Stone",2022/9/27,Journal arXiv preprint arXiv:2209.13641,"While current systems for autonomous robot navigation can produce safe and efficient motion plans in static environments, they usually generate suboptimal behaviors when multiple robots must navigate together in confined spaces. For example, when two robots meet each other in a narrow hallway, they may either turn around to find an alternative route or collide with each other. This paper presents a new approach to navigation that allows two robots to pass each other in a narrow hallway without colliding, stopping, or waiting. Our approach, Perceptual Hallucination for Hallway Passing (PHHP), learns to synthetically generate virtual obstacles (i.e., perceptual hallucination) to facilitate passing in narrow hallways by multiple robots that utilize otherwise standard autonomous navigation systems. Our experiments on physical robots in a variety of hallways show improved performance compared to multiple baselines.",
Human-Interactive Robot Learning (HIRL),"Reuth Mirsky, Kim Baraka, Taylor Kessler Faulkner, Justin Hart, Harel Yedidsion, Xuesu Xiao",2022/3/7,Conference 2022 17th ACM/IEEE International Conference on Human-Robot Interaction (HRI),"With robots poised to enter our daily environments, we conjecture that they will not only need to work for people, but also learn from them. An active area of investigation in the robotics, machine learning, and human-robot interaction communities is the design of teachable robotic agents that can learn interactively from human input. To refer to these research efforts, we use the umbrella term Human-Interactive Robot Learning (HIRL). While algorithmic solutions for robots learning from people have been investigated in a variety of ways, HIRL, as a fairly new research area, is still lacking: 1) a formal set of definitions to classify related but distinct research problems or solutions, 2) benchmark tasks, interactions, and metrics to evaluate the performance of HIRL algorithms and interactions, and 3) clear long-term research challenges to be addressed by different communities. The main goal of this workshop will be to …",
A Protocol for Validating Social Navigation Policies,"Alexander Toshkov Toshev, Anthony G Francis, Edward Lee, Leila Takayama, Soeren Pirk, Xuesu Xiao",2022,"Description Enabling socially acceptable behavior for situated agents is a major goal of recent robotics research. Robots not only need to operate safely around humans, but also in a way that their behavior complies to social expectations. A key challenge for developing socially-compliant polices is to measure the performance of the behavior they generate for the robot. Due to the enormous complexity of social behavior, metrics for measuring the success and failure of algorithms are difficult to obtain. In this paper, we introduce a protocol to establish a social navigation benchmark that focuses on defining a set of canonical social navigation scenarios and an in-situ metric for social scenarios based on questionnaires. Our protocol can be replicated verbatim or it can be used to define a social navigation benchmark for novel scenarios. Our goal is to introduce a protocol for benchmarking social scenarios that is homogeneous and …","Enabling socially acceptable behavior for situated agents is a major goal of recent robotics research. Robots not only need to operate safely around humans, but also in a way that their behavior complies to social expectations. A key challenge for developing socially-compliant polices is to measure the performance of the behavior they generate for the robot. Due to the enormous complexity of social behavior, metrics for measuring the success and failure of algorithms are difficult to obtain. In this paper, we introduce a protocol to establish a social navigation benchmark that focuses on defining a set of canonical social navigation scenarios and an in-situ metric for social scenarios based on questionnaires. Our protocol can be replicated verbatim or it can be used to define a social navigation benchmark for novel scenarios. Our goal is to introduce a protocol for benchmarking social scenarios that is homogeneous and …",
Visual Representation Learning for Preference-Aware Path Planning,"Kavan Singh Sikand, Sadegh Rabiee, Adam Uccello, Xuesu Xiao, Garrett Warnell, Joydeep Biswas",2021/9,Journal arXiv e-prints,"Autonomous mobile robots deployed in outdoor environments must reason about different types of terrain for both safety (eg, prefer dirt over mud) and deployer preferences (eg, prefer dirt path over flower beds). Most existing solutions to this preference-aware path planning problem use semantic segmentation to classify terrain types from camera images, and then ascribe costs to each type. Unfortunately, there are three key limitations of such approaches--they 1) require pre-enumeration of the discrete terrain types, 2) are unable to handle hybrid terrain types (eg, grassy dirt), and 3) require expensive labelled data to train visual semantic segmentation. We introduce Visual Representation Learning for Preference-Aware Path Planning (VRL-PAP), an alternative approach that overcomes all three limitations: VRL-PAP leverages unlabeled human demonstrations of navigation to autonomously generate triplets for …",
Adaptive Planner Parameter Learning for Mobile Robot Navigation in the Wild,"Xuesu Xiao, Bo Liu, Garrett Warnell, Zizhao Wang, Zifan Xu, Gauraang Dhamankar, Anirudh Nair, Peter Stone",2021,"Description Mobile robot navigation has been studied by the robotics community for decades (Quinlan and Khatib 1993; Fox, Burgard, and Thrun 1997). In relatively controlled and uniform environments, eg, indoor laboratories, existing navigation systems can produce robust navigation behaviors with verifiable guarantees that the robot will not collide with obstacles while moving.However, when facing the variety of navigation environments encountered during deployment in the wild, existing navigation systems require robotics experts to make insitu adjustments to adapt systems through sensor calibration (Xiao et al. 2017) or parameter tuning (eg maximum speed, sampling rate, inflation radius))(Zheng 2017). This dependency on expert roboticists onsite during deployment makes it difficult for non-expert users to successfully deploy mobile robots in the wild.","Mobile robot navigation has been studied by the robotics community for decades (Quinlan and Khatib 1993; Fox, Burgard, and Thrun 1997). In relatively controlled and uniform environments, eg, indoor laboratories, existing navigation systems can produce robust navigation behaviors with verifiable guarantees that the robot will not collide with obstacles while moving.",
Safe Learning from Hallucination for Navigation in the Wild,"Xuesu Xiao, Bo Liu, Garrett Warnell, Peter Stone",2021,"Description One conundrum of using machine learning to find safe navigation systems that can be deployed in the wild is that, in order to produce safe motions in obstacle-occupied spaces, a robot needs to first gather experience in those dangerous spaces before it has learned how to generate safe motions. This conundrum becomes even more severe when the goal is to navigate in challenging or even adversarial real-world scenarios. One solution to learn navigation in the wild is to learn from pre-supplied, good demonstrations (eg, from a human expert (Xiao et al. 2020a; Wang et al. 2020)) or perform exploration based on trial-and-error (Xu et al. 2020) in the deployment environment (or one very similar to it), but both of these approaches become costly in dangerous spaces in the wild.Although learning to find the single optimal plan among the variety of maneuvers the robot can perform is difficult without many trial-and-error attempts or an expert who is already capable of doing so, given a plan performed in obstacle-free space, it is relatively easy to find an obstacle configuration for which that plan is optimal. Based on this observation, instead of finding the optimal motion plan for a specific obstacle configuration, we consider this “dual” problem of classical motion planning and seek to find the obstacle configuration (s) where a specific motion plan is guaranteed to be optimal. We name this process hallucination. Solving this problem gives us the freedom to allow random exploration in a completely safe obstacle-free space and collect an extensive amount of motion plans, whose optimally will be assured by a class of hallucination techniques. In this work …","One conundrum of using machine learning to find safe navigation systems that can be deployed in the wild is that, in order to produce safe motions in obstacle-occupied spaces, a robot needs to first gather experience in those dangerous spaces before it has learned how to generate safe motions. This conundrum becomes even more severe when the goal is to navigate in challenging or even adversarial real-world scenarios. One solution to learn navigation in the wild is to learn from pre-supplied, good demonstrations (eg, from a human expert (Xiao et al. 2020a; Wang et al. 2020)) or perform exploration based on trial-and-error (Xu et al. 2020) in the deployment environment (or one very similar to it), but both of these approaches become costly in dangerous spaces in the wild.",
Lifelong Learning for Resource-Constrained Robot Navigation in the Wild,"Bo Liu, Xuesu Xiao, Peter Stone",2021,Journal Intelligence (www. aaai. org),"Classical mobile robots are designed to be adaptive to different navigation environments by in-situ adjustment of the underlying navigation system, such as by sensor calibration (Xiao et al. 2017) or by parameter tuning (Xiao et al. 2020). However, without adjustment from expert knowledge, the untuned system may repeat the same mistakes (eg stuck in the same bottleneck) even though it has navigated in the same environment multiple times. Recent success in using machine learning for mobile robot navigation indicates the potential of improving navigation performance from a robot’s past experience in the same environment (Kahn et al. 2018). When facing different navigation environments, however, learning methods cannot generalize well to unseen scenarios: They must re-learn to navigate in the new environments. More importantly, the learned system is prone to catastrophic forgetting, which causes the robot to forget what was learned in previous environments (French 1999).",
Motion Planners Learned from Geometric Hallucination,"Xuesu Xiao, Bo Liu, Peter Stone",2020/10/19,Journal arXiv preprint arXiv:2010.09158,"Learning motion planners to move robot from one point to another within an obstacle-occupied space in a collision-free manner requires either an extensive amount of data or high-quality demonstrations. This requirement is caused by the fact that among the variety of maneuvers the robot can perform, it is difficult to find the single optimal plan without many trial-and-error or an expert who is already capable of doing so. However, given a plan performed in obstacle-free space, it is relatively easy to find an obstacle geometry, where this plan is optimal. We consider this ""dual"" problem of classical motion planning and name this process of finding appropriate obstacle geometry as hallucination. In this work, we present two different approaches to hallucinate (1) the most constrained and (2) a minimal obstacle space where a given plan executed during an exploration phase in a completely safe obstacle-free environment remains optimal. We then train an end-to-end motion planner that can produce motions to move through realistic obstacles during deployment. Both methods are tested on a physical mobile robot in real-world cluttered environments.",
Design and realization of an automobile running platform with external panorama simulation,"Xue-Su Xiao, Mu-Zhou Wu, Jian-Tao Li, Hong-Yi Zhang",2012,Journal Jidian Gongcheng/ Mechanical & Electrical Engineering Magazine,"Aiming at the incapability of effective and vivid demonstration of the external dynamic panorama during the automobile's motion in automobile exhibitions, which are caused by a variety of factors such as the restriction of space, a practical automobile running simulation platform was designed and implemented. Taking into account both the current market demand and technological development of the actual situation, the platform was designed with the idea of relative motion and semi-physical simulation technology. Based on a model car, the system can provide running simulation of automobile's external state and panorama. It was realized by the cooperative work of the four mechanical subsystems, i. e. automobile(model), automobile-pose control, track-pose control and effect-scenery control, and the electronic control system with two control modes, i. e. instant control and functional module control. The simulation …",
Team Coordination on Graphs with State-Dependent Edge Cost,"Sara Oughourli, Manshi Limbu, Zechen Hu, Xuan Wang, Xuesu Xiao, Daigo Shishika","This paper studies a team coordination problem in a graph environment. Specifically, we incorporate “support” action which an agent can take to reduce the cost for its teammate to traverse some edges that have higher costs otherwise. Due to this added feature, the graph traversal is no longer a standard multi-agent path planning problem. To solve this new problem, we propose a novel formulation that poses it as a planning problem in the joint state space: the joint state graph (JSG). Since the edges of JSG implicitly incorporate the support actions taken by the agents, we are able to now optimize the joint actions by solving a standard single-agent path planning problem in JSG. One main drawback of this approach is the curse of dimensionality in both the number of agents and the size of the graph. To improve scalability in graph size, we further propose a hierarchical decomposition method to perform path planning in two levels. We provide complexity analysis as well as a statistical analysis to demonstrate the efficiency of our algorithm.","Scholar articles Team Coordination on Graphs with State-Dependent Edge CostS Oughourli, M Limbu, Z Hu, X Wang, X Xiao…","This paper studies a team coordination problem in a graph environment. Specifically, we incorporate “support” action which an agent can take to reduce the cost for its teammate to traverse some edges that have higher costs otherwise. Due to this added feature, the graph traversal is no longer a standard multi-agent path planning problem. To solve this new problem, we propose a novel formulation that poses it as a planning problem in the joint state space: the joint state graph (JSG). Since the edges of JSG implicitly incorporate the support actions taken by the agents, we are able to now optimize the joint actions by solving a standard single-agent path planning problem in JSG. One main drawback of this approach is the curse of dimensionality in both the number of agents and the size of the graph. To improve scalability in graph size, we further propose a hierarchical decomposition method to perform path planning in two levels. We provide complexity analysis as well as a statistical analysis to demonstrate the efficiency of our algorithm.",
Using Human-Inspired Signals to Disambiguate Navigational Intentions,"Bonny Mahajan, Jamin Goo, Kathryn Baldauf, Sydney Owen, Peter Stone","People are proficient at communicating their intentions in order to avoid conflicts when navigating in narrow, crowded environments. Mobile robots, on the other hand, often lack both the ability to interpret human intentions and the ability to clearly communicate their own intentions to people sharing their space. This work addresses the second of these points, leveraging insights about how people implicitly communicate with each other through gaze to enable mobile robots to more clearly signal their navigational intention. We present a human study measuring the importance of gaze in coordinating people’s navigation. This study is followed by the development of a virtual agent head which is added to a mobile robot platform. Comparing the performance of a robot with a virtual agent head against one with an LED turn signal demonstrates its ability to impact people’s navigational choices, and that people more easily interpret the gaze cue than the LED turn signal.","Scholar articles Using Human-Inspired Signals to Disambiguate Navigational IntentionsB Mahajan, J Goo, K Baldauf, S Owen, P StoneRelated articles All 5 versions ","People are proficient at communicating their intentions in order to avoid conflicts when navigating in narrow, crowded environments. Mobile robots, on the other hand, often lack both the ability to interpret human intentions and the ability to clearly communicate their own intentions to people sharing their space. This work addresses the second of these points, leveraging insights about how people implicitly communicate with each other through gaze to enable mobile robots to more clearly signal their navigational intention. We present a human study measuring the importance of gaze in coordinating people’s navigation. This study is followed by the development of a virtual agent head which is added to a mobile robot platform. Comparing the performance of a robot with a virtual agent head against one with an LED turn signal demonstrates its ability to impact people’s navigational choices, and that people more easily interpret the gaze cue than the LED turn signal.",
Improving Autonomous Navigation by Self-Supervised Environment Synthesis,"Zifan Xu, Anirudh Nair, Xuesu Xiao, Peter Stone","Machine learning approaches have recently enabled autonomous navigation for mobile robots in a data-driven manner. Since existing learning-based navigation systems are trained with data generated in specific training environments, during real-world deployment at scale, it is inevitable that the robot will encounter unseen scenarios, which are out of the training distribution and therefore lead to poor navigation performance. To address this issue, we introduce Self-supervised Environment Synthesis (SES), in which autonomous mobile robots with a pre-trained policy can identify difficult navigation scenarios during real-world deployment, synthesize representative environments to be added to the training distribution, and then learn from the improved training set. In our experiments, extensive empirical evaluations indicate that, based on selfidentified difficult navigation scenarios experienced in a largescale real-world deployment, SES can effectively synthesize representative environments, learning from which can further improve future navigation performance in unseen similar realworld scenarios.","Scholar articles Improving Autonomous Navigation by Self-Supervised Environment SynthesisZ Xu, A Nair, X Xiao, P StoneRelated articles ","Machine learning approaches have recently enabled autonomous navigation for mobile robots in a data-driven manner. Since existing learning-based navigation systems are trained with data generated in specific training environments, during real-world deployment at scale, it is inevitable that the robot will encounter unseen scenarios, which are out of the training distribution and therefore lead to poor navigation performance. To address this issue, we introduce Self-supervised Environment Synthesis (SES), in which autonomous mobile robots with a pre-trained policy can identify difficult navigation scenarios during real-world deployment, synthesize representative environments to be added to the training distribution, and then learn from the improved training set. In our experiments, extensive empirical evaluations indicate that, based on selfidentified difficult navigation scenarios experienced in a largescale real-world deployment, SES can effectively synthesize representative environments, learning from which can further improve future navigation performance in unseen similar realworld scenarios.",
Visually Adaptive Geometric Navigation,"Shravan Ravi, Shreyas Satewar, Gary Wang, Xuesu Xiao, Garrett Warnell, Joydeep Biswas, Peter Stone","While classical autonomous navigation systems can move robots from one point to another in a collisionfree manner due to geometric modeling, recent approaches to visual navigation allow robots to consider semantic information. However, most visual navigation systems do not explicitly reason about geometry, which may potentially lead to collisions. This paper presents Visually Adaptive Geometric Navigation (VAGN), which marries the two schools of navigation approaches to produce a navigation system that is able to adapt to the visual appearance of the environment while maintaining collision-free behavior. Employing a classical geometric navigation system to address geometric safety and efficiency, VAGN consults visual perception to dynamically adjust the classical planner’s hyperparameters (eg, maximum speed, inflation radius) to enable navigational behaviors not possible with purely geometric reasoning. VAGN is implemented on two different physical ground robots with different action spaces, navigation systems, and parameter sets. VAGN demonstrates superior navigation performance in both a test course with rich semantic and geometric features and a real-world deployment compared to other navigation baselines using visual and/or geometric input.","Scholar articles Visually Adaptive Geometric NavigationS Ravi, S Satewar, G Wang, X Xiao, G Warnell…Related articles All 4 versions ","While classical autonomous navigation systems can move robots from one point to another in a collisionfree manner due to geometric modeling, recent approaches to visual navigation allow robots to consider semantic information. However, most visual navigation systems do not explicitly reason about geometry, which may potentially lead to collisions. This paper presents Visually Adaptive Geometric Navigation (VAGN), which marries the two schools of navigation approaches to produce a navigation system that is able to adapt to the visual appearance of the environment while maintaining collision-free behavior. Employing a classical geometric navigation system to address geometric safety and efficiency, VAGN consults visual perception to dynamically adjust the classical planner’s hyperparameters (eg, maximum speed, inflation radius) to enable navigational behaviors not possible with purely geometric reasoning. VAGN is implemented on two different physical ground robots with different action spaces, navigation systems, and parameter sets. VAGN demonstrates superior navigation performance in both a test course with rich semantic and geometric features and a real-world deployment compared to other navigation baselines using visual and/or geometric input.",
Lifelong Learning for Navigation,"Bo Liu, Xuesu Xiao, Peter Stone","This paper presents a self-improving lifelong learning framework for a mobile robot navigating in different environments. Classical static navigation methods require environment-specific in-situ system adjustment, eg from human experts, or may repeat their mistakes regardless of how many times they have navigated in the same environment. Having the potential to improve with experience, learningbased navigation is highly dependent on access to training resources, eg sufficient memory and fast computation, and is prone to forgetting previously learned capability, especially when facing different environments. In this work, we propose Lifelong Learning for Navigation (LLfN) which (1) improves a mobile robot’s navigation behavior purely based on its own experience, and (2) retains the robot’s capability to navigate in previous environments after learning in new ones. LLfN is implemented and tested entirely onboard a physical robot with a limited memory and computation budget.","Scholar articles Lifelong Learning for NavigationB Liu, X Xiao, P StoneRelated articles ","This paper presents a self-improving lifelong learning framework for a mobile robot navigating in different environments. Classical static navigation methods require environment-specific in-situ system adjustment, eg from human experts, or may repeat their mistakes regardless of how many times they have navigated in the same environment. Having the potential to improve with experience, learningbased navigation is highly dependent on access to training resources, eg sufficient memory and fast computation, and is prone to forgetting previously learned capability, especially when facing different environments. In this work, we propose Lifelong Learning for Navigation (LLfN) which (1) improves a mobile robot’s navigation behavior purely based on its own experience, and (2) retains the robot’s capability to navigate in previous environments after learning in new ones. LLfN is implemented and tested entirely onboard a physical robot with a limited memory and computation budget.",
