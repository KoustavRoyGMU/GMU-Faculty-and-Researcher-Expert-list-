titles,authors,date,source,descriptions,citations
Empirically revisiting the test independence assumption,"Sai Zhang, Darioush Jalali, Jochen Wuttke, Kıvanç Muşlu, Wing Lam, Michael D Ernst, David Notkin",2014/7/21,Conference Proceedings of the 2014 International Symposium on Software Testing and Analysis," In a test suite, all the test cases should be independent: no test should affect any other test’s result, and running the tests in any order should produce the same test results. Techniques such as test prioritization generally assume that the tests in a suite are independent. Test dependence is a little-studied phenomenon. This paper presents five results related to test dependence. ",169
"Bugs. jar: a large-scale, diverse dataset of real-world java bugs","Ripon K Saha, Yingjun Lyu, Wing Lam, Hiroaki Yoshida, Mukul R Prasad",2018/5/28,Book Proceedings of the 15th International Conference on Mining Software Repositories,"We present Bugs.jar, a large-scale dataset for research in automated debugging, patching, and testing of Java programs. Bugs.jar is comprised of 1,158 bugs and patches, drawn from 8 large, popular open-source Java projects, spanning 8 diverse and prominent application categories. It is an order of magnitude larger than Defects4J, the only other dataset in its class. We discuss the methodology used for constructing Bugs.jar, the representation of the dataset, several use-cases, and an illustration of three of the use-cases through the application of 3 specific tools on Bugs.jar, namely our own tool, Elixir, and two third-party tools, Ekstazi and JaCoCo.",145
iDFlakies: A framework for detecting and partially classifying flaky tests,"Wing Lam, Reed Oei, August Shi, Darko Marinov, Tao Xie",2019/4/22,"Conference 2019 12th ieee conference on software testing, validation and verification (icst)","Regression testing is increasingly important with the wide use of continuous integration. A desirable requirement for regression testing is that a test failure reliably indicates a problem in the code under test and not a false alarm from the test code or the testing infrastructure. However, some test failures are unreliable, stemming from flaky tests that can nondeterministically pass or fail for the same code under test. There are many types of flaky tests, with order-dependent tests being a prominent type. To help advance research on flaky tests, we present (1) a framework, iDFlakies, to detect and partially classify flaky tests; (2) a dataset of flaky tests in open-source projects; and (3) a study with our dataset. iDFlakies automates experimentation with our tool for Maven-based Java projects. Using iDFlakies, we build a dataset of 422 flaky tests, with 50.5% order-dependent and 49.5% not. Our study of these flaky tests finds the …",109
Neural detection of semantic code clones via tree-based convolution,"Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, Qianxiang Wang",2019/5/25,Conference 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC),"Code clones are similar code fragments that share the same semantics but may differ syntactically to various degrees. Detecting code clones helps reduce the cost of software maintenance and prevent faults. Various approaches of detecting code clones have been proposed over the last two decades, but few of them can detect semantic clones, i.e., code clones with dissimilar syntax. Recent research has attempted to adopt deep learning for detecting code clones, such as using tree-based LSTM over Abstract Syntax Tree (AST). However, it does not fully leverage the structural information of code fragments, thereby limiting its clone-detection capability. To fully unleash the power of deep learning for detecting code clones, we propose a new approach that uses tree-based convolution to detect semantic clones, by capturing both the structural information of a code fragment from its AST and lexical information from …",104
Root causing flaky tests in a large-scale industrial setting,"Wing Lam, Patrice Godefroid, Suman Nath, Anirudh Santhiar, Suresh Thummalapenta",2019/7/10,Book Proceedings of the 28th ACM SIGSOFT International Symposium on Software Testing and Analysis,"In today’s agile world, developers often rely on continuous integration pipelines to help build and validate their changes by executing tests in an efficient manner. One of the significant factors that hinder developers’ productivity is flaky tests—tests that may pass and fail with the same version of code. Since flaky test failures are not deterministically reproducible, developers often have to spend hours only to discover that the occasional failures have nothing to do with their changes. However, ignoring failures of flaky tests can be dangerous, since those failures may represent real faults in the production code. Furthermore, identifying the root cause of flakiness is tedious and cumbersome, since they are often a consequence of unexpected and non-deterministic behavior due to various factors, such as concurrency and external dependencies. ",98
Automated test input generation for Android: Are we really there yet in an industrial case?,"Xia Zeng, Dengfeng Li, Wujie Zheng, Fan Xia, Yuetang Deng, Wing Lam, Wei Yang, Tao Xie",2016/11/1,Conference Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering," Given the ever increasing number of research tools to automatically generate inputs to test Android applications (or simply apps), researchers recently asked the question ""Are we there yet?"" (in terms of the practicality of the tools). By conducting an empirical study of the various tools, the researchers found that Monkey (the most widely used tool of this category in industrial practices) outperformed all of the research tools that they studied. In this paper, we present two significant extensions of that study. First, we conduct the first industrial case study of applying Monkey against WeChat, a popular messenger app with over 762 million monthly active users, and report the empirical findings on Monkey's limitations in an industrial setting. Second, we develop a new approach to address major limitations of Monkey and accomplish substantial code-coverage improvements over Monkey, along with empirical insights for …",90
iFixFlakies: A framework for automatically fixing order-dependent flaky tests,"August Shi, Wing Lam, Reed Oei, Tao Xie, Darko Marinov",2019/8/12,Book Proceedings of the 2019 27th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering,"Regression testing provides important pass or fail signals that developers use to make decisions after code changes. However, flaky tests, which pass or fail even when the code has not changed, can mislead developers. A common kind of flaky tests are order-dependent tests, which pass or fail depending on the order in which the tests are run. Fixing order-dependent tests is often tedious and time-consuming. ",77
A study on the lifecycle of flaky tests,"Wing Lam, Kıvanç Muşlu, Hitesh Sajnani, Suresh Thummalapenta",2020/6/27,Book Proceedings of the ACM/IEEE 42nd International Conference on Software Engineering,"During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects.",63
Automated test input generation for Android: Towards getting there in an industrial case,"Haibing Zheng, Dengfeng Li, Beihai Liang, Xia Zeng, Wujie Zheng, Yuetang Deng, Wing Lam, Wei Yang, Tao Xie",2017/5/20,Conference 2017 IEEE/ACM 39th International Conference on Software Engineering: Software Engineering in Practice Track (ICSE-SEIP),"Monkey, a random testing tool from Google, has been popularly used in industrial practices for automatic test input generation for Android due to its applicability to a variety of application settings, e.g., ease of use and compatibility with different Android platforms. Recently, Monkey has been under the spotlight of the research community: recent studies found out that none of the studied tools from the academia were actually better than Monkey when applied on a set of open source Android apps. Our recent efforts performed the first case study of applying Monkey on WeChat, a popular messenger app with over 800 million monthly active users, and revealed many limitations of Monkey along with developing our improved approach to alleviate some of these limitations. In this paper, we explore two optimization techniques to improve the effectiveness and efficiency of our previous approach. We also conduct manual …",54
Understanding Reproducibility and Characteristics of Flaky Tests Through Test Reruns in Java Projects,"Wing Lam, Stefan Winter, Angello Astorga, Victoria Stodden, Darko Marinov",2020/10/12,Conference 2020 IEEE 31st International Symposium on Software Reliability Engineering (ISSRE),"Flaky tests are tests that can non-deterministically pass and fail. They pose a major impediment to regression testing, because they provide an inconclusive assessment on whether recent code changes contain faults or not. Prior studies of flaky tests have proposed tools to detect flaky tests and identified various sources of flakiness in tests, e.g., order-dependent (OD) tests that deterministically fail for some order of tests in a test suite but deterministically pass for some other orders. Several of these studies have focused on OD tests. We focus on an important and under-explored source of flakiness in tests: non-order-dependent tests that can nondeterministically pass and fail even for the same order of tests. Instead of using specialized tools that aim to detect flaky tests, we run tests using the tool configured by the developers. Specifically, we perform our empirical evaluation on Java projects that rely on the Maven …",39
Record and replay for Android: Are we there yet in industrial cases?,"Wing Lam, Zhengkai Wu, Dengfeng Li, Wenyu Wang, Haibing Zheng, Hui Luo, Peng Yan, Yuetang Deng, Tao Xie",2017/8/21,Conference Proceedings of the 2017 11th Joint Meeting on Foundations of Software Engineering," Mobile applications, or apps for short, are gaining popularity. The input sources (e.g., touchscreen, sensors, transmitters) of the smart devices that host these apps enable the apps to offer a rich experience to the users, but these input sources pose testing complications to the developers (e.g., writing tests to accurately utilize multiple input sources together and be able to replay such tests at a later time). To alleviate these complications, researchers and practitioners in recent years have developed a variety of record-and-replay tools to support the testing expressiveness of smart devices. These tools allow developers to easily record and automate the replay of complicated usage scenarios of their app. Due to Android's large share of the smart-device market, numerous record-and-replay tools have been developed using a variety of techniques to test Android apps. To better understand the strengths and weaknesses …",37
A Large-Scale Longitudinal Study of Flaky Tests,"Wing Lam, Stefan Winter, Anjiang Wei, Tao Xie, Darko Marinov, Jonathan Bell",2020,"Conference Proceedings of the ACM SIGPLAN conference on Object-Oriented Programming, Systems, Languages, and Applications","Flaky tests are tests that can non-deterministically pass or fail for the same code version. These tests undermine regression testing efficiency, because developers cannot easily identify whether a test fails due to their recent changes or due to flakiness. Ideally, one would detect flaky tests right when flakiness is introduced, so that developers can then immediately remove the flakiness. Some software organizations, e.g., Mozilla and Netflix, run some tools—detectors—to detect flaky tests as soon as possible. However, detecting flaky tests is costly due to their inherent non-determinism, so even state-of-the-art detectors are often impractical to be used on all tests for each project change. To combat the high cost of applying detectors, these organizations typically run a detector solely on newly added or directly modified tests, i.e., not on unmodified tests or when other changes occur (including changes to the test suite, the …",33
Dependent-test-aware regression testing techniques,"Wing Lam, August Shi, Reed Oei, Sai Zhang, Michael D Ernst, Tao Xie",2020/7/18,Book Proceedings of the 29th ACM SIGSOFT International Symposium on Software Testing and Analysis,"Developers typically rely on regression testing techniques to ensure that their changes do not break existing functionality. Unfortunately, these techniques suffer from flaky tests, which can both pass and fail when run multiple times on the same version of code and tests. One prominent type of flaky tests is order-dependent (OD) tests, which are tests that pass when run in one order but fail when run in another order. Although OD tests may cause flaky-test failures, OD tests can help developers run their tests faster by allowing them to share resources. We propose to make regression testing techniques dependent-test-aware to reduce flaky-test failures. ",31
When tests collide: Evaluating and coping with the impact of test dependence,"Wing Lam, Sai Zhang, Michael D Ernst",2015/3,"Journal University of Washington Department of Computer Science and Engineering, Tech. Rep","In a test suite, all the test cases should be independent: no test should affect any other test’s result, and running the tests in any order should produce the same test results. The assumption of test independence is important so that tests behave consistently as designed. In addition, many downstream testing techniques, including test prioritization, test selection, and test parallelization, assume test independence. However, this critical assumption often does not hold in practice.",26
Probabilistic and Systematic Coverage of Consecutive Test-Method Pairs for Detecting Order-Dependent Flaky Tests,"Anjiang Wei, Pu Yi, Tao Xie, Darko Marinov, Wing Lam",2021,"Conference Tools and Algorithms for the Construction and Analysis of Systems: 27th International Conference, TACAS 2021, Held as Part of the European Joint Conferences on Theory and Practice of Software, ETAPS 2021, Luxembourg City, Luxembourg, March 27–April 1, 2021, Proceedings, Part I 27","Software developers frequently check their code changes by running a set of tests against their code. Tests that can nondeterministically pass or fail when run on the same code version are called flaky tests. These tests are a major problem because they can mislead developers to debug their recent code changes when the failures are unrelated to these changes. One prominent category of flaky tests is order-dependent (OD) tests, which can deterministically pass or fail depending on the order in which the set of tests are run. By detecting OD tests in advance, developers can fix these tests before they change their code. Due to the high cost required to explore all possible orders (n! permutations for n tests), prior work has developed tools that randomize orders to detect OD tests. Experiments have shown that randomization can detect many OD tests, and that most OD tests depend on just one other test to …",12
An infrastructure approach to improving effectiveness of Android UI testing tools,"Wenyu Wang, Wing Lam, Tao Xie",2021/7/11,Book Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis,"Due to the importance of Android app quality assurance, many Android UI testing tools have been developed by researchers over the years. However, recent studies show that these tools typically achieve low code coverage on popular industrial apps. In fact, given a reasonable amount of run time, most state-of-the-art tools cannot even outperform a simple tool, Monkey, on popular industrial apps with large codebases and sophisticated functionalities. Our motivating study finds that these tools perform two types of operations, UI Hierarchy Capturing (capturing information about the contents on the screen) and UI Event Execution (executing UI events, such as clicks), often inefficiently using UIAutomator, a component of the Android framework. In total, these two types of operations use on average 70% of the given test time. ",11
A Characteristic Study of Parameterized Unit Tests in. NET Open Source Projects,"Wing Lam, Siwakorn Srisakaokul, Blake Bassett, Peyman Mahdian, Tao Xie, Pratap Lakshman, Jonathan de Halleux",2018,Conference 32nd European Conference on Object-Oriented Programming (ECOOP 2018),"In the past decade, parameterized unit testing has emerged as a promising method to specify program behaviors under test in the form of unit tests. Developers can write parameterized unit tests (PUTs), unit-test methods with parameters, in contrast to conventional unit tests, without parameters. The use of PUTs can enable powerful test generation tools such as Pex to have strong test oracles to check against, beyond just uncaught runtime exceptions. In addition, PUTs have been popularly supported by various unit testing frameworks for .NET and the JUnit framework for Java. However, there exists no study to offer insights on how PUTs are written by developers in either proprietary or open source development practices, posing barriers for various stakeholders to bring PUTs to widely adopted practices in software industry. To fill this gap, we first present categorization results of the Microsoft MSDN Pex Forum posts (contributed primarily by industrial practitioners) related to PUTs. We then use the categorization results to guide the design of the first characteristic study of PUTs in .NET open source projects. We study hundreds of PUTs that open source developers wrote for these open source projects. Our study findings provide valuable insights for various stakeholders such as current or prospective PUT writers (eg, developers), PUT framework designers, test-generation tool vendors, testing researchers, and testing educators.",9
iPFlakies: A Framework for Detecting and Fixing Python Order-Dependent Flaky Tests,"Ruixin Wang, Yang Chen, Wing Lam",2022,"Description Developers typically run tests after code changes. Flaky tests, which are tests that can nondeterministically pass and fail when run on the same version of code, can mislead developers about their recent changes. Much of the prior work on flaky tests is focused on Java projects. One prominent category of flaky tests that the prior work focused on is order-dependent (OD) tests, which are tests that pass or fail depending on the order in which tests are run. For example, our prior work proposed using other tests in the test suite to fix (or correctly set up) the state needed by Java OD tests to pass. Unlike Java flaky tests, flaky tests in other programming languages have received less attention. To help with this problem, another piece of prior work recently studied flaky tests in Python projects and detected many OD tests. Unfortunately, the work did not identify the other tests in the test suites that can be used to fix the OD tests …","Developers typically run tests after code changes. Flaky tests, which are tests that can nondeterministically pass and fail when run on the same version of code, can mislead developers about their recent changes. Much of the prior work on flaky tests is focused on Java projects. One prominent category of flaky tests that the prior work focused on is order-dependent (OD) tests, which are tests that pass or fail depending on the order in which tests are run. For example, our prior work proposed using other tests in the test suite to fix (or correctly set up) the state needed by Java OD tests to pass.",2
Repairing test dependence,Wing Lam,2016/11/1,Conference Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"In a test suite, all the tests should be independent: no test should affect another test's result, and running the tests in any order should yield the same test results. The assumption of such test independence is important so that tests behave consistently as designed. However, this critical assumption often does not hold in practice due to test dependence. ",2
Finding Polluter Tests Using Java PathFinder,"Pu Yi, Anjiang Wei, Wing Lam, Tao Xie, Darko Marinov",2021/7/21,Journal ACM SIGSOFT Software Engineering Notes,"Tests that modify (i.e., ""pollute"") the state shared among tests in a test suite are called \polluter tests"". Finding these tests is im- portant because they could result in di erent test outcomes based on the order of the tests in the test suite. Prior work has proposed the PolDet technique for nding polluter tests in runs of JUnit tests on a regular Java Virtual Machine (JVM). Given that Java PathFinder (JPF) provides desirable infrastructure support, such as systematically exploring thread schedules, it is a worthwhile attempt to re-implement techniques such as PolDet in JPF. We present a new implementation of PolDet for nding polluter tests in runs of JUnit tests in JPF. We customize the existing state comparison in JPF to support the so-called \common-root iso- morphism"" required by PolDet. We find that our implementation is simple, requiring only -200 lines of code, demonstrating that JPF is a sophisticated infrastructure for …",1
"Detecting, characterizing, and taming flaky tests",Wing Lam,2021/7/12,Institution University of Illinois at Urbana-Champaign,"As software evolves, developers typically perform regression testing to ensure that their code changes do not break existing functionalities. During regression testing, developers can waste time debugging their code changes because of spurious failures from flaky tests, which are tests that non-deterministically pass or fail on the same code. These spurious failures mislead developers about their code changes because the failures are often due to bugs that existed before the code changes. One prominent category of flaky tests is order-dependent (OD) flaky tests. Each OD test has at least one order in which the test passes and another order in which the test fails, and for every test order, the test either passes or fails in all runs of that test order. Another prominent category is async-wait (AW) flaky tests. Each AW test makes at least one asynchronous call and passes if the asynchronous call finishes on time but fails if the call finishes too early or too late. This dissertation tackles three main aspects of flaky tests. First, this dissertation presents novel techniques to detect flaky tests so that developers can preemptively prevent the problem of flaky tests from affecting their regression testing results. Second, this dissertation presents novel techniques to characterize flaky tests to help developers better understand their flaky tests and to help researchers invent new solutions to the flaky-test problem. Lastly, this dissertation presents novel techniques to tame the problem of flaky tests by accommodating the flakiness so that flaky tests do not mislead developers during regression testing. For detecting flaky tests, this dissertation presents (1) iDFlakies, a …",
Neural detection of semantic code clones via tree-based convolution. In 2019 IEEE/ACM 27th International Conference on Program Comprehension (ICPC),"Hao Yu, Wing Lam, Long Chen, Ge Li, Tao Xie, Qianxiang Wang",2019,Pages 70-80,,
Dependent-Test-Aware Regression Testing Techniques,,,,,
