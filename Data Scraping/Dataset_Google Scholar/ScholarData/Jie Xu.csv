titles,authors,date,source,descriptions,citations
Simulation optimization: A review and exploration in the new era of cloud computing and big data,"Jie Xu, Edward Huang, Chun-Hung Chen, Loo Hay Lee",2015/6/21,Source Asia-Pacific Journal of Operational Research,"Recent advances in simulation optimization research and explosive growth in computing power have made it possible to optimize complex stochastic systems that are otherwise intractable. In the first part of this paper, we classify simulation optimization techniques into four categories based on how the search is conducted. We provide tutorial expositions on representative methods from each category, with a focus in recent developments, and compare the strengths and limitations of each category. In the second part of this paper, we review applications of simulation optimization in various contexts, with detailed discussions on health care, logistics, and manufacturing systems. Finally, we explore the potential of simulation optimization in the new era. Specifically, we discuss how simulation optimization can benefit from cloud computing and high-performance computing, its integration with big data analytics, and the …",209
Industrial strength COMPASS: A comprehensive algorithm and software for optimization via simulation,"Jie Xu, Barry L Nelson, Jeff L Hong",2010/2/8,Journal ACM Transactions on Modeling and Computer Simulation (TOMACS),"Industrial Strength COMPASS (ISC) is a particular implementation of a general framework for optimizing the expected value of a performance measure of a stochastic simulation with respect to integer-ordered decision variables in a finite (but typically large) feasible region defined by linear-integer constraints. The framework consists of a global-search phase, followed by a local-search phase, and ending with a “clean-up” (selection of the best) phase. Each phase provides a probability 1 convergence guarantee as the simulation effort increases without bound: Convergence to a globally optimal solution in the global-search phase; convergence to a locally optimal solution in the local-search phase; and convergence to the best of a small number of good solutions in the clean-up phase. In practice, ISC stops short of such convergence by applying an improvement-based transition rule from the global phase to the local …",188
A comparison study of validity indices on swarm-intelligence-based clustering,"Rui Xu, Jie Xu, Donald C Wunsch",2012/3/15,"Journal IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)","Swarm intelligence has emerged as a worthwhile class of clustering methods due to its convenient implementation, parallel capability, ability to avoid local minima, and other advantages. In such applications, clustering validity indices usually operate as fitness functions to evaluate the qualities of the obtained clusters. However, as the validity indices are usually data dependent and are designed to address certain types of data, the selection of different indices as the fitness functions may critically affect cluster quality. Here, we compare the performances of eight well-known and widely used clustering validity indices, namely, the Caliński–Harabasz index, the    index, the Davies–Bouldin index, the Dunn index with two of its generalized versions, the    index, and the silhouette statistic index, on both synthetic and real data sets in the framework of differential-evolution–particle-swarm-optimization (DEPSO)-based …",138
Simulation optimization in the era of Industrial 4.0 and the Industrial Internet,"Jie Xu, Edward Huang, Liam Hsieh, Loo Hay Lee, Qing-Shan Jia, Chun-Hung Chen",2016/11/1,Journal Journal of Simulation,"Simulation is an established tool for predicting and evaluating the performance of complex stochastic systems that are analytically intractable. Recent research in simulation optimization and explosive growth in computing power have made it feasible to use simulations to optimize the design and operations of systems directly. Concurrently, ubiquitous sensing, pervasive computing, and unprecedented systems interconnectivity have ushered in a new era of industrialization (the so-called Industrial 4.0/Industrial Internet). In this article, we argue that simulation optimization is a decision-making tool that can be applied to many scenarios to tremendous effect. By capitalizing on an unprecedented integration of sensing, computing, and control, simulation optimization provides the “smart brain” required to drastically improve the efficiency of industrial systems. We explore the potential of simulation optimization and discuss …",132
An adaptive hyperbox algorithm for high-dimensional discrete optimization via simulation problems,"Jie Xu, Barry L Nelson, L Jeff Hong",2013/2,Journal INFORMS Journal on Computing,"We propose an adaptive hyperbox algorithm (AHA), which is an instance of a locally convergent, random search algorithm for solving discrete optimization via simulation problems. Compared to the COMPASS algorithm, AHA is more efficient in high-dimensional problems. By analyzing models of the behavior of COMPASS and AHA, we show why COMPASS slows down significantly as dimension increases, whereas AHA is less affected. Both AHA and COMPASS can be used as the local search algorithm within the Industrial Strength COMPASS framework, which consists of a global search phase, a local search phase, and a final cleanup phase. We compare the performance of AHA to COMPASS within the framework of Industrial Strength COMPASS and as stand-alone algorithms. Numerical experiments demonstrate that AHA scales up well in high-dimensional problems and has similar performance to …",92
MO2TOS: Multi-Fidelity Optimization with Ordinal Transformation and Optimal Sampling,"Jie Xu, Si Zhang, Edward Huang, Chun-Hung Chen, Loo Hay Lee, Nurcin Celik",2016/6/20,Journal Asia-Pacific Journal of Operational Research,"Simulation optimization can be used to solve many complex optimization problems in automation applications such as job scheduling and inventory control. We propose a new framework to perform efficient simulation optimization when simulation models with different fidelity levels are available. The framework consists of two novel methodologies: ordinal transformation (OT) and optimal sampling (OS). The OT methodology uses the low-fidelity simulations to transform the original solution space into an ordinal space that encapsulates useful information from the low-fidelity model. The OS methodology efficiently uses high-fidelity simulations to sample the transformed space in search of the optimal solution. Through theoretical analysis and numerical experiments, we demonstrate the promising performance of the multi-fidelity optimization with ordinal transformation and optimal sampling (MO2TOS) framework.",87
Optimal computing budget allocation for particle swarm optimization in stochastic optimization,"Si Zhang, Jie Xu, Loo Hay Lee, Ek Peng Chew, Wai Peng Wong, Chun-Hung Chen",2016/7/18,Journal IEEE Transactions on Evolutionary Computation,"Particle swarm optimization (PSO) is a popular metaheuristic for deterministic optimization. Originated in the interpretations of the movement of individuals in a bird flock or fish school, PSO introduces the concept of personal best and global best to simulate the pattern of searching for food by flocking and successfully translate the natural phenomena to the optimization of complex functions. Many real-life applications of PSO cope with stochastic problems. To solve a stochastic problem using PSO, a straightforward approach is to equally allocate computational effort among all particles and obtain the same number of samples of fitness values. This is not an efficient use of computational budget and leaves considerable room for improvement. This paper proposes a seamless integration of the concept of optimal computing budget allocation into PSO to improve the computational efficiency of PSO for stochastic …",64
A simulation budget allocation procedure for enhancing the efficiency of optimal subset selection,"Si Zhang, Loo Hay Lee, Ek Peng Chew, Jie Xu, Chun-Hung Chen",2015/4/17,Journal IEEE Transactions on Automatic Control,"Selecting the optimal subset is highly beneficial to numerous developments in simulation optimization. This paper studies the problem of maximizing the probability of correctly selecting the top-  designs out of  designs under a computing budget constraint. We develop a new procedure which is more efficient and robust than currently existing procedures in the literature. We also provide an analysis on its asymptotic convergence rate. Based on this analysis, we show that our new procedure achieves a higher convergence rate than other procedures under certain conditions. Numerical testing supports our analytical analysis and shows that the new procedure is significantly more efficient and robust.",64
Discrete optimization via simulation,"L Jeff Hong, Barry L Nelson, Jie Xu",2015,Journal Handbook of simulation optimization,"This chapter describes tools and techniques that are useful for optimization via simulation—maximizing or minimizing the expected value of a performance measure of a stochastic simulation—when the decision variables are discrete. Ranking and selection, globally and locally convergent random search and ordinal optimization are covered, along with a collection of “enhancements” that may be applied to many different discrete optimization via simulation algorithms. We also provide strategies for using commercial solvers.",60
Efficient multi-fidelity simulation optimization,"Jie Xu, Si Zhang, Edward Huang, Chun-Hung Chen, Loo Hay Lee, Nurcin Celik",2014/12/7,Book Proceedings of the 2014 winter simulation conference,"Simulation models of different fidelity levels are often available for a complex system. High-fidelity simulations are accurate but time-consuming. Therefore, they can only be applied to a small number of solutions. Low-fidelity simulations are faster and can evaluate a large number of solutions. But their results may contain significant bias and variability. We propose an Multi-fidelity Optimization with Ordinal Transformation and Optimal Sampling (MO2TOS) framework to exploit the benefits of high- and low-fidelity simulations to efficiently identify a (near) optimal solution. MO2TOS uses low-fidelity simulations for all solutions and then assigns a fixed budget of high-fidelity simulations to solutions based on low-fidelity simulation results. We show the benefits of MO2TOS via theoretical analysis and numerical experiments with deterministic simulations and stochastic simulations where noise is negligible with sufficient …",49
A metamodel-based Monte Carlo simulation approach for responsive production planning of manufacturing systems,"Minqi Li, Feng Yang, Reha Uzsoy, Jie Xu",2016/1/1,Journal Journal of Manufacturing Systems,"Production planning is concerned with finding a release plan of jobs into a manufacturing system so that its actual outputs over time match the customer demand with the least cost. For a given release plan, the system outputs, work in process inventory (WIP) levels and job completions, are non-stationary bivariate time series that interact with time series representing customer demand, resulting in the fulfillment/non-fulfillment of demand and the holding cost of both WIP and finished-goods inventory. The relationship between a release plan and its resulting performance metrics (typically, mean/variance of the total cost and the fill rate) has proven difficult to quantify. This work develops a metamodel-based Monte Carlo simulation (MCS) method to accurately capture the dynamic, stochastic behavior of a manufacturing system, and to allow real-time evaluation of a release plan's performance metrics. This evaluation …",46
Analytics with digital-twinning: A decision support system for maintaining a resilient port,"Chenhao Zhou, Jie Xu, Elise Miller-Hooks, Weiwen Zhou, Chun-Hung Chen, Loo Hay Lee, Ek Peng Chew, Haobin Li",2021/4/1,Journal Decision Support Systems,"In this paper, a Decision Support System (DSS) with digital twinning-based resilience analysis is proposed as a modern tool for port resilience computation and updating. The proposed DSS assesses the resilience of a port under possible disruptive events given its design, operations and potential pre-defined post-event recovery actions to mitigate the impact of the disruption. Digital twinning provides the fidelity required to realistically predict port performance with taken post-event recovery actions under various possible disruptive events. In addition to hedging against impacts from probabilistically known disruption events, this approach also enables inclusion of ordinary operational uncertainties within the resilience evaluation. This is not generally feasible with other existing resilience quantification approaches. To tackle computational challenges of applying a digital twin for real-world size applications, an optimal …",35
A new particle swarm optimization algorithm for noisy optimization problems,"Sajjad Taghiyeh, Jie Xu",2016/9,Journal Swarm Intelligence,"We propose a new particle swarm optimization algorithm for problems where objective functions are subject to zero-mean, independent, and identically distributed stochastic noise. While particle swarm optimization has been successfully applied to solve many complex deterministic nonlinear optimization problems, straightforward applications of particle swarm optimization to noisy optimization problems are subject to failure because the noise in objective function values can lead the algorithm to incorrectly identify positions as the global/personal best positions. Instead of having the entire swarm follow a global best position based on the sample average of objective function values, the proposed new algorithm works with a set of statistically global best positions that include one or more positions with objective function values that are statistically equivalent, which is achieved using a combination of statistical …",35
Clustering with differential evolution particle swarm optimization,"Rui Xu, Jie Xu, Donald C Wunsch",2010/7/18,Conference IEEE Congress on Evolutionary Computation,"The applications of recently developed meta-heuristics in cluster analysis, such as particle swarm optimization (PSO) and differential evolution (DE), have increasingly attracted attention and popularity in a wide variety of communities owing to their effectiveness in solving complicated combinatorial optimization problems. Here, we propose to use a hybrid of PSO and DE, known as differential evolution particle swarm optimization (DEPSO), in order to further improve search capability and achieve higher flexibility in exploring the natural while hidden data structures of data of interest. Empirical results show that the DEPSO-based clustering algorithm achieves better performance in terms of the number of epochs required to reach a pre-specified cutoff value of the fitness function than either of the other approaches used. Further experimental studies on both synthetic and real data sets demonstrate the effectiveness of …",35
Speeding up COMPASS for high-dimensional discrete optimization via simulation,"L Jeff Hong, Barry L Nelson, Jie Xu",2010/11/1,Journal Operations Research Letters,"The convergent optimization via most promising area stochastic search (COMPASS) algorithm is a locally convergent random search algorithm for solving discrete optimization via simulation problems. COMPASS has drawn a significant amount of attention since its introduction. While the asymptotic convergence of COMPASS does not depend on the problem dimension, the finite-time performance of the algorithm often deteriorates as the dimension increases. In this paper, we investigate the reasons for this deterioration and propose a simple change to the solution-sampling scheme that significantly speeds up COMPASS for high-dimensional problems without affecting its convergence guarantee.",34
Multi-fidelity model integration for engineering design,"Edward Huang, Jie Xu, Si Zhang, Chun-Hung Chen",2015/1/1,Journal Procedia Computer Science,"Systems engineers have long been using analytical and computational models to approximately predict the behaviors of systems and use these models to guide the design of engineered systems. However, until now, systems engineers are still forced to compromise on model fidelity, modeling accuracy, and the optimality of the selected design alternatives through the whole engineering design process. Although high-fidelity models can provide an accurate estimation, running these models is usually time-consuming. On the other hand, low-fidelity models are much faster, but their results may contain significant bias and variability. In this research, we propose a Multi-fidelity Optimization with Ordinal Transformation and Optimal Sampling (MO2TOS) framework to integrate both high- and low-fidelity model to efficiently identify a (near) optimal design. The proposed framework uses low-fidelity models to evaluate all …",33
An efficient simulation budget allocation method incorporating regression for partitioned domains,"Mark W Brantley, Loo Hay Lee, Chun-Hung Chen, Jie Xu",2014/5/1,Journal Automatica,"Simulation can be a very powerful tool to help decision making in many applications but exploring multiple courses of actions can be time consuming. Numerous ranking and selection (R&S) procedures have been developed to enhance the simulation efficiency of finding the best design. To further improve efficiency, one approach is to incorporate information from across the domain into a regression equation. However, the use of a regression metamodel also inherits some typical assumptions from most regression approaches, such as the assumption of an underlying quadratic function and the simulation noise is homogeneous across the domain of interest. To extend the limitation while retaining the efficiency benefit, we propose to partition the domain of interest such that in each partition the mean of the underlying function is approximately quadratic. Our new method provides approximately optimal rules for …",28
Realization of multiwavelength label optical packet switching,"Shilin Xiao, Qingji Zeng, Jianxin Wang, Jie Xu, Yun Wang",2003/4/2,Journal IEEE photonics technology letters,"The realization of a new kind of multiwavelength label optical packet switching is discussed. In this switching, an optical header is labeled by several optical pulses at different wavelengths in the same optical communication channel band as optical payload. The optical transmitter with header generation and packet formation, receiver with data restoration and switching node with route processing are introduced. The switching principle is verified by a simplified experiment.",26
Optimal selection of media vehicles using customer databases,"Edward C Malthouse, Dingxi Qiu, Jie Xu",2012/12/1,Journal Expert systems with applications,"This paper investigates the problem where an organization must select among multiple media vehicles for a marketing campaign, and determine how many names from each vehicle to impress (the contact depth). The organization can estimate the return from contacting each prospective customer, and this return decreases as depth increases. Different vehicles have different marginal costs per impression, and may have minimum-spend requirements or activation costs to use the vehicle. Decisions are to be made to maximize profit subject to a constraint on the total amount spent. We proposed an optimization model and two computationally efficient methods that often lead to global optimal solutions under practical assumptions. The model is illustrated with two data sets.",22
Dynamic data driven application systems for smart cities and urban infrastructures,"Richard M Fujimoto, Nurcin Celik, Haluk Damgacioglu, Michael Hunter, Dong Jin, Young-Jun Son, Jie Xu",2016/12/11,Conference 2016 Winter Simulation Conference (WSC),The smart cities vision relies on the use of information and communication technologies to efficiently manage and maximize the utility of urban infrastructures and municipal services in order to improve the quality of life of its inhabitants. Many aspects of smart cities are dynamic data driven application systems (DDDAS) where data from sensors monitoring the system are used to drive computations that in turn can dynamically adapt and improve the monitoring process as the city evolves. Several leading DDDAS researchers offer their views concerning the DDDAS paradigm applied to realizing smart cities and outline research challenges that lie ahead.,21
An effective learning procedure for multi-fidelity simulation optimization with ordinal transformation,"Ruidi Chen, Jie Xu, Si Zhang, Chun-Hung Chen, Loo Hay Lee",2015/8/24,Conference 2015 IEEE International Conference on Automation Science and Engineering (CASE),"Simulation models of different fidelity levels are often available for the same complex system. High-fidelity models generate accurate measurements of the performance of a system design but can only be simulated for a very limited number of designs due to its prohibitively expensive computation cost. In contrast, low-fidelity models produce approximate estimates of the objective function but are lightweight and can evaluate a large number of designs in a short amount of time. Ordinal transformation is a novel framework that combines the merits of high- and low-fidelity simulation models to perform efficient optimization. In this paper, we propose an effective learning procedure that improves the prediction accuracy of low-fidelity models. Numerical experiment demonstrates the promising performance of learning within the ordinal transformation framework.",20
MicroRNA expression profile based cancer classification using Default ARTMAP,"Rui Xu, Jie Xu, Donald C Wunsch II",2009/7/1,Journal Neural Networks,"High-throughput messenger RNA (mRNA) expression profiling with microarray has been demonstrated as a more effective method of cancer diagnosis and treatment than the traditional morphology or clinical parameter based methods. Recently, the discovery of a category of small non-coding RNAs, named microRNAs (miRNAs), provides another promising method of cancer classification. miRNAs play a critical role in the tumorigenic process by functioning either as oncogenes or as tumor suppressors. Here, we apply a neural based classifier, Default ARTMAP, to classify broad types of cancers based on their miRNA expression fingerprints. As the miRNA expression data usually have high dimensionalities, particle swarm optimization (PSO) is used for selecting important miRNAs that contribute to the discrimination of different cancer types. Experimental results on the multiple human cancers show that Default …",20
An ordinal transformation framework for multi-fidelity simulation optimization,"Jie Xu, Si Zhang, Edward Huang, Chun-Hung Chen, Loo Hay Lee, Nurcin Celik",2014/8/18,Conference 2014 IEEE International Conference on Automation Science and Engineering (CASE),"Simulation models of different levels of fidelity are often available for evaluating alternative solutions of a complex system. High-fidelity simulations generate accurate predictions but can be very time-consuming to run. Therefore, they can only be applied to a small number of solutions. Low-fidelity simulations are much faster and can evaluate a large number of solutions. But simulation results may contain significant bias and variability. We propose a novel ordinal transformation framework to exploit the benefits of both high- and low-fidelity simulation models to efficiently identify a (near) optimal solution. A two-stage simulation optimization method under the ordinal transformation framework is described. Through preliminary theoretical analysis and numerical experiments, we demonstrate the promising performance of ordinal transformation, which opens up a new and potentially fruitful research avenue.",19
A novel performance preserving VM splitting and assignment scheme,"Liu Liu, Jie Xu, Hongfang Yu, Lemin Li, Chunming Qiao",2014/6/10,Conference 2014 IEEE International Conference on Communications (ICC),"Server consolidation schemes whereby each server is replaced with a virtual machine (VM) and multiple such VMs are run on a single physical server can reduce the number of physical servers needed, and in turn, both the cost and energy consumption in datacenters. However, existing schemes have not fully exploited the flexibility in the usage and allocation of virtualization resources, so as to allow one application originally deployed on a single large VM (LVM) to be split and hosted by multiple smaller VMs (SVM). Using multiple SVM instead of a LVM enables resource allocation at a smaller granularity and hence may further increase the utilization and reduce the number of physical servers. However, a major challenge to be overcome when deploying multiple SVMs for one application is to preserve the performance of the application in terms of e.g., response delay. In this paper, we show through experiment …",17
Semiconductor wafer fabrication production planning using multi-fidelity simulation optimisation,"Fan Zhang, Jie Song, Yingzhuo Dai, Jie Xu",2020/11/1,Journal International Journal of Production Research,"Semiconductor wafer fabrication is a complicated and time-consuming production process in the semiconductor manufacturing industry. It is very important for the manufacturer to come up with production plans that can most efficiently utilise the manufacturing equipment and fulfil customer orders placed in a planning horizon. Because of the complexity of the manufacturing processes, it is necessary to use high-fidelity discrete-event simulations to provide accurate estimates of delivery lead time for any given production plan. However, high-fidelity simulations are time-consuming, and thus decision-makers may only evaluate a small number of production plans once customer orders are received. In this paper, we propose the use of a multi-fidelity simulation optimisation approach to efficiently evaluate and select the best production plan from a large set of alternative plans under consideration. We develop an open …",16
Efficient discrete optimization via simulation using stochastic kriging,Jie Xu,2012/12/9,Conference Proceedings of the 2012 winter simulation conference (wsc),"We propose to use a global metamodeling technique known as stochastic kriging to improve the efficiency of Discrete Optimization-via-Simulation (DOvS) algorithms. Stochastic kriging metamodel allows the DOvS algorithm to utilize all information collected during the optimization process and identify solutions that are most likely to lead to significant improvement in solution quality. We call the approach Stochastic Kriging for OPtimization Efficiency (SKOPE). In this paper, we integrate SKOPE with a locally convergent DOvS algorithm known as Adaptive Hyperbox Algorithm (AHA). Numerical experiments show that SKOPE significantly improves the performance of AHA in the early stage of optimization, which is very helpful for DOvS applications where the number of simulations for an optimization task is severely limited due to a short decision time window and time-consuming simulation.",16
Efficient simulation sampling allocation using multifidelity models,"Yijie Peng, Jie Xu, Loo Hay Lee, Jianqiang Hu, Chun-Hung Chen",2018/12/9,Journal IEEE Transactions on Automatic Control,"Simulation is often used to estimate the performance of alternative system designs for selecting the best. For a complex system, high-fidelity simulation is usually time-consuming and expensive. In this paper, we provide a new framework that integrates information from the multifidelity models to increase efficiency for selecting the best. A Gaussian mixture model is introduced to capture performance clustering information in the multifidelity models. Posterior information obtained by a clustering analysis incorporates both cluster-wise information and idiosyncratic information for each design. We propose a new budget allocation method to efficiently allocate high-fidelity simulation replications, utilizing posterior information. Numerical experiments show that the proposed multifidelity framework achieves a significant boost in efficiency.",15
Balancing search and estimation in random search based stochastic simulation optimization,"Chenbo Zhu, Jie Xu, Chun-Hung Chen, Loo Hay Lee, Jian-Qiang Hu",2016/1/26,Journal IEEE Transactions on Automatic control,"Stochastic simulation optimization involves two fundamental steps: 1) searching the solution space to generate candidate solutions for comparison and 2) estimating the performance of each candidate solution via multiple simulations and selecting a solution as the best solution found. Comparisons of solutions via simulation estimation are subject to error due to the stochastic noise in simulation output. While estimation errors can be reduced by increasing the number of simulation replications, it would in turn limit the number of candidate solutions that can be generated for comparison in a fixed computation budget. Under a random search framework, we derive an analytical formula to (approximately) optimally determine the number of candidate solutions generated in the search step and simulation replications in the estimation step to maximize the quality of the solution selected as the best by the random search …",14
Dynamic traffic grooming in interconnected WDM SDH/SONET rings,"Jie Xu, Qingji Zeng",2001/7/26,"Conference Technologies, Protocols, and Services for Next-Generation Internet","Recently, wavelength division multiplexing (WDM) technology has been widely employed to increase the capacity of existing SDH/SONET self-healing rings. As wavelengths are no longer such precious resources, SDH/SONET add-drop multiplexers (ADMs) become the dominant cost factor in network deployment. Traffic grooming studies how to intelligently arranging the placement of ADMs on wavelengths to reduce the number of ADMs required to support certain traffic patterns. In this paper, we address dynamic traffic grooming in interconnected WDM unidirectional rings. An optically interconnected dual-homing strategy is adopted to implement optical layer survivability. A genetic algorithm (GA) based approach is proposed for static traffic grooming. Another GA based approach is proposed for combining different topologies to support a given traffic set. Numerical results were reported on ADM savings.",13
Mitigating cascading outages in severe weather using simulation-based optimization,"Jie Xu, Rui Yao, Feng Qiu",2020/7/10,Journal IEEE Transactions on Power Systems,"Severe weather events can trigger cascading power outages and lead to significant losses. In this work, we investigate cascading outage mitigation under severe weather conditions. Given day-ahead weather forecasts and component failure models, we aim to identify a set of power lines that can be hardened to minimize the expected impact of potential cascading outages. Since the expected load shedding cannot be expressed as an explicit function of line hardening decisions and system states, we developed a cascading outage simulator to estimate the expected value of load shedding under various initial weather-related disruption scenarios generated using a weather forecast. To avoid massive enumeration of all possible combinations of line hardening decisions and reduce the simulation efforts, we employed an efficient simulation-based optimization approach that quickly identifies the (near) optimal line …",11
Generalized ordinal learning framework (golf) for decision making with future simulated data,"Giulia Pedrielli, K Selcuk Candan, Xilun Chen, Logan Mathesen, Alireza Inanalouganji, Jie Xu, Chun-Hung Chen, Loo Hay Lee",2019/12/15,Journal Asia-Pacific Journal of Operational Research,"Real-time decision making has acquired increasing interest as a means to efficiently operating complex systems. The main challenge in achieving real-time decision making is to understand how to develop next generation optimization procedures that can work efficiently using: (i) real data coming from a large complex dynamical system, (ii) simulation models available that reproduce the system dynamics. While this paper focuses on a different problem with respect to the literature in RL, the methods proposed in this paper can be used as a support in a sequential setting as well. The result of this work is the new Generalized Ordinal Learning Framework (GOLF) that utilizes simulated data interpreting them as low accuracy information to be intelligently collected offline and utilized online once the scenario is revealed to the user. GOLF supports real-time decision making on complex dynamical systems once a specific …",11
An importance sampling method for portfolio CVaR estimation with Gaussian copula models,"Pu Huang, Dharmashankar Subramanian, Jie Xu",2010/12/5,Conference Proceedings of the 2010 Winter Simulation Conference,We developed an importance sampling method to estimate Conditional Value-at-Risk for portfolios in which inter-dependent asset losses are modeled via a Gaussian copula model. Our method constructs an importance sampling distribution by shifting the latent variables of the Gaussian copula and thus can handle arbitrary marginal asset distributions. It admits an intuitive geometric explanation and is easy to implement. We also present numerical experiments that confirm its superior performance compared to the naive approach.,11
Real-time digital twin-based optimization with predictive simulation learning,"Travis Goodwin, Jie Xu, Nurcin Celik, Chun-Hung Chen",2022/3/7,Journal Journal of Simulation,"Digital twinning presents an exciting opportunity enabling real-time optimization of the control and operations of cyber-physical systems (CPS) with data-driven simulations, while facing prohibitive computational burdens. This paper introduces a method, Sequential Allocation using Machine-learning Predictions as Light-weight Estimates (SAMPLE) to address this computational challenge by leveraging machine learning models trained off-line in a predictive simulation learning setting prior to a real-time decision. SAMPLE integrates machine learning predictions with data generated by real-time execution of a digital twin in a rigorous yet flexible way, and optimally guides the digital twin simulation to achieve the computational efficiency required for real-time decision-making in a CPS. Numerical experiments demonstrate the viability of SAMPLE to select optimal decisions in real-time for CPS control and operations …",10
A reinforcement learning based system for minimizing cloud storage service cost,"Haoyu Wang, Haiying Shen, Qi Liu, Kevin Zheng, Jie Xu",2020/8/17,Book Proceedings of the 49th International Conference on Parallel Processing,"Currently, many web applications are deployed on cloud storage service provided by cloud service providers (CSPs). A CSP offers different types of storage including hot, cold and archive storage and sets unit prices for these different types, which vary substantially. By properly assigning the data files of a web application to different types of storage based on their usage profiles and the CSP’s pricing policy, a cloud customer potentially can achieve substantial cost savings and minimize the payment to the CSP. However, no previous research handles this problem. Towards this goal, we present a Markov Decision Process formulation for the cost minimization problem, and then develop a reinforcement learning based approach to effectively solve the problem, which changes the type of storage of each data file periodically to minimize money cost in long term. We then propose a method to aggregate concurrently …",10
Multi-fidelity sampling for efficient simulation-based decision making in manufacturing management,"Jie Song, Yunzhe Qiu, Jie Xu, Feng Yang",2019/7/3,Journal IISE Transactions,"Today’s manufacturers operate in highly dynamic and uncertain market environments. Process-level disturbances present further challenges. Consequently, it is of strategic importance for a manufacturing company to develop robust manufacturing capabilities that can quickly adapt to varying customer demands in the presence of external and internal uncertainty and stochasticity. Discrete-event simulations have been used by manufacturing managers to conduct “look-ahead” analysis and optimize resource allocation and production plan. However, simulations of complex manufacturing systems are time-consuming. Therefore, there is a great need for a highly efficient procedure to allocate a limited number of simulations to improve a system’s performance. In this article, we propose a multi-fidelity sampling algorithm that greatly increases the efficiency of simulation-based robust manufacturing management by …",10
VMSA: a performance preserving online VM splitting and placement algorithm in dynamic cloud environments,"Liu Liu, Jie Xu, Hongfang Yu, Lemin Li, Chunming Qiao",2016/8,Journal The Journal of Supercomputing,"Server consolidation schemes whereby each server is replaced with a virtual machine (VM) and multiple such VMs are run on a single physical server can reduce the number of physical servers needed, and in turn, both the cost and energy consumption in data centers. However, existing schemes have not fully exploited the flexibility in the usage and allocation of virtualization resources, so as to allow one application originally deployed on a single large VM (LVM) to be split and hosted by multiple smaller VMs (SVM). Using multiple SVMs instead of an LVM enables resource allocation at a smaller granularity and thus may further increase the utilization and reduce the number of physical servers. However, a major challenge to overcome when deploying multiple SVMs for one application is to preserve the performance of the application in terms of response delay. In this paper, we show through theoretical …",9
Monte Carlo tree search with optimal computing budget allocation,"Yunchuan Li, Michael Fu, Jie Xu",2019/12/11,Conference 2019 IEEE 58th Conference on Decision and Control (CDC),"We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to the widely used Upper Confidence Bound (UCB) type of tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation trade-off when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption, and can be applied to game trees with mild modifications. A numerical experiment is conducted to demonstrate the efficiency of our algorithm in selecting the best action at the root.",8
Parallel empirical stochastic branch and bound for large-scale discrete optimization via simulation,"Scott Rosen, Peter Salemi, Brian Wickham, Ashley Williams, Christine Harvey, Erin Catlett, Sajjad Taghiyeh, Jie Xu",2016/12/11,Conference 2016 winter simulation conference (WSC),"Real-life simulation optimization applications often deal with large-scale simulation models that are time-consuming to execute. Parallel computing environments, such as high performance computing clusters and cloud computing services, provide the computing power needed to scale to such applications. In this paper, we show how the Empirical Stochastic Branch and Bound algorithm, an effective globally convergent random search algorithm for discrete optimization via simulation, can be adapted to a high-performance computing environment to effectively utilize the power of parallelism. We propose a master-worker structure driven by MITRE's Goal-Directed Grid-Enabled Simulation Experimentation Environment. Numerical experiments with the popular Ackley benchmark test function and a real-world simulation called runwaySimulator demonstrate the number of cores needed to achieve a good scaled …",8
A new optimal sampling rule for multi-fidelity optimization via ordinal transformation,"Si Zhang, Jie Xu, Edward Huang, Chun-Hung Chen",2016/8/21,Conference 2016 IEEE International Conference on Automation Science and Engineering (CASE),"Simulation optimization is often applied to solve large-scale complex systems problems in which high-fidelity simulation models are usually complex and time-consuming to run. As a result, the number of objective function evaluations is often very limited and presents a major hurdle for optimization. Recently, a new multi-fidelity optimization framework referred to as ordinal transformation was proposed to make use of a low-fidelity approximation model to speed up optimization. In this paper, we propose a new optimal sampling rule that leverages the theoretical properties of ordinal transformation to efficiently search for the optimal solution. The new optimal sampling rule maximizes the approximate probability of correctly selecting the best solution and is asymptotically optimal. Preliminary numerical experiments demonstrate that the new method achieves competitive finite-time performance and leads to significant …",7
Reducing electronic multiplexing in WDM rings: An evolutionary approach,"Jie Xu, Qingji Zeng",2001/10,Conference 2001 International Conferences on Info-Tech and Info-Net. Proceedings (Cat. No. 01EX479),"Wavelength division multiplexing (WDM) technology has been widely applied to increase the capacity of existing SDH/SONET self-healing rings. As the number of wavelengths goes up to several dozens, SDH/SONET add-drop multiplexers (ADMs) become the dominant cost factor in network deployment. Traffic grooming studies how to arrange the placement of ADMs intelligently on wavelengths so as to reduce the number of ADMs required. We address traffic grooming in SDH/SONET WDM rings featuring an evolutionary approach. By showing that traffic grooming can be formulated as standard combinatorial optimization problems, we propose hybrid genetic algorithms (GAs) for the problem. Our algorithms can be applied to traffic grooming assuming arbitrary traffic patterns. Numerical results are reported on the performance of our grooming algorithms, which demonstrate performance better than greedy …",7
An optimal computing budget allocation tree policy for Monte Carlo tree search,"Yunchuan Li, Michael C Fu, Jie Xu",2021/6/14,Journal IEEE Transactions on Automatic Control,"We analyze a tree search problem with an underlying Markov decision process, in which the goal is to identify the best action at the root that achieves the highest cumulative reward. We present a new tree policy that optimally allocates a limited computing budget to maximize a lower bound on the probability of correctly selecting the best action at each node. Compared to widely used upper confidence bound (UCB) tree policies, the new tree policy presents a more balanced approach to manage the exploration and exploitation tradeoff when the sampling budget is limited. Furthermore, UCB assumes that the support of reward distribution is known, whereas our algorithm relaxes this assumption. Numerical experiments demonstrate the efficiency of our algorithm in selecting the best action at the root.",6
Stochastic optimization using Hellinger distance,"Anand N Vidyashankar, Jie Xu",2015/12/6,Conference 2015 Winter Simulation Conference (WSC),"Stochastic optimization facilitates decision making in uncertain environments. In typical problems, probability distributions are fit to historical data for the chance variables and then optimization is carried out, as if the estimated probability distributions are the “truth”. However, this perspective is optimistic in nature and can frequently lead to sub-optimal or infeasible results because the distribution can be misspecified and the historical data set may be contaminated. In this paper, we propose to integrate existing approaches to decision making under uncertainty with robust and efficient estimation procedures using Hellinger distance. Within the existing decision-making methodologies that make use of parametric models, our approach offers robustness against model misspecifications and data contamination. Additionally, it also facilitates quantification of the impact of uncertainty in historical data on optimization results.",6
Drug resistance or re-emergence? Simulating equine parasites,"Jie Xu, Anand Vidyashankar, Martin K Nielsen",2014/11/18,Journal ACM Transactions on Modeling and Computer Simulation (TOMACS),"Emerging drug resistance in parasitology and its impact on human and animal health are of serious concern. Attempts by the parasitology community to address this issue led to the introduction of so-called selective therapy where a proportion of the population is left untreated. This has led to re-emergence of parasites that have heretofore been controlled. Using stochastic simulations, this article explores the tradeoff between drug resistance and re-emergence. More importantly, the article identifies the importance of the parasite fitness parameter vector and its role in drug resistance. Suggestions for further biological work and statistical analyses are also provided.",6
Using default ARTMAP for cancer classification with MicroRNA expression signatures,"Rui Xu, Jie Xu, Donald C Wunsch",2009/6/14,Conference 2009 International Joint Conference on Neural Networks,"High-throughput messenger RNA (mRNA) expression profiling with microarray has been demonstrated as a more effective method of cancer diagnosis and treatment than the traditional morphology or clinical parameter-based methods. Recently, the discovery of a class of small non-coding RNAs, named microRNAs (miRNAs), provides another promising method of cancer classification. MIRNAs play a critical role in the tumorigenic process by functioning either as oncogenes or as tumor suppressors. Here, we apply a neural-based classifier, default ARTMAP, to classify different types of cancers based on their miRNA expression fingerprints. Experimental results on the multiple human cancers show that default ARTMAP performs consistently well on all the data, and the classification accuracy is better than or comparable to that of the other popular classifiers.",6
Advancing self-healing capabilities in interconnected Microgrids via DDDAS with relational database management,"A Yavuz, J Darville, N Celik, J Xu, C Chen, B Langhals, R Engle",2020,Journal Proceedings of the 2020 Winter Simulation Conference,,5
"Robust estimation of value-at-risk through distribution-free and parametric approaches using the joint severity and frequency model: applications in financial, actuarial, and …","Sabyasachi Guharay, KC Chang, Jie Xu",2017/7/26,Journal Risks,"Value-at-Risk (VaR) is a well-accepted risk metric in modern quantitative risk management (QRM). The classical Monte Carlo simulation (MCS) approach, denoted henceforth as the classical approach, assumes the independence of loss severity and loss frequency. In practice, this assumption does not always hold true. Through mathematical analyses, we show that the classical approach is prone to significant biases when the independence assumption is violated. This is also corroborated by studying both simulated and real-world datasets. To overcome the limitations and to more accurately estimate VaR, we develop and implement the following two approaches for VaR estimation: the data-driven partitioning of frequency and severity (DPFS) using clustering analysis, and copula-based parametric modeling of frequency and severity (CPFS). These two approaches are verified using simulation experiments on synthetic data and validated on five publicly available datasets from diverse domains; namely, the financial indices data of Standard & Poor’s 500 and the Dow Jones industrial average, chemical loss spills as tracked by the US Coast Guard, Australian automobile accidents, and US hurricane losses. The classical approach estimates VaR inaccurately for 80% of the simulated data sets and for 60% of the real-world data sets studied in this work. Both the DPFS and the CPFS methodologies attain VaR estimates within 99% bootstrap confidence interval bounds for both simulated and real-world data. We provide a process flowchart for risk practitioners describing the steps for using the DPFS versus the CPFS methodology for VaR estimation …",5
A metamodeling-based approach for production planning,"Minqi Li, Feng Yang, Jie Xu",2014/12/7,Conference Proceedings of the Winter Simulation Conference 2014,"In production planning, one of the major challenges for plan optimization lies in quantifying the dependence of the objective criterion (typically total cost) upon the decision variables that specify a release plan of jobs. Existing methods either fall short in capturing such a relationship, which involves non-stationary stochastic processes of a manufacturing system (e.g., the number of jobs over time), or require discrete-event simulation (DES) to evaluate the objective criterion for each candidate decision, which is time-consuming. To enable the accurate and precise estimation of the objective for any decision plan within a reasonable time, this work proposed a metamodeling-based approach. The metamodels take the form of difference equations, embody the high-fidelity of DES, and can be used to address “what if” questions in a timely manner. When embedded in the optimization of production planning, the metamodels …",5
Determining the optimal sampling set size for random search,"Chenbo Zhu, Jie Xu, Chun-Hung Chen, Loo Hay Lee, Jianqiang Hu",2013/12/8,Conference 2013 Winter Simulations Conference (WSC),"Random search is a core component of many well known simulation optimization algorithms such as nested partition and COMPASS. Given a fixed computation budget, a critical decision is how many solutions to sample from a search area, which directly determines the number of simulation replications for each solution assuming that each solution receives the same number of simulation replications. This is another instance of the exploration vs. exploitation tradeoff in simulation optimization. Modeling the performance profile of all solutions in the search area as a normal distribution, we propose a method to (approximately) optimally determine the size of the sampling set and the number of simulation replications and use numerical experiments to demonstrate its performance.",5
Optimal computing budget allocation for regression with gradient information,"Tianxiang Wang, Jie Xu, Jian-Qiang Hu, Chun-Hung Chen",2021/12/1,Journal Automatica,"We consider the problem of optimizing the performance of a stochastic system, e.g., a discrete-event system, where the system performance is evaluated using stochastic simulations. Our objective is to allocate simulation budget to maximize the probability of correct selection (PCS) of the best design, where both system performance and gradient information can be obtained simultaneously via simulation. The objective function is assumed to be quadratic, or can be approximated by a quadratic regression model. The main contribution of our work is to utilize gradient information to enhance the efficiency of traditional Optimal Computing Budget Allocation (OCBA). We develop near-optimal rules that determine design points where simulations should be run and the number of runs allocated to each point. Our numerical experiments demonstrate that the proposed approach performs much better than other existing …",4
A study on efficient computing budget allocation for a two-stage problem,"Tianxiang Wang, Jie Xu, Jian-Qiang Hu",2021/4/5,Journal Asia-Pacific Journal of Operational Research,"We consider how to allocate simulation budget to estimate the risk measure of a system in a two-stage simulation optimization problem. In this problem, the first stage simulation generates scenarios that serve as inputs to the second stage simulation. For each sampled first stage scenario, the second stage procedure solves a simulation optimization problem by evaluating a number of decisions and selecting the optimal decision for the scenario. It also provides the estimated performance of the system over all sampled first stage scenarios to estimate the system’s reliability or risk measure, which is defined as the probability of the system’s performance exceeding a given threshold under various scenarios. Usually, such a two-stage procedure is very computationally expensive. To address this challenge, we propose a simulation budget allocation procedure to improve the computational efficiency for two-stage …",4
Advancing self-healing capabilities in interconnected microgrids via dynamic data driven applications system with relational database management,"Abdurrahman Yavuz, Joshua Darville, Nurcin Celik, Jie Xu, Chun-Hung Chen, Brent Langhals, Ryan Engle",2020/12/14,Conference 2020 winter simulation conference (wsc),"A microgrid is an interdependent electrical distribution system containing renewable energy sources, local demand and a coupled connection to the main grid. A very appealing feature of a microgrid is its capability to self-heal from disruptions, which is made even more viable with the emergence of interconnected collaborative microgrids. In this study, we present a dynamic data driven application system framework that integrates a relational database management system (RDBMS) to advance self-healing capabilities among interconnected microgrids. A RDBMS facilitates access to various sensors in the microgrid for fast abnormality detection and for determining the optimal self-healing action to implement. We build an agent-based simulation model (ABM) for three self-healing interconnected microgrids. Using the ABM, we compare self-healing operations of microgrids with and without an RDBMS. Simulation …",4
Fast and accurate method for estimating portfolio CVaR risk,"Soumyadip Ghosh, Pu Huang, Dharmashankar Subramanian, Jie Xu",2013/1/15,Patent office US,"A method, system and computer program product for measuring a risk of an asset portfolio. The system estimates a β-level CVaR (Conditional Value-at-Risk) of the asset portfolio by modeling interdependencies between assets in the asset portfolio. The modeling is based on Gaussian copula model.",4
Stochastic control framework for determining feasible alternatives in sampling allocation,"Yijie Peng, Jie Song, Jie Xu, Edwin KP Chong",2019/9/17,Journal IEEE Transactions on Automatic Control,"We formulate the optimal dynamic sampling allocation decision problem for feasibility determination as a stochastic control problem in a Bayesian setting. This new formulation addresses the limitations of previous static optimization formulations. In an approximate dynamic programming paradigm, we propose an approximately optimal allocation policy that maximizes a single feature of the value function one step ahead. Numerical results demonstrate the efficiency of the proposed method.",3
Model calibration,Jie Xu,2017,Journal Advances in Modeling and Simulation: Seminal Research from 50 Years of Winter Simulation Conferences,"Simulation model calibration refers to the iterative process of comparing the outputs of a simulation model with the observed quantities in the real system, and making changes to model input parameters accordingly to achieve an acceptable level of agreement between the simulation model and the real system. While calibration in a broader context may involve structural changes to the simulation model, this chapter focuses on the calibration of simulation model parameters that cannot be accurately estimated or specified for various reasons. When the simulation is time-consuming, has significant noise, and/or has a large number of parameters to calibrate, automatic and efficient calibration methods are critical to the success of any simulation-based analysisAnalysis and optimizationOptimization. This chapter discusses two main categories of general calibration methods: (1) direct calibration methods that search for …",3
Improving ordinal transformation through optimal combination of multi-model predictions,"Si Zhang, Jie Xu, Edward Huang, Chun-Hung Chen, Siyang Gao",2016/3/14,Conference 2016 IEEE International Conference on Industrial Technology (ICIT),"Optimization of large-scale complex systems often involves high-fidelity computational simulation models that are very time-consuming. As a result, the number of objective function evaluations is often very limited and presents a major hurdle for optimization. Previous works on a new framework known as ordinal transformation (OT) provides a method that makes use of a low-fidelity approximate model to speed up optimization. The effectiveness of OT depends crucially on the accuracy of the predictions by the approximate model. In this paper, we study how to improve the quality of the predictions when there are two or more low-fidelity models. We set up an optimization formulation that allows us to identify the optimal linear combination of multiple low-fidelity model outputs to improve the quality of the prediction. Preliminary numerical experiments demonstrate that the new method is very effective and can lead to …",3
Rare event simulation for stochastic fixed point equations related to the smoothing transformation,"Jeffrey F Collamore, Anand N Vidyashankar, Jie Xu",2013/12/8,Conference 2013 Winter Simulations Conference (WSC),"In several applications arising in compute science, cascade theory, and other applied areas, it is of interest to evaluate the tail probabilities of non-homogeneous stochastic fixed point equations. Recently, techniques have been developed for the related linear recursions, yielding tail estimates and importance sampling methods for these recursions. However, such methods do not routinely generalize to non-homogeneous recursions. Drawing on techniques from the weighted branching process literature, we present a consistent, strongly efficient importance sampling algorithm for estimating the tail probabilities for the case of non-homogeneous recursions.",3
Efficient simulation optimization with simulation learning,"Travis Goodwin, Jie Xu, Chun-Hung Chen, Nurcin Celik",2021/8/23,Conference 2021 IEEE 17th International Conference on Automation Science and Engineering (CASE),"Simulation optimization has found great success in automation science and engineering, such as the optimization of manufacturing systems, thanks to its capability to fully account for the complexity and uncertainty in systems. However, it remains a challenge to use simulation optimization in applications where decision time window is very short because of computational efficiency challenge. In this paper, a new framework known as Sequential Allocation using Machine-learning Predictions as Light-weight Estimates (SAMPLE) is proposed to address this challenge. SAMPLE utilizes an offline simulation learning phase to train machine learning models using simulation data. When a decision needs to be made, SAMPLE utilizes machine learning predictions under a Bayesian framework to determine optimal allocation of simulation sampling budget. The proposed approach enables fast-time simulation-based …",2
"Estimation of Value-at-Risk Using Mixture Copula Model for Heavy-Tailed Operational Risk Losses in Financial, Insurance & Climatological Data","Sabyasachi Guharay, KC Chang, Jie Xu",2018/7/10,Conference 2018 21st International Conference on Information Fusion (FUSION),"Data fusion techniques are being regularly used for analysis in Operational Risk Management (ORM). A popular and commonly used risk metric of interest, Value-at-Risk (VaR), has always been difficult to robustly estimate for different data types. The classical Monte Carlo simulation (MCS) approach (denoted henceforth as classical approach) assumes the independence of loss severity and loss frequency. In practice, this assumption may not always hold. To overcome this limitation and handle cases with heavy-tail data and more robustly estimate the corresponding VaR, we adopt a new approach known as Mixture Copula-based Parametric Modeling of Frequency and Severity (MCPFS). The proposed approach is verified via large-scale MCS experiments and validated on four publicly available financial datasets. We compare MCPFS with the classical approach for robust VaR estimation. We observe that the …",2
An optimization approach for team coordination through information sharing,"Yijie Peng, Edward Huang, Jie Xu, Chun-Hung Chen",2017/8/20,Conference 2017 13th IEEE Conference on Automation Science and Engineering (CASE),"Team coordination and information sharing are important in concurrent engineering (CE), where multiple design teams execute their tasks simultaneously and then share information to update their designs, e.g., through integrated tests. The process then iterates until the global design objective is optimized. When properly controlled and executed, CE can be an effective method to speed up the design process for complex and large-scale projects thanks to its parallel nature. Recently, a coordinate optimization framework is proposed in [1] to model and control the information sharing in CE. It can be shown that under a convexity assumption, CE converges to a globally optimal design. In this paper, we study how the coordinate optimization framework can be applied to CE in a general environment where the objective function is non-convex. We propose a simulation optimization method using a domain space cutting …",2
Adaptive nested rare event simulation algorithms,"Anand N Vidyashankar, Jie Xu",2013/12/8,Conference 2013 Winter Simulations Conference (WSC),"Nested simulation algorithms are used in several scientific investigations such as climate, statistical mechanics, and financial and actuarial risk management. Recently, these methods have also been used in the context of Bayesian computations and are referred to as Nested Sampling. In several of these problems, the inner level computation typically involves simulating events with very small probability, leading to rare event importance sampling methods. The quality of the resulting estimates depend on the allocation of computational resources between inner and outer level simulations. We introduce a novel adaptive rare event simulation algorithm that allocates the computational resources by taking in to account marginal changes in the rare event probabilities. We establish the consistency and efficiency of our algorithm and theoretically and numerically compare our results with the non-adaptive methods. We …",2
Risk-adjusted approach to optimize investments in product development portfolios,"Dharmashankar Subramanian, Pu Huang, C Pulavarthi, Jie Xu, H Sekhar, S Zhan, S Tripathi, S Kumar",2010/5/13,Journal IBM Journal of Research and Development," Companies invest in a portfolio of products with the financial objective of increasing revenue and net profit. They also have a limited product development budget and uncertainty around which products will be successful. In this paper, we offer a methodology to manage the allocation of a limited budget across a portfolio of products. Specifically, we provide a practical approach for quantifying the risk in relation to attaining financial objectives, and we offer an approach to reallocate the limited budget across the various products. This approach also provides long-term financial implications of investment decisions that are taken today. This practical end-to-end methodology can build on existing portfolio management practices prevalent in many companies. The approach uses all available measured and estimated data, expert opinions, and mathematical techniques for risk elicitation, Monte Carlo simulation for risk …",2
Traffic grooming in interconnected multigranularity WDM SDH/SONET rings,"Jie Xu, QingJi Zeng, Yun Wang, Xudong Yang",2001/10/17,Conference Optical Networking,"All-optical wavelength division multiplexing (WDM) networks using wavelength routing technology are considered to be a potential and practical solution to the next generation of wide area backbone networks. The ever-increasing demand for bandwidth further enhances the application of WDM optical networking technology. The main problem involved in design and optimization of wavelength-routed optical network is lightpath routing and wavelength assignment, namely, R&WA. In previous work on the R&WA problem, the project function has been analyzed by mathematic tools, such as integrate linear programming (ILP), heuristic algorithms or probability model, under the assumption of an ideal physical layer over which transmission impairments are ignored. In practical application, however, such impairments and their influence over traffic performance should be considered seriously, and the lightpath selection …",2
Quantifying the benefits of traffic grooming in interconnected WDM rings using a two-stage multiplexing scheme,"Jie Xu, QingJi Zeng",2001/10/11,"Conference Fiber Optic Components, Subsystems, and Systems for Telecommunications","With the wide spread use of wavelength-division-multiplexing (WDM) technology to enlarge the capacity of existing communications networks, the cost of electronic processing equipment, such as SDH/SONET add-drop multiplexers (ADMs), IP routers, MPLS switches, has replaced the cost of optics as the dominant cost factor in network deployment. Traffic grooming studies the logical topology design problem in WDM networks where low-speed connection requests are groomed into lightpaths connecting nodes that may be physically apart. In this paper, we address traffic grooming in interconnected WDM bi-directional rings. We propose the use of a two-stage multiplexing scheme supporting dual-homing interconnection, which actually provides additional levels of grooming. We present a set of formal mathematical formulation of the whole problem. Optimal solution can be obtained from the formulation.",2
Simulation optimization in manufacturing and services,"Carlo Meloni, Giulia Pedrielli, Inneke Van Nieuwenhuyse, Jie Xu",2020/12,Journal Flexible Services and Manufacturing Journal,"Simulation Optimization is an active field of research and is also increasingly being used in practical simulation applications and incorporated into both simulation and optimization software packages. This Special Issue of Flexible Services and Manufacturing (FSM) journal is devoted to report the recent advancements in Simulation Optimization techniques for manufacturing and services systems and how they are applied in practice. Indeed, many systems in several areas of manufacturing and service management are too complex to be studied analytically, and computer simulation has long been a useful tool for evaluating the performance of such systems. However, a simple evaluation of performance is often insufficient. To this aim, a deeper exploratory process may be provided by Simulation Optimization, which is the process of finding the combination of decision variables corresponding to the best performance …",1
Dynamic sampling for feasibility determination,"Yijie Peng, Jie Song, Jie Xu, Edwin Chong",2018/8/20,Conference 2018 IEEE 14th International Conference on Automation Science and Engineering (CASE),"We formulate the sampling allocation decision for feasibility determination as a dynamic policy in a Bayesian setting. This new formulation addresses the limitations of previous static optimization formulation. In an approximate dynamic programming paradigm, we propose an approximately optimal allocation policy that maximizes a single-feature of the value function one-step ahead. Numerical results demonstrate the efficiency of the proposed method.",1
Efficient Estimation in the Tails of Gaussian Copulas,"Kalyani Nagaraj, Jie Xu, Raghu Pasupathy, Soumyadip Ghosh",2016/7/5,Journal arXiv preprint arXiv:1607.01375,"We consider the question of efficient estimation in the tails of Gaussian copulas. Our special focus is estimating expectations over multi-dimensional constrained sets that have a small implied measure under the Gaussian copula. We propose three estimators, all of which rely on a simple idea: identify certain \emph{dominating} point(s) of the feasible set, and appropriately shift and scale an exponential distribution for subsequent use within an importance sampling measure. As we show, the efficiency of such estimators depends crucially on the local structure of the feasible set around the dominating points. The first of our proposed estimators $\estOpt$ is the ""full-information"" estimator that actively exploits such local structure to achieve bounded relative error in Gaussian settings. The second and third estimators $\estExp$, $\estLap$ are ""partial-information"" estimators, for use when complete information about the constraint set is not available, they do not exhibit bounded relative error but are shown to achieve polynomial efficiency. We provide sharp asymptotics for all three estimators. For the NORTA setting where no ready information about the dominating points or the feasible set structure is assumed, we construct a multinomial mixture of the partial-information estimator $\estLap$ resulting in a fourth estimator $\estNt$ with polynomial efficiency, and implementable through the ecoNORTA algorithm. Numerical results on various example problems are remarkable, and consistent with theory.",1
Optical network management with OSC,"Fengqing Liu, QingJi Zeng, Shilin Xiao, Xu Zhu, Jie Xu",2001/10/11,"Conference Fiber Optic Components, Subsystems, and Systems for Telecommunications","How to transport the control and management information of optical networking is a major concern these days. We compare several means and arrive at a conclusion that OSC (optical supervisory channel) is a better choice. The content of OSC is given and OSC channel wavelength, bit rate and coding method are discussed. An OSC example of WDM metro network are illuminated, which demonstrates the processing operation of overheads in OSC, FDI behavior for three types of failure, and Self-management of OSC subsystem. To fulfill the demand of intelligent and distributed management of optical transport network, an OSC of OC-3 (155Mbps) channel bit rate may be needed in the near future.",1
Efficient estimation of a risk measure requiring two-stage simulation optimization,"Tianxiang Wang, Jie Xu, Jian-Qiang Hu, Chun-Hung Chen",2023/3/16,Journal European Journal of Operational Research,"This paper is concerned with the efficient estimation of the risk measure of a system where the estimation requires solving a two-stage simulation optimization problem. The first stage samples risk factors that specify a second stage simulation optimization problem. The second stage solves a simulation optimization problem and outputs the best performance of the system under the realized risk factors, which are then aggregated across all first stage samples to produce an estimate of the risk measure. Applications of such an estimation scheme arise frequently in important industries such as financial, healthcare, logistics, and manufacturing. Because a large number of first stage samples are typically needed, each of which requires solving a computationally expensive simulation optimization problem, the two-stage simulation optimization approach faces a major computational efficiency challenge. In response to this …",
A Sequential Sampling-based Particle Swarm Optimization to Control Droop Coefficients of Distributed Generation Units in Microgrid Clusters,"Abdurrahman Yavuz, Nurcin Celik, Chun-Hung Chen, Jie Xu",2023/3/1,Journal Electric power systems research,"Microgrids can operate in a grid-connected or islanded mode and host renewable energy sources. However, there are significant challenges associated with enabling and securing the islanded microgrids’ operational control, with the problem of optimal selection of droop coefficients taking the lead. The problem of optimal selection of droop coefficients is further exacerbated due to the nonlinear nature of the system and stochasticity stemming from renewable energy sources and demand loads. We present a new solution approach to this problem by integrating a Newton-Raphson (NR) algorithm into a sequential sampling-based particle swarm optimization (PSO). The scalability and fast quadratic convergence of the NR algorithm make it ideal for solving the power flow equations of islanded microgrids. The ability of PSO to run without any assumptions on the structure of objective functions or constraints renders it a …",
Managing Customer Contact Centers with Delay Announcements and Automated Service,"Miao Yu, Jie Xu, Jiafu Tang",2023/2/20,Journal IISE Transactions,"This paper presents a study of the queueing system resulting from the service of customers in a generic customer contact center which has both automated service and traditional human agent service. Contact center managers would prefer customers use the provided automated service to reduce average customer queuing time as well as staffing costs of required human agent service agents. However, forcing customers to use automated service may lead to customer dissatisfaction. In this study, we propose to increase the use of automated service by using delay announcements as a tool to help guide customers to use automated service who would otherwise have chosen agent service. We present a stochastic optimization formulation to determine the optimal staffing level and delay announcement policy, with an objective to minimize staffing cost, customer balking, and reneging penalty. Closed-form solutions …",
Analysis Of The Impact Of Cyber Attack On Semiconductor Manufacturing Energy Quantification,"Busra Ezici, Paulo Costa, Jie Xu",2022/7/18,Conference 2022 Annual Modeling and Simulation Conference (ANNSIM),"Semiconductor manufacturers deal with complex processes, diverse product lines, and rapidly changing technologies while facing a competitive global market. There is also increased adoption of digital technologies with globalization, which creates an interconnection between each process. Digital connectivity and adaptation to advanced automated technologies increase dependence on data and the vulnerability to cyber-attacks. The presence of cyber-attacks in manufacturing causes delays in processing times for manufacturing, which impact performance and energy consumption. This study proposes a framework consisting of an energy quantification model, cyber risk modeling, and simulation analyses to address the impact of cyber threats on the energy quantification of semiconductor manufacturing. The performance of the wafer fab is analyzed using the cycle time, throughput rate, and work-in-process level …",
Robust Sampling Budget Allocation Under Deep Uncertainty,"Michael Perry, Jie Xu, Edward Huang, Chun-Hung Chen",2022/1/28,"Journal IEEE Transactions on Systems, Man, and Cybernetics: Systems","A novel methodology is introduced for optimally allocating a sampling budget. Sampling budget allocation problems arise frequently in various settings. For example, in the design of complex engineering systems, given both the complexity of these systems and the imperfect information on new technologies, designers often face deep uncertainty as to system performance. Consequently, designers need to sample multiple alternative designs under a limited budget. This article proposes a minimax regret approach to allocate the sampling budget in the presence of deep uncertainty pertaining to system performance. The objective is to maximize the probability of selecting the design with the minimum–maximum regret under a limited sampling budget and imperfect information. To effectively solve the minimax regret problem, an approximation methodology that provides good solutions with quantifiable uncertainty is …",
Workflow Modeling and Simulation Analysis for Semiconductor Wafer Fab Manufacturing,"Busra Ezici, Paulo Costa, Jie Xu",2022,Journal IIE Annual Conference. Proceedings,"Semiconductor manufacturers must deal with complex processes, sophisticated equipment, diverse product lines, and rapidly changing technologies, all while facing a very competitive global market. In this environment, it is crucial to increase efficiency, which requires sound performance metrics. The manufacturing of semiconductors begins with the arrival of raw input and materials, which is then followed by front-end (wafer fab) and back-end operations (assembly, sort, test). Some manufacturing processes use a significant amount of energy, especially those involving wafer fab tools. This study addresses the establishment and assessment of performance metrics by exploring the model and simulation of semiconductor wafer fab manufacturing processes. We use a workflow modeling tool to model controlflow patterns and validate the data and resources during the manufacturing processes in the wafer fab, and …",
An Economic Analysis of Cloud Computing Service Using Reclaimed Resources,"Chenbo Zhu, Haiying Shen, Jie Xu",2019/7/8,Conference 2019 IEEE 12th International Conference on Cloud Computing (CLOUD),"Resource under-utilization is prevalent in data centers as a result of substantial stochastic workload variations and resource provision guarantee for the requirements to provide high-level service availability. To avoid resource under-utilization, recently, the Economy cloud computing service class (Economy class in short) is proposed that sells allocated but unused (i.e., reclaimed) physical resources with long-term service level objectives (SLOs). Though the Economy class would lead to revenue benefit, how much of the reclaimed resources should be used as the Economy class (i.e., capacity planning) and how to set its price and SLAs to maximize revenue benefit have not been studied. To address this problem, in this paper, we propose a multi-level SLA for Economy class, which has multiple levels of SLOs and penalty. We then present a rigorous theoretical analysis on the capacity planning and pricing of the …",
"Flexible Expected Shortfall Estimation Using Parametric & Non-Parametric Methods with Applications in Finance, Insurance & Climatology","Sabyasachi Guharay, KC Chang, Jie Xu",2019/7/2,Conference 2019 22th International Conference on Information Fusion (FUSION),"Techniques employing Data fusion concepts are regularly being used in Quantitative Risk Management (QRM) for robust analysis. In our previous work, we studied the most commonly used risk metric of interest, Value-at-Risk (VaR). While VaR is a commonly used risk metric, an alternative risk metric, Expected Shortfall (ES) is well known to have better theoretical properties than VaR. We extend our previous work on studying VaR to include estimating the ES also known as Conditional Value-at-Risk (CVaR). The standard approach of estimating CVaR involves using Monte Carlo simulation (MCS) approach (denoted henceforth as classical approach). This approach involves breaking down the losses into loss severity and loss frequency assuming independence among them. In practice, this assumption may not always hold. To overcome this limitation and handle cases with both light & heavy-tail data, we propose …",
A Coordinate Optimization Approach for Concurrent Design,"Yijie Peng, Edward Huang, Jie Xu, Zhongshun Shi, Chun-Hung Chen",2018/9/26,Journal IEEE Transactions on Automatic Control,"In concurrent design (CCD), multiple design teams execute their tasks simultaneously and then exchange information to update their designs. The process then iterates until the termination criterion is met. When properly controlled and executed, CCD can be an effective method to shorten the time in product development for complex and large-scale projects thanks to its parallel nature. In this note, we propose a coordinate optimization framework to model and control team coordination through information sharing in CCD. It can be shown that under a certain convexity assumption, CCD converges to a globally optimal design if the information sharing intensity is smaller than a certain threshold. Numerical experiments substantiate the theoretical results.",
Flexible estimation of risk metric using copula model for the joint severity-frequency loss framework,"Sabyasachi Guharay, KC Chang, Jie Xu",2017/7/10,Conference 2017 20th International Conference on Information Fusion (Fusion),"Predictive analytics and data fusion techniques are being regularly used for analysis in Quantitative Risk Management (QRM). The primary risk metric of interest, Value-at-Risk (VaR), has always been difficult to robustly estimate for different data types. The classical Monte Carlo simulation (MCS) approach (denoted henceforth as classical approach) assumes the independence of loss severity and loss frequency. In practice, this assumption may not always hold. To overcome this limitation and more robustly estimate the corresponding VaR, we propose a new approach known as Copula-based Parametric Modeling of Frequency and Severity (CPFS). The proposed approach is verified via large-scale MCS experiments and validated on three publicly available datasets. We compare CPFS with the classical approach and a Data-driven Partitioning of Frequency and Severity (DPFS) approach for robust VaR estimation …",
Unified control plane scheme for IP over WDM,"Yun Wang, QingJi Zeng, Jie Xu, Xudong Yang, Chun Jiang",2001/10/17,Conference Optical Networking,"The development of advanced optical networking technology put forward a requirement for a new control plane to implement such functionalities as resource discovery, state information dissemination, path selection, and path management. In this paper, the development and evolution of optical networks are analyzed at first, then the requirements for the unified control plane are put forward, finally, the functional modules and interface properties for the unified control plane are investigated in detail.",
Online Appendix for “Efficient Simulation Sampling Allocation Using Multi-Fidelity Models”,"Yijie Peng, Jie Xu, Loo Hay Lee, Jianqiang Hu, Chun-Hung Chen","We present the algorithm that groups designs into performance clusters using low-fidelity information g as explained in Section III in the main body of the paper for a given number of clusters M. BIC can then be used to determine the number of clusters to use for the MFBA procedure. Since the likelihood of the Gaussian mixture distribution has multiple local optima (Fraley and Raftery, 2002), we run the Algorithm 100 times from randomly generated initial values and choose the best result as the final output of the algorithm. It takes about 1 second to run the clustering algorithm once in a Laptop with Intel Core i7-7500U CPU.","Scholar articles Online Appendix for “Efficient Simulation Sampling Allocation Using Multi-Fidelity Models”Y Peng, J Xu, LH Lee, J Hu, CH ChenRelated articles ","We present the algorithm that groups designs into performance clusters using low-fidelity information g as explained in Section III in the main body of the paper for a given number of clusters M. BIC can then be used to determine the number of clusters to use for the MFBA procedure. Since the likelihood of the Gaussian mixture distribution has multiple local optima (Fraley and Raftery, 2002), we run the Algorithm 100 times from randomly generated initial values and choose the best result as the final output of the algorithm. It takes about 1 second to run the clustering algorithm once in a Laptop with Intel Core i7-7500U CPU.",
The Impact of Supply Chain Structure on Product Line Architecture,"Jie Xu, Wallace J Hopp, Barry L Nelson","Modular design and lean production have been used by firms to justify expansion of their product lines by lowering product development and supply chain cost. We examine product line proliferation strategies for firms with different market conditions and supply chain structures in a modular design environment, where a product is made up of a platform and a component. Using a model that combines a nested logit representation of demand with a macro level depiction of supply chain cost structure, we show that a Make-To-Order (MTO) system makes platforms and components complementary, while they may be substitutes in a Make-To-Stock (MTS) system. Furthermore, an MTO system favors a fatter product line with a proliferation of component options, while an MTS system favors a leaner product line with streamlined component options but possibly more platforms. Finally, when a firm is not able to implement the …","Scholar articles The Impact of Supply Chain Structure on Product Line ArchitectureJ Xu, WJ Hopp, BL NelsonRelated articles ","Modular design and lean production have been used by firms to justify expansion of their product lines by lowering product development and supply chain cost. We examine product line proliferation strategies for firms with different market conditions and supply chain structures in a modular design environment, where a product is made up of a platform and a component. Using a model that combines a nested logit representation of demand with a macro level depiction of supply chain cost structure, we show that a Make-To-Order (MTO) system makes platforms and components complementary, while they may be substitutes in a Make-To-Stock (MTS) system. Furthermore, an MTO system favors a fatter product line with a proliferation of component options, while an MTS system favors a leaner product line with streamlined component options but possibly more platforms. Finally, when a firm is not able to implement the …",
