titles,authors,date,source,descriptions,citations
Efficient channel estimation for massive MIMO systems via truncated two-dimensional atomic norm minimization,"Yue Wang, Ping Xu, Zhi Tian",2017/5/21,Conference 2017 IEEE International Conference on Communications (ICC),"In millimeter-wave (mmWave) multiple-input multiple-output (MIMO) systems, channel estimation in the presence of sparse multipath fading boils down to two-dimensional (2D) direction-of-arrival (DOA) estimation followed by path gain estimation. To achieve super-resolution angle estimation at affordable complexity, this paper develops an efficient channel estimation approach by applying a truncated atomic norm minimization (T-ANM) technique, which is implemented via partial antenna activation during training-based channel estimation. This technique makes use of a key observation that the sparse scattering characteristics of mmWave MIMO channel gives rise to a low-rank two-level Toeplitz structure in the angular domain. Because of the low-rank property, only a subset of the transceiver antennas needs to be activated to save training resources. Meanwhile, the Toeplitz structure enables ANM-based gridless …",33
COKE: Communication-Censored Decentralized Kernel Learning,"Ping Xu, Yue Wang, Xiang Chen, Zhi Tian",2021/1/1,Journal Journal of Machine Learning Research,"This paper studies the decentralized optimization and learning problem where multiple interconnected agents aim to learn an optimal decision function defined over a reproducing kernel Hilbert space by jointly minimizing a global objective function, with access to their own locally observed dataset. As a non-parametric approach, kernel learning faces a major challenge in distributed implementation: the decision variables of local objective functions are data-dependent and thus cannot be optimized under the decentralized consensus framework without any raw data exchange among agents. To circumvent this major challenge, we leverage the random feature (RF) approximation approach to enable consensus on the function modeled in the RF space by data-independent parameters across different agents. We then design an iterative algorithm, termed DKLA, for fast-convergent implementation via ADMM. Based …",21
COKE: COMMUNICATION-CENSORED KERNEL LEARNING VIA RANDOM FEATURES,"Ping Xu, Zhi Tian, Zhe Zhang, Yue Wang",2019/6,Conference IEEE Data Science Workshop,"Distributed kernel-based methods are attractive in nonlinear learning tasks where either a dataset is too large to be processed on a single machine or the data are only locally available to geographically-located sites. For the first case, we propose to split the large dataset into multiple mini-batches and distribute them to distinct sites for parallel learning through the alternating direction method of multipliers (ADMM). For the second case, we develop a decentralized ADMM so that each site can solve the learning task collaboratively through one-hop communications. To circumvent the curse of dimensionality in kernel-based methods, we leverage the random feature approximation to map the large-volume data into a smaller feature space. This also results in a common set of decision parameters that can be exchanged among sites. Motivated by the need to conserve energy and reduce communication overheads, we …",13
An energy-efficient distributed average consensus scheme via infrequent communication,"Ping Xu, Zhi Tian, Yue Wang",2018/11/26,Conference 2018 IEEE Global Conference on Signal and Information Processing (GlobalSIP),"Distributed average consensus finds extensive applications in multi-agent systems where a group of agents are required to agree upon a common value. Motivated by the need to conserve energy for large-scale multi-agent systems, we propose an infrequent communication scheme that allows the system to reach average consensus at low energy consumption. The proposed scheme divides the original large network into two smaller sub-networks with some overlapping nodes. Agents in the two sub-networks take turns to broadcast and update their local estimates, until reaching consensus. We prove that the proposed scheme guarantees asymptotic convergence when both subnetworks are connected and have overlapping nodes. Simulations corroborate the energy saving capability of the proposed scheme.",7
Dc-cnn: computational flow redefinition for efficient cnn through structural decoupling,"Fuxun Yu, Zhuwei Qin, Di Wang, Ping Xu, Chenchen Liu, Zhi Tian, Xiang Chen",2020/3/9,"Conference 2020 Design, Automation & Test in Europe Conference & Exhibition (DATE)","Recently Convolutional Neural Networks (CNNs) are widely applied into novel intelligent applications and systems. However, the CNN computation performance is significantly hindered by its computation flow, which computes the model structure sequentially by layers with massive convolution operations. Such a layer-wise sequential computation flow can cause certain performance issues, such as resource under-utilization, huge memory overhead, etc. To solve these problems, we propose a novel CNN structural decoupling method, which could decouple CNN models into ""critical paths"" and eliminate the inter-layer data dependency. Based on this method, we redefine the CNN computation flow into parallel and cascade computing paradigms, which can significantly enhance the CNN computation performance with both multi-core and single-core CPU processors. Experiments show that, our DC-CNN framework …",5
A class of distributed event-triggered average consensus algorithms for multi-agent systems,"Ping Xu, Cameron Nowzari, Zhi Tian",2020/7/31,Journal International Journal of Control,"This paper proposes a class of distributed event-triggered algorithms that solve the average consensus problem in multi-agent systems. By designing events such that a specifically chosen Lyapunov function is monotonically decreasing, event-triggered algorithms succeed in reducing communications among agents while still ensuring that the entire system converges to the desired state. However, depending on the chosen Lyapunov function the transient behaviours can be very different. Moreover, performance requirements also vary from application to application. Consequently, we are instead interested in considering a class of Lyapunov functions such that each Lyapunov function produces a different event-triggered coordination algorithm to solve the multi-agent average consensus problem. The proposed class of algorithms all guarantee exponential convergence of the resulting system and exclusion of Zeno …",4
A class of event-triggered coordination algorithms for multi-agent systems on weight-balanced digraphs,"Ping Xu, Cameron Nowzari, Zhi Tian",2018/6/27,Conference 2018 Annual American Control Conference (ACC),"This paper revisits the multi-agent average consensus problem on weight-balanced directed graphs. In order to reduce communication among the agents, many recent works have considered event-triggered communication and control as a method to reduce communication while still ensuring that the entire network converges to the desired state. One common way to do this is to design events such that a specifically chosen Lyapunov function is monotonically decreasing; however, depending on the chosen Lyapunov function the transient behaviors can be very different. Consequently, we are instead interested in considering a class of Lyapunov functions such that each Lyapunov function produces a different event-triggered coordination algorithm to solve the multi-agent average consensus problem. The proposed class of algorithms all guarantee exponential convergence of the resulting network and exclusion of …",4
QC-ODKLA: Quantized and Communication-Censored Online Decentralized Kernel Learning via Linearized ADMM,"Ping Xu, Yue Wang, Xiang Chen, Zhi Tian",2022/8/4,Journal arXiv preprint arXiv:2208.02777,"This paper focuses on online kernel learning over a decentralized network. Each agent in the network receives continuous streaming data locally and works collaboratively to learn a nonlinear prediction function that is globally optimal in the reproducing kernel Hilbert space with respect to the total instantaneous costs of all agents. In order to circumvent the curse of dimensionality issue in traditional online kernel learning, we utilize random feature (RF) mapping to convert the non-parametric kernel learning problem into a fixed-length parametric one in the RF space. We then propose a novel learning framework named Online Decentralized Kernel learning via Linearized ADMM (ODKLA) to efficiently solve the online decentralized kernel learning problem. To further improve the communication efficiency, we add the quantization and censoring strategies in the communication stage and develop the Quantized and Communication-censored ODKLA (QC-ODKLA) algorithm. We theoretically prove that both ODKLA and QC-ODKLA can achieve the optimal sublinear regret  over  time slots. Through numerical experiments, we evaluate the learning effectiveness, communication, and computation efficiencies of the proposed methods.",1
Deep Kernel Learning Networks with Multiple Learning Paths,"Ping Xu, Yue Wang, Xiang Chen, Zhi Tian",2022/5/23,"Conference ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","This paper proposes deep kernel learning networks with multiple learning paths (DKL-MLP) for nonlinear function approximation. Leveraging the random feature (RF) mapping technique, kernel methods can be implemented as a two-layer neural network, at drastically reduced workload on weight training. Motivated by the representation power of the deep architecture in deep neural networks, we devise a vanilla deep kernel learning network (DKL) by applying RF mapping at each layer and learn the last layer only. To improve the learning performance of DKL, we add multiple trainable paths to DKL and develop the DKL-MLP method so that some implicit information from earlier hidden layers to the output layer can be learned. We prove that both DKL and DKL-MLP permit universal representation of a wide variety of interesting functions with arbitrarily small error and have no bad local minimum. Numerical …",1
Robust Distributed Learning Against Both Distributional Shifts and Byzantine Attacks,"Guanqiang Zhou, Ping Xu, Yue Wang, Zhi Tian",2022/10/29,Journal arXiv preprint arXiv:2210.16682,"In distributed learning systems, robustness issues may arise from two sources. On one hand, due to distributional shifts between training data and test data, the trained model could exhibit poor out-of-sample performance. On the other hand, a portion of working nodes might be subject to byzantine attacks which could invalidate the learning result. Existing works mostly deal with these two issues separately. In this paper, we propose a new algorithm that equips distributed learning with robustness measures against both distributional shifts and byzantine attacks. Our algorithm is built on recent advances in distributionally robust optimization as well as norm-based screening (NBS), a robust aggregation scheme against byzantine attacks. We provide convergence proofs in three cases of the learning model being nonconvex, convex, and strongly convex for the proposed algorithm, shedding light on its convergence behaviors and endurability against byzantine attacks. In particular, we deduce that any algorithm employing NBS (including ours) cannot converge when the percentage of byzantine nodes is 1/3 or higher, instead of 1/2, which is the common belief in current literature. The experimental results demonstrate the effectiveness of our algorithm against both robustness issues. To the best of our knowledge, this is the first work to address distributional shifts and byzantine attacks simultaneously.",
Communication-Efficient Optimization and Learning for Distributed Multi-Agent Systems,Ping Xu,2022,Institution George Mason University,"Distributed learning has attracted extensive interest in recent years, owing to the explosion of data generated from mobile sensors, social media services, and other networked multi-agent applications. In many of these applications, the observed data are usually kept private at local sites without being aggregated to a fusion center, either due to the prohibitively high cost of raw data transmission or privacy concerns. Meanwhile, each agent in the network only communicates with its neighbors within a one-hop local range to save transmission power. Moreover, distributed learning is typically implemented in an iterative manner for computational feasibility and efficiency. This incurs frequent communications among agents to exchange their locally computed updates of the shared learning model, which can cause tremendous communication overhead in terms of both link bandwidth and transmission power. Under this …",
COKE: Communication-censored kernel learning for decentralized non-parametric learning,"Ping Xu, Yue Wang, Xiang Chen, Zhi Tian",2020,Journal arXiv preprint arXiv:2001.10133,,
