titles,authors,date,source,descriptions,citations
Weakly-convex–concave min–max optimization: provable algorithms and applications in machine learning,"Hassan Rafique, Mingrui Liu, Qihang Lin, Tianbao Yang",2022/5/4,Journal Optimization Methods and Software,"Min–max problems have broad applications in machine learning, including learning with non-decomposable loss and learning with robustness to data distribution. Convex–concave min–max problem is an active topic of research with efficient algorithms and sound theoretical foundations developed. However, it remains a challenge to design provably efficient algorithms for non-convex min–max problems with or without smoothness. In this paper, we study a family of non-convex min–max problems, whose objective function is weakly convex in the variables of minimization and is concave in the variables of maximization. We propose a proximally guided stochastic subgradient method and a proximally guided stochastic variance-reduced method for the non-smooth and smooth instances, respectively, in this family of problems. We analyse the time complexities of the proposed methods for finding a nearly stationary …",195
First-order convergence theory for weakly-convex-weakly-concave min-max problems,"Mingrui Liu, Hassan Rafique, Qihang Lin, Tianbao Yang",2021/1/1,Journal The Journal of Machine Learning Research,"In this paper, we consider first-order convergence theory and algorithms for solving a class of non-convex non-concave min-max saddle-point problems, whose objective function is weakly convex in the variables of minimization and weakly concave in the variables of maximization. It has many important applications in machine learning including training Generative Adversarial Nets (GANs). We propose an algorithmic framework motivated by the inexact proximal point method, where the weakly monotone variational inequality (VI) corresponding to the original min-max problem is solved through approximately solving a sequence of strongly monotone VIs constructed by adding a strongly monotone mapping to the original gradient mapping. We prove first-order convergence to a nearly stationary solution of the original min-max problem of the generic algorithmic framework and establish different rates by employing …",94
Towards Better Understanding of Adaptive Gradient Algorithms in Generative Adversarial Nets,"Mingrui Liu, Youssef Mroueh, Jerret Ross, Wei Zhang, Xiaodong Cui, Payel Das, Tianbao Yang",2019/12/26,Conference International Conference on Learning Representations 2020,"Adaptive gradient algorithms perform gradient-based updates using the history of gradients and are ubiquitous in training deep neural networks. While adaptive gradient methods theory is well understood for minimization problems, the underlying factors driving their empirical success in min-max problems such as GANs remain unclear. In this paper, we aim at bridging this gap from both theoretical and empirical perspectives. First, we analyze a variant of Optimistic Stochastic Gradient (OSG) proposed in~\citep{daskalakis2017training} for solving a class of non-convex non-concave min-max problem and establish  complexity for finding -first-order stationary point, in which the algorithm only requires invoking one stochastic first-order oracle while enjoying state-of-the-art iteration complexity achieved by stochastic extragradient method by~\citep{iusem2017extragradient}. Then we propose an adaptive variant of OSG named Optimistic Adagrad (OAdagrad) and reveal an \emph{improved} adaptive complexity , where  characterizes the growth rate of the cumulative stochastic gradient and . To the best of our knowledge, this is the first work for establishing adaptive complexity in non-convex non-concave min-max optimization. Empirically, our experiments show that indeed adaptive gradient algorithms outperform their non-adaptive counterparts in GAN training. Moreover, this observation can be explained by the slow growth rate of the cumulative stochastic gradient, as observed empirically.",53
Stochastic AUC Maximization with Deep Neural Networks,"Mingrui Liu, Zhuoning Yuan, Yiming Ying, Tianbao Yang",2019/8/28,Conference International Conference on Learning Representations 2020,"Stochastic AUC maximization has garnered an increasing interest due to better fit to imbalanced data classification. However, existing works are limited to stochastic AUC maximization with a linear predictive model, which restricts its predictive power when dealing with extremely complex data. In this paper, we consider stochastic AUC maximization problem with a deep neural network as the predictive model. Building on the saddle point reformulation of a surrogated loss of AUC, the problem can be cast into a {\it non-convex concave} min-max problem. The main contribution made in this paper is to make stochastic AUC maximization more practical for deep neural networks and big data with theoretical insights as well. In particular, we propose to explore Polyak-\L{}ojasiewicz (PL) condition that has been proved and observed in deep learning, which enables us to develop new stochastic algorithms with even faster convergence rate and more practical step size scheme. An AdaGrad-style algorithm is also analyzed under the PL condition with adaptive convergence rate. Our experimental results demonstrate the effectiveness of the proposed algorithms.",53
Fast Stochastic AUC Maximization with -Convergence Rate,"Mingrui Liu, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, Tianbao Yang",2018/7/3,Conference International Conference on Machine Learning,"In this paper, we consider statistical learning with AUC (area under ROC curve) maximization in the classical stochastic setting where one random data drawn from an unknown distribution is revealed at each iteration for updating the model. Although consistent convex surrogate losses for AUC maximization have been proposed to make the problem tractable, it remains an challenging problem to design fast optimization algorithms in the classical stochastic setting due to that the convex surrogate loss depends on random pairs of examples from positive and negative classes. Building on a saddle point formulation for a consistent square loss, this paper proposes a novel stochastic algorithm to improve the standard  convergence rate to  convergence rate without strong convexity assumption or any favorable statistical assumptions (eg, low noise), where  is the number of random samples. To the best of our knowledge, this is the first stochastic algorithm for AUC maximization with a statistical convergence rate as fast as  up to a logarithmic factor. Extensive experiments on eight large-scale benchmark data sets demonstrate the superior performance of the proposed algorithm comparing with existing stochastic or online algorithms for AUC maximization.",53
ADMM without a fixed penalty parameter: Faster convergence with new adaptive penalization,"Yi Xu, Mingrui Liu, Qihang Lin, Tianbao Yang",2017,Journal Advances in neural information processing systems,"Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous problems in machine learning, statistics and signal processing. However, it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization, they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper, we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function, which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side, given the local sharpness characterized by an exponent , we show that the proposed ADMM enjoys an improved iteration complexity of \footnote { suppresses a logarithmic factor.} in the deterministic setting and an iteration complexity of  in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side, we demonstrate that the proposed algorithms …",48
A decentralized parallel algorithm for training generative adversarial nets,"Mingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jarret Ross, Tianbao Yang, Payel Das",2020,Journal Advances in Neural Information Processing Systems,"Generative Adversarial Networks (GANs) are a powerful class of generative models in the deep learning community. Current practice on large-scale GAN training utilizes large models and distributed large-batch training strategies, and is implemented on deep learning frameworks (eg, TensorFlow, PyTorch, etc.) designed in a centralized manner. In the centralized network topology, every worker needs to either directly communicate with the central node or indirectly communicate with all other workers in every iteration. However, when the network bandwidth is low or network latency is high, the performance would be significantly degraded. Despite recent progress on decentralized algorithms for training deep neural networks, it remains unclear whether it is possible to train GANs in a decentralized manner. The main difficulty lies at handling the nonconvex-nonconcave min-max optimization and the decentralized communication simultaneously. In this paper, we address this difficulty by designing the\textbf {first gradient-based decentralized parallel algorithm} which allows workers to have multiple rounds of communications in one iteration and to update the discriminator and generator simultaneously, and this design makes it amenable for the convergence analysis of the proposed decentralized algorithm. Theoretically, our proposed decentralized algorithm is able to solve a class of non-convex non-concave min-max problems with provable non-asymptotic convergence to first-order stationary point. Experimental results on GANs demonstrate the effectiveness of the proposed algorithm.",44
Improved Schemes for Episodic Memory-based Lifelong Learning,"Yunhui Guo*, Mingrui Liu*, Tianbao Yang, Tajana Rosing",2020,Journal Advances in Neural Information Processing Systems,"Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge. This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning. Recently, episodic memory based approaches such as GEM and A-GEM have shown remarkable performance. In this paper, we provide the first unified view of episodic memory based approaches from an optimization's perspective. This view leads to two improved schemes for episodic memory based lifelong learning, called MEGA-\rom {1} and MEGA-\rom {2}. MEGA-\rom {1} and MEGA-\rom {2} modulate the balance between old tasks and the new task by integrating the current gradient with the gradient computed on the episodic memory. Notably, we show that GEM and A-GEM are degenerate cases of MEGA-\rom {1} and MEGA-\rom {2} which consistently put the same emphasis on the current task, regardless of how the loss changes over time. Our proposed schemes address this issue by using novel loss-balancing updating rules, which drastically improve the performance over GEM and A-GEM. Extensive experimental results show that the proposed schemes significantly advance the state-of-the-art on four commonly used lifelong learning benchmarks, reducing the error by up to 18%.",41
Adaptive negative curvature descent with applications in non-convex optimization,"Mingrui Liu, Zhe Li, Xiaoyu Wang, Jinfeng Yi, Tianbao Yang",2018,Conference Advances in Neural Information Processing Systems,"Negative curvature descent (NCD) method has been utilized to design deterministic or stochastic algorithms for non-convex optimization aiming at finding second-order stationary points or local minima. In existing studies, NCD needs to approximate the smallest eigen-value of the Hessian matrix with a sufficient precision (eg, ) in order to achieve a sufficiently accurate second-order stationary solution (ie, $\lambda_ {\min}(\nabla^ 2 f (\x))\geq-\epsilon_2) $. One issue with this approach is that the target precision  is usually set to be very small in order to find a high quality solution, which increases the complexity for computing a negative curvature. To address this issue, we propose an adaptive NCD to allow for an adaptive error dependent on the current gradient's magnitude in approximating the smallest eigen-value of the Hessian, and to encourage competition between a noisy NCD step and gradient descent step. We consider the applications of the proposed adaptive NCD for both deterministic and stochastic non-convex optimization, and demonstrate that it can help reduce the the overall complexity in computing the negative curvatures during the course of optimization without sacrificing the iteration complexity.",38
Spatiotemporal dynamics in a network composed of neurons with different excitabilities and excitatory coupling,"WeiWei Xiao, HuaGuang Gu, MingRui Liu",2016/12,Journal Science China Technological Sciences,"Spiral waves have been observed in the biological experiments on rat cortex perfused with drugs which can block inhibitory synapse and switch neuron excitability from type II to type I. To simulate the spiral waves observed in the experiment, the spatiotemporal patterns are investigated in a network composed of neurons with type I and II excitabilities and excitatory coupling. Spiral waves emerge when the percentage (p) of neurons with type I excitability in the network is at middle levels, which is dependent on the coupling strength. Compared with other spatial patterns which appear at different p values, spiral waves exhibit optimal spatial correlation at a certain spatial frequency, implying the occurrence of spatial coherence resonance-like phenomenon. Some dynamical characteristics of the network such as mean firing frequency and synchronous degree can be well interpreted with distinct properties …",27
Communication-Efficient Distributed Stochastic AUC Maximization with Deep Neural Networks,"Zhishuai Guo, Mingrui Liu, Zhuoning Yuan, Li Shen, Wei Liu, Tianbao Yang",2020/5/5,Journal International Conference on Machine Learning,"In this paper, we study distributed algorithms for large-scale AUC maximization with a deep neural network as a predictive model. Although distributed learning techniques have been investigated extensively in deep learning, they are not directly applicable to stochastic AUC maximization with deep neural networks due to its striking differences from standard loss minimization problems (eg, cross-entropy). Towards addressing this challenge, we propose and analyze a communication-efficient distributed optimization algorithm based on a\emph {non-convex concave} reformulation of the AUC maximization, in which the communication of both the primal variable and the dual variable between each worker and the parameter server only occurs after multiple steps of gradient-based updates in each worker. Compared with the naive parallel version of an existing algorithm that computes stochastic gradients at individual machines and averages them for updating the model parameter, our algorithm requires a much less number of communication rounds and still achieves linear speedup in theory. To the best of our knowledge, this is the\textbf {first} work that solves the\emph {non-convex concave min-max} problem for AUC maximization with deep neural networks in a communication-efficient distributed manner while still maintaining the linear speedup property in theory. Our experiments on several benchmark datasets show the effectiveness of our algorithm and also confirm our theory.",25
Adaptive accelerated gradient converging methods under holderian error bound condition,"Mingrui Liu, Tianbao Yang",2016/11/23,Journal Advances in Neural Information Processing Systems 30,"Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, ie, leveraging the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the H\""{o} lderian error bound (HEB) condition.{\it The key technique} for our development is a novel synthesis of {\it adaptive regularization and a conditional restarting scheme}, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning:(i) if the objective function is coercive and semi-algebraic, PG's convergence speed is essentially , where  is the total number of iterations;(ii) if the objective function consists of an , , , or huber norm regularization and a convex smooth piecewise quadratic loss (eg, square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a {\it faster linear convergence} than PG without any other assumptions (eg, restricted eigen-value condition). It is notable that our linear convergence results for the aforementioned problems are global instead of local …",25
Fast rates of erm and stochastic approximation: Adaptive to error bound conditions,"Mingrui Liu, Xiaoxuan Zhang, Lijun Zhang, Rong Jin, Tianbao Yang",2018/5/11,Journal Advances in Neural Information Processing Systems 30,"Error bound conditions (EBC) are properties that characterize the growth of an objective function when a point is moved away from the optimal set. They have recently received increasing attention in the field of optimization for developing optimization algorithms with fast convergence. However, the studies of EBC in statistical learning are hitherto still limited. The main contributions of this paper are two-fold. First, we develop fast and intermediate rates of empirical risk minimization (ERM) under EBC for risk minimization with Lipschitz continuous, and smooth convex random functions. Second, we establish fast and intermediate rates of an efficient stochastic approximation (SA) algorithm for risk minimization with Lipschitz continuous random functions, which requires only one pass of  samples and adapts to EBC. For both approaches, the convergence rates span a full spectrum between  and  depending on the power constant in EBC, and could be even faster than  in special cases for ERM. Moreover, these convergence rates are automatically adaptive without using any knowledge of EBC. Overall, this work not only strengthens the understanding of ERM for statistical learning but also brings new fast stochastic algorithms for solving a broad range of statistical learning problems.",15
Improving efficiency in large-scale decentralized distributed training,"Wei Zhang, Xiaodong Cui, Abdullah Kayi, Mingrui Liu, Ulrich Finkler, Brian Kingsbury, George Saon, Youssef Mroueh, Alper Buyuktosunoglu, Payel Das, David Kung, Michael Picheny",2020/5/4,"Conference ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","Decentralized Parallel SGD (D-PSGD) and its asynchronous variant Asynchronous Parallel SGD (AD-PSGD) is a family of distributed learning algorithms that have been demonstrated to perform well for large-scale deep learning tasks. One drawback of (A)D-PSGD is that the spectral gap of the mixing matrix decreases when the number of learners in the system increases, which hampers convergence. In this paper, we investigate techniques to accelerate (A)D-PSGD based training by improving the spectral gap while minimizing the communication cost. We demonstrate the effectiveness of our proposed techniques by running experiments on the 2000-hour Switchboard speech recognition task and the ImageNet computer vision task. On an IBM P9 supercomputer, our system is able to train an LSTM acoustic model in 2.28 hours with 7.5% WER on the Hub5-2000 Switchboard (SWB) test set and 13.3% WER on the …",14
Generalization guarantee of SGD for pairwise learning,"Yunwen Lei, Mingrui Liu, Yiming Ying",2021/12/6,Journal Advances in Neural Information Processing Systems,"Recently, there is a growing interest in studying pairwise learning since it includes many important machine learning tasks as specific examples, eg, metric learning, AUC maximization and ranking. While stochastic gradient descent (SGD) is an efficient method, there is a lacking study on its generalization behavior for pairwise learning. In this paper, we present a systematic study on the generalization analysis of SGD for pairwise learning to understand the balance between generalization and optimization. We develop a novel high-probability generalization bound for uniformly-stable algorithms to incorporate the variance information for better generalization, based on which we establish the first nonsmooth learning algorithm to achieve almost optimal high-probability and dimension-independent generalization bounds in linear time. We consider both convex and nonconvex pairwise learning problems. Our stability analysis for convex problems shows how the interpolation can help generalization. We establish a uniform convergence of gradients, and apply it to derive the first generalization bounds on population gradients for nonconvex problems. Finally, we develop better generalization bounds for gradient-dominated problems.",13
Adam: A Stochastic Method with Adaptive Variance Reduction,"Mingrui Liu, Wei Zhang, Francesco Orabona, Tianbao Yang",2020/11/24,Journal arXiv preprint arXiv:2011.11985,"Adam is a widely used stochastic optimization method for deep learning applications. While practitioners prefer Adam because it requires less parameter tuning, its use is problematic from a theoretical point of view since it may not converge. Variants of Adam have been proposed with provable convergence guarantee, but they tend not be competitive with Adam on the practical performance. In this paper, we propose a new method named Adam (pronounced as Adam-plus). Adam retains some of the key components of Adam but it also has several noticeable differences: (i) it does not maintain the moving average of second moment estimate but instead computes the moving average of first moment estimate at extrapolated points; (ii) its adaptive step size is formed not by dividing the square root of second moment estimate but instead by dividing the root of the norm of first moment estimate. As a result, Adam requires few parameter tuning, as Adam, but it enjoys a provable convergence guarantee. Our analysis further shows that Adam enjoys adaptive variance reduction, i.e., the variance of the stochastic gradient estimator reduces as the algorithm converges, hence enjoying an adaptive convergence. We also propose a more general variant of Adam with different adaptive step sizes and establish their fast convergence rate. Our empirical studies on various deep learning tasks, including image classification, language modeling, and automatic speech recognition, demonstrate that Adam significantly outperforms Adam and achieves comparable performance with best-tuned SGD and momentum SGD.",12
Non-convex min–max optimization: provable algorithms and applications in machine learning (2018),"H Rafique, M Liu, Q Lin, T Yang",1810,Journal arXiv preprint arXiv:1810.02060,,10
Will bilevel optimizers benefit from loops,"Kaiyi Ji, Mingrui Liu, Yingbin Liang, Lei Ying",2022/5/27,Journal arXiv preprint arXiv:2205.14224,"Bilevel optimization has arisen as a powerful tool for solving a variety of machine learning problems. Two current popular bilevel optimizers AID-BiO and ITD-BiO naturally involve solving one or two sub-problems, and consequently, whether we solve these problems with loops (that take many iterations) or without loops (that take only a few iterations) can significantly affect the overall computational efficiency. Existing studies in the literature cover only some of those implementation choices, and the complexity bounds available are not refined enough to enable rigorous comparison among different implementations. In this paper, we first establish unified convergence analysis for both AID-BiO and ITD-BiO that are applicable to all implementation choices of loops. We then specialize our results to characterize the computational complexity for all implementations, which enable an explicit comparison among them. Our result indicates that for AID-BiO, the loop for estimating the optimal point of the inner function is beneficial for overall efficiency, although it causes higher complexity for each update step, and the loop for approximating the outer-level Hessian-inverse-vector product reduces the gradient complexity. For ITD-BiO, the two loops always coexist, and our convergence upper and lower bounds show that such loops are necessary to guarantee a vanishing convergence error, whereas the no-loop scheme suffers from an unavoidable non-vanishing convergence error. Our numerical experiments further corroborate our theoretical results.",9
Faster online learning of optimal threshold for consistent F-measure optimization,"Xiaoxuan Zhang*, Mingrui Liu*, Xun Zhou, Tianbao Yang",2018,Conference Advances in Neural Information Processing Systems,"In this paper, we consider online F-measure optimization (OFO). Unlike traditional performance metrics (eg, classification error rate), F-measure is non-decomposable over training examples and is a non-convex function of model parameters, making it much more difficult to be optimized in an online fashion. Most existing results of OFO usually suffer from high memory/computational costs and/or lack statistical consistency guarantee for optimizing F-measure at the population level. To advance OFO, we propose an efficient online algorithm based on simultaneously learning a posterior probability of class and learning an optimal threshold by minimizing a stochastic strongly convex function with unknown strong convexity parameter. A key component of the proposed method is a novel stochastic algorithm with low memory and computational costs, which can enjoy a convergence rate of  for learning the optimal threshold under a mild condition on the convergence of the posterior probability, where  is the number of processed examples. It is provably faster than its predecessor based on a heuristic for updating the threshold. The experiments verify the efficiency of the proposed algorithm in comparison with state-of-the-art OFO algorithms.",8
Understanding adamw through proximal methods and scale-freeness,"Zhenxun Zhuang, Mingrui Liu, Ashok Cutkosky, Francesco Orabona",2022/1/31,Journal arXiv preprint arXiv:2202.00089,"Adam has been widely adopted for training deep neural networks due to less hyperparameter tuning and remarkable performance. To improve generalization, Adam is typically used in tandem with a squared  regularizer (referred to as Adam-). However, even better performance can be obtained with AdamW, which decouples the gradient of the regularizer from the update rule of Adam-. Yet, we are still lacking a complete explanation of the advantages of AdamW. In this paper, we tackle this question from both an optimization and an empirical point of view. First, we show how to re-interpret AdamW as an approximation of a proximal gradient method, which takes advantage of the closed-form proximal mapping of the regularizer instead of only utilizing its gradient information as in Adam-. Next, we consider the property of ""scale-freeness"" enjoyed by AdamW and by its proximal counterpart: their updates are invariant to component-wise rescaling of the gradients. We provide empirical evidence across a wide range of deep learning experiments showing a correlation between the problems in which AdamW exhibits an advantage over Adam- and the degree to which we expect the gradients of the network to exhibit multiple scales, thus motivating the hypothesis that the advantage of AdamW could be due to the scale-free updates.",6
Robustness to Unbounded Smoothness of Generalized SignSGD,"Michael Crawshaw, Mingrui Liu, Francesco Orabona, Wei Zhang, Zhenxun Zhuang",2022/8/23,Journal arXiv preprint arXiv:2208.11195,"Traditional analyses in non-convex optimization typically rely on the smoothness assumption, namely requiring the gradients to be Lipschitz. However, recent evidence shows that this smoothness condition does not capture the properties of some deep learning objective functions, including the ones involving Recurrent Neural Networks and LSTMs. Instead, they satisfy a much more relaxed condition, with potentially unbounded smoothness. Under this relaxed assumption, it has been theoretically and empirically shown that the gradient-clipped SGD has an advantage over the vanilla one. In this paper, we show that clipping is not indispensable for Adam-type algorithms in tackling such scenarios: we theoretically prove that a generalized SignSGD algorithm can obtain similar convergence rates as SGD with clipping but does not need explicit clipping at all. This family of algorithms on one end recovers SignSGD and on the other end closely resembles the popular Adam algorithm. Our analysis underlines the critical role that momentum plays in analyzing SignSGD-type and Adam-type algorithms: it not only reduces the effects of noise, thus removing the need for large mini-batch in previous analyses of SignSGD-type algorithms, but it also substantially reduces the effects of unbounded smoothness and gradient norms. We also compare these algorithms with popular optimizers on a set of deep learning tasks, observing that we can match the performance of Adam while beating the others.",5
Improved schemes for episodic memory based lifelong learning algorithm,"Yunhui Guo, Mingrui Liu, Tianbao Yang, Tajana Rosing",2020/1,Journal Conference on Neural Information Processing Systems,"Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge. This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning. Recently, episodic memory based approaches such as GEM [1] and A-GEM [2] have shown remarkable performance. In this paper, we provide the first unified view of episodic memory based approaches from an optimization’s perspective. This view leads to two improved schemes for episodic memory based lifelong learning, called MEGA-I and MEGA-II. MEGA-I and MEGA-II modulate the balance between old tasks and the new task by integrating the current gradient with the gradient computed on the episodic memory. Notably, we show that GEM and A-GEM are degenerate cases of MEGA-I and MEGA-II which consistently put the same emphasis on the current task, regardless of how the loss changes over time. Our proposed schemes address this issue by using novel loss-balancing updating rules, which drastically improve the performance over GEM and A-GEM. Extensive experimental results show that the proposed schemes significantly advance the state-of-the-art on four commonly used lifelong learning benchmarks, reducing the error by up to 18%. Implementation is available at: https://github. com/yunhuiguo/MEGA",3
Fast Composite Optimization and Statistical Recovery in Federated Learning,"Yajie Bao, Michael Crawshaw, Shan Luo, Mingrui Liu",2022/6/28,Conference International Conference on Machine Learning,"As a prevalent distributed learning paradigm, Federated Learning (FL) trains a global model on a massive amount of devices with infrequent communication. This paper investigates a class of composite optimization and statistical recovery problems in the FL setting, whose loss function consists of a data-dependent smooth loss and a non-smooth regularizer. Examples include sparse linear regression using Lasso, low-rank matrix recovery using nuclear norm regularization, etc. In the existing literature, federated composite optimization algorithms are designed only from an optimization perspective without any statistical guarantees. In addition, they do not consider commonly used (restricted) strong convexity in statistical recovery problems. We advance the frontiers of this problem from both optimization and statistical perspectives. From optimization upfront, we propose a new algorithm named Fast Federated Dual Averaging for strongly convex and smooth loss and establish state-of-the-art iteration and communication complexity in the composite setting. In particular, we prove that it enjoys a fast rate, linear speedup, and reduced communication rounds. From statistical upfront, for restricted strongly convex and smooth loss, we design another algorithm, namely Multi-stage Federated Dual Averaging, and prove a high probability complexity bound with linear speedup up to optimal statistical precision. Numerical experiments in both synthetic and real data demonstrate that our methods perform better than other baselines. To the best of our knowledge, this is the first work providing fast optimization algorithms and statistical recovery guarantees for …",2
On the Last Iterate Convergence of Momentum Methods,"Xiaoyu Li, Mingrui Liu, Francesco Orabona",2022/3/20,Conference International Conference on Algorithmic Learning Theory,"SGD with Momentum (SGDM) is a widely used family of algorithms for large scale optimization of machine learning problems. Yet, when optimizing generic convex functions, no advantage is known for any SGDM algorithm over plain SGD. Moreover, even the most recent results require changes to the SGDM algorithms, like averaging of the iterates and a projection onto a bounded domain, which are rarely used in practice. In this paper, we focus on the convergence rate of the last iterate of SGDM. For the first time, we prove that for any constant momentum factor, there exists a Lipschitz and convex function for which the last iterate of SGDM suffers from a suboptimal convergence rate of  after  iterations. Based on this fact, we study a class of (both adaptive and non-adaptive) Follow-The-Regularized-Leader-based SGDM algorithms with\emph {increasing momentum} and\emph {shrinking updates}. For these algorithms, we show that the last iterate has optimal convergence  for unconstrained convex stochastic optimization problems without projections onto bounded domains nor knowledge of . Further, we show a variety of results for FTRL-based SGDM when used with adaptive stepsizes. Empirical results are shown as well.",2
Asynchronous decentralized distributed training of acoustic models,"Xiaodong Cui, Wei Zhang, Abdullah Kayi, Mingrui Liu, Ulrich Finkler, Brian Kingsbury, George Saon, David Kung",2021/10/26,"Journal IEEE/ACM Transactions on Audio, Speech, and Language Processing","Large-scale distributed training of deep acoustic models plays an important role in today’s high-performance automatic speech recognition (ASR). In this paper we investigate a variety of asynchronous decentralized distributed training strategies based on data parallel stochastic gradient descent (SGD) to show their superior performance over the commonly-used synchronous distributed training via allreduce, especially when dealing with large batch sizes. Specifically, we study three variants of asynchronous decentralized parallel SGD (ADPSGD), namely, fixed and randomized communication patterns on a ring as well as a delay-by-one scheme. We introduce a mathematical model of ADPSGD, give its theoretical convergence rate, and compare the empirical convergence behavior and straggler resilience properties of the three variants. Experiments are carried out on an IBM supercomputer for training deep long …",2
Stochastic non-convex optimization with strong high probability second-order convergence,"Mingrui Liu, Tianbao Yang",2017/10/25,Journal arXiv preprint arXiv:1710.09447,"In this paper, we study stochastic non-convex optimization with non-convex random functions. Recent studies on non-convex optimization revolve around establishing second-order convergence, i.e., converging to a nearly second-order optimal stationary points. However, existing results on stochastic non-convex optimization are limited, especially with a high probability second-order convergence. We propose a novel updating step (named NCG-S) by leveraging a stochastic gradient and a noisy negative curvature of a stochastic Hessian, where the stochastic gradient and Hessian are based on a proper mini-batch of random functions. Building on this step, we develop two algorithms and establish their high probability second-order convergence. To the best of our knowledge, the proposed stochastic algorithms are the first with a second-order convergence in {\it high probability} and a time complexity that is {\it almost linear} in the problem's dimensionality.",2
A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks,"Mingrui Liu, Zhenxun Zhuang, Yunwei Lei, Chunyang Liao",2022/5/10,Journal arXiv preprint arXiv:2205.05040,"In distributed training of deep neural networks or Federated Learning (FL), people usually run Stochastic Gradient Descent (SGD) or its variants on each machine and communicate with other machines periodically. However, SGD might converge slowly in training some deep neural networks (e.g., RNN, LSTM) because of the exploding gradient issue. Gradient clipping is usually employed to address this issue in the single machine setting, but exploring this technique in the FL setting is still in its infancy: it remains mysterious whether the gradient clipping scheme can take advantage of multiple machines to enjoy parallel speedup. The main technical difficulty lies in dealing with nonconvex loss function, non-Lipschitz continuous gradient, and skipping communication rounds simultaneously. In this paper, we explore a relaxed-smoothness assumption of the loss landscape which LSTM was shown to satisfy in previous works and design a communication-efficient gradient clipping algorithm. This algorithm can be run on multiple machines, where each machine employs a gradient clipping scheme and communicate with other machines after multiple steps of gradient-based updates. Our algorithm is proved to have  iteration complexity for finding an -stationary point, where  is the number of machines. This indicates that our algorithm enjoys linear speedup. We prove this result by introducing novel analysis techniques of estimating truncated random variables, which we believe are of independent interest. Our experiments on several benchmark datasets and various scenarios demonstrate that our algorithm indeed exhibits fast convergence …",1
On the Initialization for Convex-Concave Min-max Problems,"Mingrui Liu, Francesco Orabona",2022/3/20,Conference International Conference on Algorithmic Learning Theory,"Convex-concave min-max problems are ubiquitous in machine learning, and people usually utilize first-order methods (eg, gradient descent ascent) to find the optimal solution. One feature which separates convex-concave min-max problems from convex minimization problems is that the best known convergence rates for min-max problems have an explicit dependence on the size of the domain, rather than on the distance between initial point and the optimal solution. This means that the convergence speed does not have any improvement even if the algorithm starts from the optimal solution, and hence, is oblivious to the initialization. Here, we show that strict-convexity-strict-concavity is sufficient to get the convergence rate to depend on the initialization. We also show how different algorithms can asymptotically achieve initialization-dependent convergence rates on this class of functions. Furthermore, we show that the so-called “parameter-free” algorithms allow to achieve improved initialization-dependent asymptotic rates without any learning rate to tune. In addition, we utilize this particular parameter-free algorithm as a subroutine to design a new algorithm, which achieves a novel non-asymptotic fast rate for strictly-convex-strictly-concave min-max problems with a growth condition and H {ö} lder continuous solution mapping. Experiments are conducted to verify our theoretical findings and demonstrate the effectiveness of the proposed algorithms.",1
Loss landscape dependent self-adjusting learning rates in decentralized stochastic gradient descent,"Wei Zhang, Mingrui Liu, Yu Feng, Xiaodong Cui, Brian Kingsbury, Yuhai Tu",2021/12/2,Journal arXiv preprint arXiv:2112.01433,"Distributed Deep Learning (DDL) is essential for large-scale Deep Learning (DL) training. Synchronous Stochastic Gradient Descent (SSGD) 1 is the de facto DDL optimization method. Using a sufficiently large batch size is critical to achieving DDL runtime speedup. In a large batch setting, the learning rate must be increased to compensate for the reduced number of parameter updates. However, a large learning rate may harm convergence in SSGD and training could easily diverge. Recently, Decentralized Parallel SGD (DPSGD) has been proposed to improve distributed training speed. In this paper, we find that DPSGD not only has a system-wise run-time benefit but also a significant convergence benefit over SSGD in the large batch setting. Based on a detailed analysis of the DPSGD learning dynamics, we find that DPSGD introduces additional landscape-dependent noise that automatically adjusts the effective learning rate to improve convergence. In addition, we theoretically show that this noise smoothes the loss landscape, hence allowing a larger learning rate. We conduct extensive studies over 18 state-of-the-art DL models/tasks and demonstrate that DPSGD often converges in cases where SSGD diverges for large learning rates in the large batch setting. Our findings are consistent across two different application domains: Computer Vision (CIFAR10 and ImageNet-1K) and Automatic Speech Recognition (SWB300 and SWB2000), and two different types of neural network models: Convolutional Neural Networks and Long Short-Term Memory Recurrent Neural Networks.",1
Nonconvex min-max optimization in deep learning: algorithms and applications,Mingrui Liu,2020,Institution The University of Iowa,"Nonconvex min-max optimization receives increasing attention in modern machine learning, especially in the context of deep learning. Examples include stochastic AUC maximization with deep neural networks and Generative Adversarial Nets (GANs), which correspond to a nonconvex-concave and nonconvex-nonconcave min-max problem respectively. The classical theory of min-max optimization mainly focuses on convex-concave setting, which is not applicable for deep learning applications with nonconvex min-max formulation. A natural question is proposed---how to design provably efficient algorithms for nonconvex min-max problems in deep learning? To answer this question, this dissertation focuses on the following four concrete aspects:",1
EPISODE: Episodic Gradient Clipping with Periodic Resampled Corrections for Federated Learning with Heterogeneous Data,"Michael Crawshaw, Yajie Bao, Mingrui Liu",2023/2/14,Journal arXiv preprint arXiv:2302.07155,"Gradient clipping is an important technique for deep neural networks with exploding gradients, such as recurrent neural networks. Recent studies have shown that the loss functions of these networks do not satisfy the conventional smoothness condition, but instead satisfy a relaxed smoothness condition, i.e., the Lipschitz constant of the gradient scales linearly in terms of the gradient norm. Due to this observation, several gradient clipping algorithms have been developed for nonconvex and relaxed-smooth functions. However, the existing algorithms only apply to the single-machine or multiple-machine setting with homogeneous data across machines. It remains unclear how to design provably efficient gradient clipping algorithms in the general Federated Learning (FL) setting with heterogeneous data and limited communication rounds. In this paper, we design EPISODE, the very first algorithm to solve FL problems with heterogeneous data in the nonconvex and relaxed smoothness setting. The key ingredients of the algorithm are two new techniques called \textit{episodic gradient clipping} and \textit{periodic resampled corrections}. At the beginning of each round, EPISODE resamples stochastic gradients from each client and obtains the global averaged gradient, which is used to (1) determine whether to apply gradient clipping for the entire round and (2) construct local gradient corrections for each client. Notably, our algorithm and analysis provide a unified framework for both homogeneous and heterogeneous data under any noise level of the stochastic gradient, and it achieves state-of-the-art complexity results. In particular, we prove that …",
Updating of statistical sets for decentralized distributed training of a machine learning model,"Xiaodong Cui, Wei Zhang, Mingrui Liu, Abdullah Kayi, Youssef Mroueh, Alper Buyuktosunoglu",2022/8/4,Patent office US,"Systems, computer-implemented methods, and computer program products to facilitate updating, such as averaging and/or training, of one or more statistical sets are provided. According to an embodiment, a system can comprise a memory that stores computer executable components and a processor that executes the computer executable components stored in the memory. The computer executable components can include a computing component that averages a statistical set, provided by the system, with an additional statistical set, that is compatible with the statistical set, to compute an averaged statistical set, where the additional statistical set is obtained from a selected additional system of a plurality of additional systems. The computer executable components also can include a selecting component that selects the selected additional system according to a randomization pattern.",
Decentralized parallel min/max optimization,"Mingrui Liu, Wei Zhang, Youssef Mroueh, Xiaodong Cui, Jarret Ross, Payel Das",2022/4/28,Patent office US,"Techniques are provided for decentralized parallel min/max optimizations. In one embodiment, the techniques involve generating gradients based on a first set of weights associated with a first node of a neural network, exchanging the first set of weights with a second set of weights associated with a second node, generating an average weight based on the first set of weights and the second set of weights, and updating the first set of weights and the second set of weights via a decentralized parallel optimistic stochastic gradient (DPOSG) algorithm based on the gradients and the average weight.",
LEARNING WITH LONG-TERM REMEMBERING: FOL-LOWING THE LEAD OF MIXED STOCHASTIC GRADI,"Yunhui Guo, Mingrui Liu, Tianbao Yang, Tajana Rosing",2019/9,Journal arXiv preprint arXiv:1909.11763,"Current deep neural networks can achieve remarkable performance on a single task. However, when the deep neural network is continually trained on a sequence of tasks, it seems to gradually forget the previous learned knowledge. This phenomenon is referred to as catastrophic forgetting and motivates the field called lifelong learning. The central question in lifelong learning is how to enable deep neural networks to maintain performance on old tasks while learning a new task. In this paper, we introduce a novel and effective lifelong learning algorithm, called MixEd stochastic GrAdient (MEGA), which allows deep neural networks to acquire the ability of retaining performance on old tasks while learning new tasks. MEGA modulates the balance between old tasks and the new task by integrating the current gradient with the gradient computed on a small reference episodic memory. Extensive experimental results show that the proposed MEGA algorithm significantly advances the state-of-the-art on all four commonly used lifelong learning benchmarks, reducing the error by up to 18%.",
Fast Online Learning Algorithms for Large-Scale Imbalanced Data,Mingrui Liu,2019/1/3,"Description Issue: What if#(positive samples)≫#(negative samples)? type label number of samples malicious-1 20 benign+ 1 10000 Disadvantage: Consider the trivial predictor with w= 0, the classifier predict every sample to be+ 1, getting classification accuracy 99.8% Conclusion: the ERM formulation is not suitable for imbalanced data","Issue: What if#(positive samples)≫#(negative samples)? type label number of samples malicious-1 20 benign+ 1 10000 Disadvantage: Consider the trivial predictor with w= 0, the classifier predict every sample to be+ 1, getting classification accuracy 99.8% Conclusion: the ERM formulation is not suitable for imbalanced data",
Imbalanced Lifelong Learning with AUC Maximization,"Xiangyu Zhu, Jie Hao, Yunhui Guo, Mingrui Liu","Imbalanced data is ubiquitous in machine learning, such as medical or fine-grained image datasets. The existing continual learning methods employ various techniques such as balanced sampling to improve classification accuracy in this setting. However, classification accuracy is not a suitable metric for imbalanced data, and hence these methods may not obtain a good classifier as measured by other metrics (e.g., recall, F1-score, Area under the ROC Curve). In this paper, we propose a solution to enable efficient imbalanced continual learning by designing an algorithm to effectively maximize one widely used metric in an imbalanced data setting: Area Under the ROC Curve (AUC). We find that simply replacing accuracy with AUC will cause \textit{gradient interference problem} due to the imbalanced data distribution. To address this issue, we propose a new algorithm, namely DIANA, which performs a novel synthesis of model \underline{D}ecoupl\underline{I}ng \underline{AN}d \underline{A}lignment. In particular, the algorithm updates two models simultaneously: one focuses on learning the current knowledge while the other concentrates on reviewing previously-learned knowledge, and the two models gradually align during training. We conduct extensive experiments on datasets across various imbalanced domains, ranging from natural images to medical and satellite images. The results show that DIANA achieves state-of-the-art performance on all the imbalanced datasets compared with several competitive baselines. We further consider standard balanced benchmarks used in lifelong learning to show the effectiveness of DIANA as a …","Scholar articles Imbalanced Lifelong Learning with AUC MaximizationX Zhu, J Hao, Y Guo, M LiuRelated articles ","Imbalanced data is ubiquitous in machine learning, such as medical or fine-grained image datasets. The existing continual learning methods employ various techniques such as balanced sampling to improve classification accuracy in this setting. However, classification accuracy is not a suitable metric for imbalanced data, and hence these methods may not obtain a good classifier as measured by other metrics (e.g., recall, F1-score, Area under the ROC Curve). In this paper, we propose a solution to enable efficient imbalanced continual learning by designing an algorithm to effectively maximize one widely used metric in an imbalanced data setting: Area Under the ROC Curve (AUC). We find that simply replacing accuracy with AUC will cause \textit{gradient interference problem} due to the imbalanced data distribution. To address this issue, we propose a new algorithm, namely DIANA, which performs a novel synthesis of model \underline{D}ecoupl\underline{I}ng \underline{AN}d \underline{A}lignment. In particular, the algorithm updates two models simultaneously: one focuses on learning the current knowledge while the other concentrates on reviewing previously-learned knowledge, and the two models gradually align during training. We conduct extensive experiments on datasets across various imbalanced domains, ranging from natural images to medical and satellite images. The results show that DIANA achieves state-of-the-art performance on all the imbalanced datasets compared with several competitive baselines. We further consider standard balanced benchmarks used in lifelong learning to show the effectiveness of DIANA as a …",
Why Does Decentralized Training Outperform Synchronous Training In The Large Batch Setting?,"Wei Zhang, Mingrui Liu, Yu Feng, Brian Kingsbury, Yuhai Tu","Distributed Deep Learning (DDL) is essential for large-scale Deep Learning (DL) training. Using a sufficiently large batch size is critical to achieving DDL runtime speedup. In a large batch setting, the learning rate must be increased to compensate for the reduced number of parameter updates. However, a large batch size may converge to sharp minima with poor generalization, and a large learning rate may harm convergence. Synchronous Stochastic Gradient Descent (SSGD) is the de facto DDL optimization method. Recently, Decentralized Parallel SGD (DPSGD) has been proven to achieve a similar convergence rate as SGD and to guarantee linear speedup for non-convex optimization problems. While there was anecdotal evidence that DPSGD outperforms SSGD in the large-batch setting, no systematic study has been conducted to explain why this is the case. Based on a detailed analysis of the DPSGD learning dynamics, we find that DPSGD introduces additional landscape-dependent noise, which has two benefits in the large-batch setting: 1) it automatically adjusts the learning rate to improve convergence; 2) it enhances weight space search by escaping local traps (e.g., saddle points) to find flat minima with better generalization. We conduct extensive studies over 12 state-of-the-art DL models/tasks and demonstrate that DPSGD consistently outperforms SSGD in the large batch setting;    and DPSGD converges in cases where SSGD diverges for large learning rates. Our findings are consistent across different application domains, Computer Vision and Automatic Speech Recognition, and different neural network models, Convolutional …","Scholar articles Why Does Decentralized Training Outperform Synchronous Training In The Large Batch Setting?W Zhang, M Liu, Y Feng, B Kingsbury, Y TuRelated articles ","Distributed Deep Learning (DDL) is essential for large-scale Deep Learning (DL) training. Using a sufficiently large batch size is critical to achieving DDL runtime speedup. In a large batch setting, the learning rate must be increased to compensate for the reduced number of parameter updates. However, a large batch size may converge to sharp minima with poor generalization, and a large learning rate may harm convergence. Synchronous Stochastic Gradient Descent (SSGD) is the de facto DDL optimization method. Recently, Decentralized Parallel SGD (DPSGD) has been proven to achieve a similar convergence rate as SGD and to guarantee linear speedup for non-convex optimization problems. While there was anecdotal evidence that DPSGD outperforms SSGD in the large-batch setting, no systematic study has been conducted to explain why this is the case. Based on a detailed analysis of the DPSGD learning dynamics, we find that DPSGD introduces additional landscape-dependent noise, which has two benefits in the large-batch setting: 1) it automatically adjusts the learning rate to improve convergence; 2) it enhances weight space search by escaping local traps (e.g., saddle points) to find flat minima with better generalization. We conduct extensive studies over 12 state-of-the-art DL models/tasks and demonstrate that DPSGD consistently outperforms SSGD in the large batch setting;    and DPSGD converges in cases where SSGD diverges for large learning rates. Our findings are consistent across different application domains, Computer Vision and Automatic Speech Recognition, and different neural network models, Convolutional …",
Attacking Lifelong Learning Models with Gradient Reversion,"Yunhui Guo, Mingrui Liu, Yandong Li, Liqiang Wang, Tianbao Yang, Tajana Rosing","Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.","Scholar articles Attacking Lifelong Learning Models with Gradient ReversionY Guo, M Liu, Y Li, L Wang, T Yang, T RosingRelated articles All 2 versions ","Lifelong  learning  aims  at  avoiding  the  catastrophic  forgetting  problem  of  traditional supervised learning  models.   Episodic memory based  lifelong learning methods such as A-GEM (Chaudhry et al., 2018b) are shown to achieve the state-of-the-art results across the benchmarks. In A-GEM, a small episodic memory is utilized to store a random subset of the examples from previous tasks.  While the model is trained on a new task, a reference gradient is computed on the episodic memory to guide the direction of the current update.  While A-GEM has strong continual learning ability,  it is not clear that if it can retain the performance in the presence of adversarial attacks.  In this paper, we examine the robustness ofA-GEM against adversarial attacks to the examples in the episodic memory.  We evaluate the effectiveness of traditional attack methods such as FGSM and PGD.The results show that A-GEM still possesses strong continual learning ability in the  presence  of  adversarial  examples  in  the  memory  and  simple  defense  techniques such as label smoothing can further alleviate the adversarial effects.  We presume that traditional attack methods are specially designed for standard supervised learning models rather than lifelong learning models. we therefore propose a principled way for attacking A-GEM called gradient reversion(GREV) which is shown to be more effective.  Our results indicate that future lifelong learning research should bear adversarial attacks in mind to develop more robust lifelong learning algorithms.",
Provable Non-Convex Min-Max Optimization,"Mingrui Liu, Hassan Rafique, Qihang Lin, Tianbao Yang","In this paper, we propose an efficient stochastic subgradient method for solving a broad class of non-convex min-max problems and establish its iteration complexities for different convergence measures depending on whether the problem is concave in terms of the variable of maximization. When the objective is weakly convex in terms of min variable and concave in terms of the max variable, we prove that the proposed algorithm converges to a nearly ϵ-stationary solution of the equivalent minimization problem with a complexity of O (1/ϵ6). When the objective is weakly convex in terms of the min variable and weakly concave in terms of the max variable, we prove the algorithm converges a nearly ϵ-stationary solution of the min-max problem with the same complexity of O (1/ϵ6). To the best of our knowledge, these are the first non-asymptotic convergence results of stochastic optimization for solving non-convex min-max problems.","Scholar articles Provable Non-Convex Min-Max OptimizationM Liu, H Rafique, Q Lin, T YangRelated articles All 2 versions ","In this paper, we propose an efficient stochastic subgradient method for solving a broad class of non-convex min-max problems and establish its iteration complexities for different convergence measures depending on whether the problem is concave in terms of the variable of maximization. When the objective is weakly convex in terms of min variable and concave in terms of the max variable, we prove that the proposed algorithm converges to a nearly ϵ-stationary solution of the equivalent minimization problem with a complexity of O (1/ϵ6). When the objective is weakly convex in terms of the min variable and weakly concave in terms of the max variable, we prove the algorithm converges a nearly ϵ-stationary solution of the min-max problem with the same complexity of O (1/ϵ6). To the best of our knowledge, these are the first non-asymptotic convergence results of stochastic optimization for solving non-convex min-max problems.",
Supplementary Material for “Fast Stochastic AUC Maximization with O (1/n)-Convergence Rate”,"Mingrui Liu, Xiaoxuan Zhang, Zaiyi Chen, Xiaoyu Wang, Tianbao Yang","•(Randomized version of vector concentration inequality) Suppose T is a random variable taking value on N+, and let X1,..., XT∈ Rd be iid random variables. If φ: Rd→ H, where H is a Hilbert space endowed with norm·(actually we can take H to be Rd endowed with infinity norm). Suppose B= sup x∈ Rd φ (x)<∞. Then we have with probability at least 1− δ,","Scholar articles Supplementary Material for “Fast Stochastic AUC Maximization with O (1/n)-Convergence Rate”M Liu, X Zhang, Z Chen, X Wang, T YangRelated articles All 6 versions ","•(Randomized version of vector concentration inequality) Suppose T is a random variable taking value on N+, and let X1,..., XT∈ Rd be iid random variables. If φ: Rd→ H, where H is a Hilbert space endowed with norm·(actually we can take H to be Rd endowed with infinity norm). Suppose B= sup x∈ Rd φ (x)<∞. Then we have with probability at least 1− δ,",
"Supplementary Material for"" Adaptive Accelerated Gradient Converging Method under Hölderian Error Bound Condition","Mingrui Liu, Tianbao Yang","Proposition 10.[3, Theorem 5 in v3] Let f: H→(−∞,+∞] be a proper, convex and lower semi-continuous with min f= f∗. Let r0> 0, ϕ∈{ϕ∈ C0 [0, r0)∩ C1 (0, r0), ϕ (0)= 0, ϕ is concave, ϕ> 0}, c> 0, ρ> 0, and x∈ arg min f. If sϕ (s)≥ cϕ (s) for all s∈(0, r0), and ϕ (f (x)− f∗)≥ D (x, arg min f) for all x∈[0< f< r0]∩ B (x, ρ), then ϕ (f (x)− f∗)∂ f (x) 2≥ c for all x∈[0< f< r0]∩ B (x, ρ).","Scholar articles Supplementary Material for"" Adaptive Accelerated Gradient Converging Method under Hölderian Error Bound ConditionM Liu, T YangRelated articles All 5 versions ","Proposition 10.[3, Theorem 5 in v3] Let f: H→(−∞,+∞] be a proper, convex and lower semi-continuous with min f= f∗. Let r0> 0, ϕ∈{ϕ∈ C0 [0, r0)∩ C1 (0, r0), ϕ (0)= 0, ϕ is concave, ϕ> 0}, c> 0, ρ> 0, and x∈ arg min f. If sϕ (s)≥ cϕ (s) for all s∈(0, r0), and ϕ (f (x)− f∗)≥ D (x, arg min f) for all x∈[0< f< r0]∩ B (x, ρ), then ϕ (f (x)− f∗)∂ f (x) 2≥ c for all x∈[0< f< r0]∩ B (x, ρ).",
