titles,authors,date,source,descriptions,citations
Saliency detection on light field,"Nianyi Li, Jinwei Ye, Yu Ji, Haibin Ling, Jingyi Yu",2014,Conference Proceedings of the IEEE conference on computer vision and pattern recognition,"Existing saliency detection approaches use images as inputs and are sensitive to foreground/background similarities, complex background textures, and occlusions. We explore the problem of using light fields as input for saliency detection. Our technique is enabled by the availability of commercial plenoptic cameras that capture the light field of a scene in a single shot. We show that the unique refocusing capability of light fields provides useful focusness, depths, and objectness cues. We further develop a new saliency detection algorithm tailored for light fields. To validate our approach, we acquire a light field database of a range of indoor and outdoor scenes and generate the ground truth saliency map. Experiments show that our saliency detection scheme can robustly handle challenging scenarios such as similar foreground and background, cluttered background, complex occlusions, etc., and achieve high accuracy and robustness.",432
Reconstructing gas flows using light-path approximation,"Yu Ji, Jinwei Ye, Jingyi Yu",2013,Conference Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,"Transparent gas flows are difficult to reconstruct: the refractive index field (RIF) within the gas volume is uneven and rapidly evolving, and correspondence matching under distortions is challenging. We present a novel computational imaging solution by exploiting the light field probe (LFProbe). A LF-probe resembles a view-dependent pattern where each pixel on the pattern maps to a unique ray. By observing the LF-probe through the gas flow, we acquire a dense set of ray-ray correspondences and then reconstruct their light paths. To recover the RIF, we use Fermat's Principle to correlate each light path with the RIF via a Partial Differential Equation (PDE). We then develop an iterative optimization scheme to solve for all light-path PDEs in conjunction. Specifically, we initialize the light paths by fitting Hermite splines to ray-ray correspondences, discretize their PDEs onto voxels, and solve a large, over-determined PDE system for the RIF. The RIF can then be used to refine the light paths. Finally, we alternate the RIF and light-path estimations to improve the reconstruction. Experiments on synthetic and real data show that our approach can reliably reconstruct small to medium scale gas flows. In particular, when the flow is acquired by a small number of cameras, the use of ray-ray correspondences can greatly improve the reconstruction.",51
Angular domain reconstruction of dynamic 3d fluid surfaces,"Jinwei Ye, Yu Ji, Feng Li, Jingyi Yu",2012/6/16,Conference 2012 IEEE Conference on Computer Vision and Pattern Recognition,"We present a novel and simple computational imaging solution to robustly and accurately recover 3D dynamic fluid surfaces. Traditional specular surface reconstruction schemes place special patterns (checkerboard or color patterns) beneath the fluid surface to establish point-pixel correspondences. However, point-pixel correspondences alone are insufficient to recover surface normal or height and they rely on additional constraints to resolve the ambiguity. In this paper, we exploit using Bokode - a computational optical device that emulates a pinhole projector - for capturing ray-ray correspondences which can then be used to directly recover the surface normals. We further develop a robust feature matching algorithm based on the Active-Appearance Model to robustly establishing ray-ray correspondences. Our solution results in an angularly sampled normal field and we derive a new angular-domain surface …",38
Dynamic fluid surface reconstruction using deep neural network,"Simron Thapa, Nianyi Li, Jinwei Ye",2020,Conference Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,"Recovering the dynamic fluid surface is a long-standing challenging problem in computer vision. Most existing image-based methods require multiple views or a dedicated imaging system. Here we present a learning-based single-image approach for 3D fluid surface reconstruction. Specifically, we design a deep neural network that estimates the depth and normal maps of a fluid surface by analyzing the refractive distortion of a reference background image. Due to the dynamic nature of fluid surfaces, our network uses recurrent layers that carry temporal information from previous frames to achieve spatio-temporally consistent reconstruction given a video input. Due to the lack of fluid data, we synthesize a large fluid dataset using physics-based fluid modeling and rendering techniques for network training and validation. Through experiments on simulated and real captured fluid images, we demonstrate that our proposed deep neural network trained on our fluid dataset can recover dynamic 3D fluid surfaces with high accuracy.",19
Ray geometry in non-pinhole cameras: a survey,"Jinwei Ye, Jingyi Yu",2014/1,Journal The Visual Computer,"A pinhole camera collects rays passing through a common 3D point and its image resembles what would be seen by human eyes. In contrast, a non-pinhole (multi-perspective) camera combines rays collected by different viewpoints. Despite their incongruity of view, their images are able to preserve spatial coherence and can depict, within a single context, details of a scene that are simultaneously inaccessible from a single view, yet easily interpretable by a viewer. In this paper, we thoroughly discuss the design, modeling, and implementation of a broad class of non-pinhole cameras and their applications in computer graphics and vision. These include mathematical (conceptual) camera models such as the General Linear Cameras and real non-pinhole cameras such as catadioptric cameras and projectors. A unique component of this paper is a ray geometry analysis that uniformly models these non …",17
3d reconstruction of mirror-type objects using efficient ray coding,"Siu-Kei Tin, Jinwei Ye, Mahdi Nezamabadi, Can Chen",2016/5/13,Conference 2016 IEEE International Conference on Computational Photography (ICCP),"Mirror-type specular objects are difficult to reconstruct: they do not possess their own appearance and the reflections from environment are view-dependent. In this paper, we present a novel computational imaging solution for reconstructing the mirror-type specular objects. Specifically, we adopt a two-layer liquid crystal display (LCD) setup to encode the illumination directions. We devise an efficient ray coding scheme by only considering the useful rays. To recover the mirror-type surface, we derive a normal integration scheme under the perspective camera model. Since the resulting surface is determined up to a scale, we develop a single view approach to resolve the scale ambiguity. To acquire the object surface as completely as possible, we further develop a multiple-surface fusion algorithm to combine the surfaces recovered from different viewpoints. Both synthetic and real experiments demonstrate that our …",13
Learning to dodge a bullet: Concyclic view morphing via deep learning,"Shi Jin, Ruiynag Liu, Yu Ji, Jinwei Ye, Jingyi Yu",2018,Conference Proceedings of the European Conference on Computer Vision (ECCV),"The bullet-time effect, presented in feature film``The Matrix"", has been widely adopted in feature films and TV commercials to create an amazing stopping-time illusion. Producing such visual effects, however, typically requires using a large number of cameras/images surrounding the subject. In this paper, we present a learning-based solution that is capable of producing the bullet-time effect from only a small set of images. Specifically, we present a view morphing framework that can synthesize smooth and realistic transitions along extit {a circular view path} using as few as three reference images. We apply a novel cyclic rectification technique to align the reference images onto a common circle and then feed the rectified results into a deep network to predict its motion field and per-pixel visibility for new view interpolation. Comprehensive experiments on synthetic and real data show that our new framework outperforms the state-of-the-art and provides an inexpensive and practical solution for producing the bullet-time effects.",11
Shape and reflectance reconstruction using concentric multi-spectral light field,"Mingyuan Zhou, Yuqi Ding, Yu Ji, S Susan Young, Jingyi Yu, Jinwei Ye",2020/4/13,Journal IEEE Transactions on Pattern Analysis and Machine Intelligence,"Recovering the shape and reflectance of non-Lambertian surfaces remains a challenging problem in computer vision since the view-dependent appearance invalidates traditional photo-consistency constraint. In this paper, we introduce a novel concentric multi-spectral light field (CMSLF) design that is able to recover the shape and reflectance of surfaces of various materials in one shot. Our CMSLF system consists of an array of cameras arranged on concentric circles where each ring captures a specific spectrum. Coupled with a multi-spectral ring light, we are able to sample viewpoint and lighting variations in a single shot via spectral multiplexing. We further show that our concentric camera and light source setting results in a unique single-peak pattern in specularity variations across viewpoints. This property enables robust depth estimation for specular points. To estimate depth and multi-spectral reflectance …",9
A rotational stereo model based on xslit imaging,"Jinwei Ye, Yu Ji, Jingyi Yu",2013,Conference Proceedings of the IEEE International Conference on Computer Vision,"Traditional stereo matching assumes perspective viewing cameras under a translational motion: the second camera is translated away from the first one to create parallax. In this paper, we investigate a different, rotational stereo model on a special multi-perspective camera, the XSlit camera [9, 24]. We show that rotational XSlit (R-XSlit) stereo can be effectively created by fixing the sensor and slit locations but switching the two slits' directions. We first derive the epipolar geometry of R-XSlit in the 4D light field ray space. Our derivation leads to a simple but effective scheme for locating corresponding epipolar"" curves"". To conduct stereo matching, we further derive a new disparity term in our model and develop a patch-based graph-cut solution. To validate our theory, we assemble an XSlit lens by using a pair of cylindrical lenses coupled with slit-shaped apertures. The XSlit lens can be mounted on commodity cameras where the slit directions are adjustable to form desirable R-XSlit pairs. We show through experiments that R-XSlit provides a potentially advantageous imaging system for conducting fixed-location, dynamic baseline stereo.",9
System and method for face recognition using three dimensions,"Shiqiong Susan Young, Jinwei Ye",2018/5/1,Patent office US,"A system for facial recognition comprising at least one processor; at least one input operatively connected to the at least one processor; a database configured to store three-dimensional facial image data comprising facial feature coordinates in a predetermined common plane; the at least one processor configured to locate three-dimensional facial features in the image of the subject, estimate three-dimensional facial feature location coordinates in the image of the subject, obtain the three-dimensional facial feature location coordinates and orientation parameters in a coordinate system in which the facial features are located in the predetermined common plane; and compare the location of the facial feature coordinates of the subject to images of people in the database; whereby recognition, comparison and/or likeness of the facial images is determined by comparing the predetermined common plane facial feature …",8
Unsupervised non-rigid image distortion removal via grid deformation,"Nianyi Li, Simron Thapa, Cameron Whyte, Albert W Reed, Suren Jayasuriya, Jinwei Ye",2021,Conference Proceedings of the IEEE/CVF International Conference on Computer Vision,"Many computer vision problems face difficulties when imaging through turbulent refractive media (eg, air and water) due to the refraction and scattering of light. These effects cause geometric distortion that requires either handcrafted physical priors or supervised learning methods to remove. In this paper, we present a novel unsupervised network to recover the latent distortion-free image. The key idea is to model non-rigid distortions as deformable grids. Our network consists of a grid deformer that estimates the distortion field and an image generator that outputs the distortion-free image. By leveraging the positional encoding operator, we can simplify the network structure while maintaining fine spatial details in the recovered images. Our method doesn't need to be trained on labeled data and has good transferability across various turbulent image datasets with different types of distortions. Extensive experiments on both simulated and real-captured turbulent images demonstrate that our method can remove both air and water distortions without much customization.",7
Robust 3D human motion reconstruction via dynamic template construction,"Zhong Li, Yu Ji, Wei Yang, Jinwei Ye, Jingyi Yu",2017/10/10,Conference 2017 International Conference on 3D Vision (3DV),"In multi-view human body capture systems, the recovered 3D geometry or even the acquired imagery data can be heavily corrupted due to occlusions, noise, limited fieldof- view, etc. Direct estimation of 3D pose, body shape or motion on these low-quality data has been traditionally challenging.In this paper, we present a graph-based non-rigid shape registration framework that can simultaneously recover 3D human body geometry and estimate pose/motion at high fidelity.Our approach first generates a global full-body template by registering all poses in the acquired motion sequence.We then construct a deformable graph by utilizing the rigid components in the global template. We directly warp the global template graph back to each motion frame in order to fill in missing geometry. Specifically,we combine local rigidity and temporal coherence constraints to maintain geometry and motion consistencies …",7
Manhattan scene understanding via xslit imaging,"Jinwei Ye, Yu Ji, Jingyi Yu",2013,Conference Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,"A Manhattan World (MW)[3] is composed of planar surfaces and parallel lines aligned with three mutually orthogonal principal axes. Traditional MW understanding algorithms rely on geometry priors such as the vanishing points and reference (ground) planes for grouping coplanar structures. In this paper, we present a novel single-image MW reconstruction algorithm from the perspective of nonpinhole cameras. We show that by acquiring the MW using an XSlit camera, we can instantly resolve coplanarity ambiguities. Specifically, we prove that parallel 3D lines map to 2D curves in an XSlit image and they converge at an XSlit Vanishing Point (XVP). In addition, if the lines are coplanar, their curved images will intersect at a second common pixel that we call Coplanar Common Point (CCP). CCP is a unique image feature in XSlit cameras that does not exist in pinholes. We present a comprehensive theory to analyze XVPs and CCPs in a MW scene and study how to recover 3D geometry in a complex MW scene from XVPs and CCPs. Finally, we build a prototype XSlit camera by using two layers of cylindrical lenses. Experimental results on both synthetic and real data show that our new XSlitcamera-based solution provides an effective and reliable solution for MW understanding.",7
Polarimetric helmholtz stereopsis,"Yuqi Ding, Yu Ji, Mingyuan Zhou, Sing Bing Kang, Jinwei Ye",2021,Conference Proceedings of the IEEE/CVF International Conference on Computer Vision,"Helmholtz stereopsis (HS) exploits the reciprocity principle of light propagation (ie, the Helmholtz reciprocity) for 3D reconstruction of surfaces with arbitrary reflectance. In this paper, we present the polarimetric Helmholtz stereopsis (polar-HS), which extends the classical HS by considering the polarization state of light in the reciprocal paths. With the additional phase information from polarization, polar-HS requires only one reciprocal image pair. We formulate new reciprocity and diffuse/specular polarimetric constraints to recover surface depths and normals using an optimization framework. Using a hardware prototype, we show that our approach produces high-quality 3D reconstruction for different types of surfaces, ranging from diffuse to highly specular.",6
Coplanar common points in non-centric cameras,"Wei Yang, Yu Ji, Jinwei Ye, S Susan Young, Jingyi Yu",2014,"Conference Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13","Discovering and extracting new image features pertaining to scene geometry is important to 3D reconstruction and scene understanding. Examples include the classical vanishing points observed in a centric camera and the recent coplanar common points (CCPs) in a crossed-slit camera [21,17]. A CCP is a point in the image plane corresponding to the intersection of the projections of all lines lying on a common 3D plane. In this paper, we address the problem of determining CCP existence in general non-centric cameras. We first conduct a ray-space analysis to show that finding the CCP of a 3D plane is equivalent to solving an array of ray constraint equations. We then derive the necessary and sufficient conditions for CCP to exist in an arbitrary non-centric camera such as non-centric catadioptric mirrors. Finally, we present robust algorithms for extracting the CCPs from a single image and validate our …",6
Depth-of-field and coded aperture imaging on xslit lens,"Jinwei Ye, Yu Ji, Wei Yang, Jingyi Yu",2014,"Conference Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part III 13","Recent coded aperture imaging systems have shown great success in scene reconstruction, extended depth-of-field and light field imaging. By far nearly all solutions are built on top of commodity cameras equipped with a single spherical lens. In this paper, we explore coded aperture solutions on a special non-centric lens called the crossed-slit (XSlit) lens. An XSlit lens uses a relay of two orthogonal cylindrical lenses, each coupled with a slit-shaped aperture. Through ray geometry analysis, we first show that the XSlit lens produces a different and potentially advantageous depth-of-field than the regular spherical lens. We then present a coded aperture strategy that individually encodes each slit aperture, one with broadband code and the other with high depth discrepancy code, for scene recovery. Synthetic and real experiments validate our theory and demonstrate the advantages of XSlit coded aperture …",5
XSlit camera,"Jingyi Yu, Jinwei Ye, Yu Ji",2020/1/28,Patent office US,"Light representing a scene is directed through a lens module coupled to an imaging sensor. The lens module includes: first and second cylindrical lenses positioned along an optical axis of the imaging sensor, and first and second slit-shaped apertures disposed on the respective first and second cylindrical lenses. A cylindrical axis of the second cylindrical lens is arranged at an angle away from parallel with respect to a cylindrical axis of the first cylindrical lens. The light directed through the lens module is captured by the imaging sensor to form at least one multi-perspective image. The at least one multi-perspective image is processed to determine a reconstruction characteristic of the scene.",4
Measuring shape of specular objects by local projection of coded patterns,"Siu-Kei Tin, Jinwei Ye",2019/1/1,Patent office US,The shape of a specular object is measured by illumination of the object by a light field generated by two or more spaced apart layers controllable to display multiple patterns that are predetermined relative to a bounding volume within which the object is positioned. The patterns code a sparse subset of the multitude of light rays that can be generated by the layers to those that can actually reach the bounding volume. A process is described by which a sparse coding of the light rays can be derived.,4
Learning to Remove Refractive Distortions from Underwater Images,"Simron Thapa, Nianyi Li, Jinwei Ye",2021,Conference Proceedings of the IEEE/CVF International Conference on Computer Vision,"The fluctuation of the water surface causes refractive distortions that severely downgrade the image of an underwater scene. Here, we present the distortion-guided network (DG-Net) for restoring distortion-free underwater images. The key idea is to use a distortion map to guide network training. The distortion map models the pixel displacement caused by water refraction. We first use a physically constrained convolutional network to estimate the distortion map from the refracted image. We then use a generative adversarial network guided by the distortion map to restore the sharp distortion-free image. Since the distortion map indicates correspondences between the distorted image and the distortion-free one, it guides the network to make better predictions. We evaluate our network on several real and synthetic underwater image datasets and show that it out-performs the state-of-the-art algorithms, especially in presence of large distortions. We also show results of complex scenarios, including outdoor swimming pool images captured by the drone and indoor aquarium images taken by cellphone camera.",3
Mirror surface reconstruction using polarization field,"Jie Lu, Yu Ji, Jingyi Yu, Jinwei Ye",2019/5/15,Conference 2019 IEEE International Conference on Computational Photography (ICCP),"Mirror surfaces are notoriously difficult to reconstruct. In this paper, we present a novel computational imaging approach for reconstructing complex mirror surfaces using a dense illumination field with angularly varying polarization states, which we call the polarization field. Specifically, we generate the polarization field using a commercial LCD with the top polarizer removed. We mathematically model the liquid crystals as polarization rotators using Jones calculus and show that the rotated polarization states of outgoing rays encode angular information (e.g., ray directions). To model reflection under the polarization field, we derive a reflection image formation model based on the Fresnel's equations and estimate incident ray positions and directions by coding the polarization field. Finally, we triangulate the incident rays with the camera rays to recover normals/depths of the mirror surface. Comprehensive simulations …",3
"Devices, systems, and methods for single-shot high-resolution multispectral image acquisition",Jinwei Ye,2016/8/18,Patent office US,"Systems, methods, and devices for generating high-resolution multispectral light-field images are described. The systems and devices a main lens include a microlens array, a multispectral-filter array that comprises spectral filters that filter light in different wavelengths, and a sensor that is configured to detect incident light. Also, the main lens, the microlens array, the multispectral-filter array, and the light sensor are disposed such that light from a scene passes through the main lens, the microlens array, and the multispectral-filter array and strikes a sensing surface of the sensor. Additionally, the multispectral-filter array is disposed so as to encode, in the light that strikes the sensing surface, a plane of the microlens array on the sensing surface of the sensor. Furthermore, the systems, methods, and devices generate high-resolution multispectral light field-images from low-resolution sub-aperture images using an …",3
Novel Optimal Multisensor Placement for Indoor Rectilinear Line-of-Sight Coverage,"Prasanga Neupane, Guannan Liu, Hsiao-Chun Wu, Shih Yu Chang, Jinwei Ye",2021/8/24,Journal IEEE Sensors Journal,"Optimal multisensor placement/deployment for an arbitrary indoor geometry still remains very challenging nowadays. It is preferable to minimize the number of sensors which can still cover the entire indoor space with a balanced load. However, this multi-objective problem is quite complex in practice. In this work, we will study this critical multisensor placement problem mathematically and algorithmically and propose a systematic approach to tackle this problem. In this paper, we focus on the line-of-sight (LoS) coverage within a rectilinear indoor environment and the sensors’ locations are restricted on the perimeter. Our proposed new approach consists of two stages. First, a partitioning algorithm is designed to partition an arbitrary rectilinear geometry into a number (as few as possible) of feasible rectilinear subareas, each of which can be fully covered by a sensor located somewhere on its external perimeter …",2
Indoor object localization and tracking using deep learning over received signal strength,"Guannan Liu, Hsiao-Chun Wu, Weidong Xiang, Jinwei Ye, Yiyan Wu, Limeng Pu",2020/10/27,Conference 2020 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),"This paper introduces a new indoor-localization approach using a deep-learning network for which the received signal-strength indicator (RSSI) is adopted as the radiofrequency fingerprint. In our proposed scheme, the RSSIs which are estimated by a channel-propagation emulator software, are adopted as the input features for deep learning. This approach is measurement-free and thus it is very cost-effective and convenient to users. A multilayer perceptron (MLP) is constructed to predict the location(s) of the mobile object(s). The time evolution of the predicted locations of an object will form the predicted trajectory thereby. Because deep-learning networks require tremendous training data to achieve good prediction accuracy, we propose to partition the indoor geometry of interest (ex., a room) into several zones. Preliminary simulation results demonstrate that the AUC (area under the receiver-operating …",2
3D LiDAR and Color Camera Data Fusion,"Yuqi Ding, Jiaming Liu, Jinwei Ye, Weidong Xiang, Hsiao-Chun Wu, Costas Busch",2020/10/27,Conference 2020 IEEE International symposium on broadband multimedia systems and broadcasting (BMSB),"3D LiDAR sensor and 2D color camera provide complementary information of a scene. It is essential to fuse both types of data to obtain a holistic representation of the real-world scene. In this paper, we present a method to fuse the data captured by rigidly mounted LiDAR and camera. We first cocalibrate the LiDAR and camera with a calibration target. We then establish 3D-2D correspondences and fuse the LiDAR and camera data through geometric transformations. Experimental results demonstrate that our method is able to fuse the 3D LiDAR point cloud with the 2D color image efficiently and accurately. Our method is also fast and can be used in autonomous driving applications.",2
3d fluid flow reconstruction using compact light field piv,"Zhong Li, Yu Ji, Jingyi Yu, Jinwei Ye",2020,"Conference Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XVI 16","Particle Imaging Velocimetry (PIV) estimates the fluid flow by analyzing the motion of injected particles. The problem is challenging as the particles lie at different depths but have similar appearances. Tracking a large number of moving particles is particularly difficult due to the heavy occlusion. In this paper, we present a PIV solution that uses a compact lenslet-based light field camera to track dense particles floating in the fluid and reconstruct the 3D fluid flow. We exploit the focal symmetry property in the light field focal stacks for recovering the depths of similar-looking particles. We further develop a motion-constrained optical flow estimation algorithm by enforcing the local motion rigidity and the Navier-Stoke fluid constraint. Finally, the estimated particle motion trajectory is used to visualize the 3D fluid flow. Comprehensive experiments on both synthetic and real data show that using a compact light field …",2
Non-Lambertian Surface Shape and Reflectance Reconstruction Using Concentric Multi-Spectral Light Field,"Mingyuan Zhou, Yu Ji, Yuqi Ding, Jinwei Ye, S Susan Young, Jingyi Yu",2019/4/9,Journal arXiv preprint arXiv:1904.04875,"Recovering the shape and reflectance of non-Lambertian surfaces remains a challenging problem in computer vision since the view-dependent appearance invalidates traditional photo-consistency constraint. In this paper, we introduce a novel concentric multi-spectral light field (CMSLF) design that is able to recover the shape and reflectance of surfaces with arbitrary material in one shot. Our CMSLF system consists of an array of cameras arranged on concentric circles where each ring captures a specific spectrum. Coupled with a multi-spectral ring light, we are able to sample viewpoint and lighting variations in a single shot via spectral multiplexing. We further show that such concentric camera/light setting results in a unique pattern of specular changes across views that enables robust depth estimation. We formulate a physical-based reflectance model on CMSLF to estimate depth and multi-spectral reflectance map without imposing any surface prior. Extensive synthetic and real experiments show that our method outperforms state-of-the-art light field-based techniques, especially in non-Lambertian scenes.",2
"Devices, systems, and methods for measuring and reconstructing the shapes of specular objects by multiview capture","Jinwei Ye, Siu-Kei Tin, Can Chen, Mahdi Nezamabadi",2017/6/22,Patent office US,"Devices, systems, and methods obtain two sets of images of an object, each of which was captured from a respective viewpoint; identify pixel regions in the two sets of images that show reflections from a light-modulating device that were reflected by a surface of the object; calculate respective surface normals for points on the surface in the pixel regions; calculate, for each viewpoint, respective unscaled surface coordinates of the points based on the respective surface normals; calculate, for each viewpoint, a respective initial scale factor based on the respective surface normals and on decoded light-modulating-device-pixel indices; calculate, for each viewpoint, scaled surface coordinates of the points based on the respective initial scale factor and the respective unscaled surface coordinates of the viewpoint; and calculate, for each viewpoint, a respective refined scale factor by minimizing discrepancies among the …",2
Image pre-compensation: Balancing contrast and ringing,"Yu Ji, Jinwei Ye, Sing Bing Kang, Jingyi Yu",2014,Conference Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,"The goal of image pre-compensation is to process an image such that after being convolved with a known kernel, will appear close to the sharp reference image. In a practical setting, the pre-compensated image has significantly higher dynamic range than the latent image. As a result, some form of tone mapping is needed. In this paper, we show how global tone mapping functions affect contrast and ringing in image pre-compensation. In particular, we show that linear tone mapping eliminates ringing but incurs severe contrast loss, while non-linear tone mapping functions such as Gamma curves slightly enhances contrast but introduces ringing. To enable quantitative analysis, we design new metrics to measure the contrast of an image with ringing. Specifically, we set out to find its"" equivalent ringing-free"" image that matches its intensity histogram and uses its contrast as the measure. We illustrate our approach on projector defocus compensation and visual acuity enhancement. Compared with the state-of-the-art, our approach significantly improves the contrast. We believe our technique is the first to analytically trade-off between contrast and ringing.",2
Novel Cascade Classifier Using Multiresolution Progressive Learning for Device-Free Indoor Localization,"Prasanga Neupane, Hsiao-Chun Wu, Guannan Liu, Weidong Xiang, Jinwei Ye, Shih Yu Chang",2021/10/14,Journal IEEE Sensors Letters,"Indoor localization of human objects has many important applications nowadays. Here, we propose a new multiresolution device-free indoor localization scheme using wireless radio frequency (RF) fingerprints. In our proposed new device-free approach, all transceiver devices are fixed in an indoor environment, so human targets do not need to carry any transceiver device with them. First, the indoor geometry is divided into several zones, and then, the received signal strength indicators measured by the receiving antennas are input features to our designed innovative machine learning model to identify within which zone the target is. Our proposed machine learning model, i.e., a multiresolution random forest classifier, is composed of a cascade architecture, which integrates and distills learned results over various zoning resolutions. The proposed new multiresolution approach greatly outperforms the existing …",1
Novel moving-target detection using a hybrid of RGB images and LiDAR point-clouds,"Rui Tian, Hsiao-Chun Wu, Jinwei Ye, Yiyan Wu",2020/10/27,Conference 2020 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),"Moving objects, especially humans and vehicles, impose very high risks on driverless cars or autonomous driving. Real-time moving-target detection plays a critical role in the future intelligent transportation systems due to public-safety concerns. Recently object tracking and scene analysis using a hybrid of high-resolution RGB images and low-resolution LiDAR point-clouds have been drawing a lot of research interest because fast but precise information rendering can be facilitated thereby. Compared to the conventional object detectors using RGB images, the new target detectors using LiDAR point-clouds can lead to three-dimensional object localization with the crucial depth information RGB images cannot provide, which is very critical to navigation of autonomous vehicles and robots. In this paper, we explore a novel moving-target detection approach which is built upon motion-estimation using RGB images …",1
Structure From Motion on XSlit Cameras,"Wei Yang, Yingliang Zhang, Jinwei Ye, Yu Ji, Zhong Li, Mingyuan Zhou, Jingyi Yu",2019/12/2,Journal IEEE Transactions on Pattern Analysis and Machine Intelligence,"We present a structure-from-motion (SfM) framework based on a special type of multi-perspective camera called the cross-slit or XSlit camera. Traditional perspective camera based SfM suffers from the scale ambiguity which is inherent to the pinhole camera geometry. In contrast, an XSlit camera captures rays passing through two oblique lines in 3D space and we show such ray geometry directly resolves the scale ambiguity when employed for SfM. To accommodate the XSlit cameras, we develop tailored feature matching, camera pose estimation, triangulation, and bundle adjustment techniques. Specifically, we devise a SIFT feature variant using non-uniform Gaussian kernels to handle the distortions in XSlit images for reliable feature matching. Moreover, we demonstrate that the XSlit camera exhibits ambiguities in pose estimation process which can not be handled by existing work. Consequently, we propose a …",1
Piv-based 3d fluid flow reconstruction using light field camera,"Zhong Li, Jinwei Ye, Yu Ji, Hao Sheng, Jingyi Yu",2019/4/15,Journal arXiv preprint arXiv:1904.06841,"Particle Imaging Velocimetry (PIV) estimates the flow of fluid by analyzing the motion of injected particles. The problem is challenging as the particles lie at different depths but have similar appearance and tracking a large number of particles is particularly difficult. In this paper, we present a PIV solution that uses densely sampled light field to reconstruct and track 3D particles. We exploit the refocusing capability and focal symmetry constraint of the light field for reliable particle depth estimation. We further propose a new motion-constrained optical flow estimation scheme by enforcing local motion rigidity and the Navier-Stoke constraint. Comprehensive experiments on synthetic and real experiments show that using a single light field camera, our technique can recover dense and accurate 3D fluid flows in small to medium volumes.",1
Efficient 3D face recognition in uncontrolled environment,"Yuqi Ding, Nianyi Li, S Susan Young, Jinwei Ye",2019,"Conference Advances in Visual Computing: 14th International Symposium on Visual Computing, ISVC 2019, Lake Tahoe, NV, USA, October 7–9, 2019, Proceedings, Part I 14"," Face recognition in an uncontrolled environment is challenging as body movement and pose variation can result in missing facial features. In this paper, we tackle this problem by fusing multiple RGB-D images with varying poses. In particular, we develop an efficient pose fusion algorithm that frontalizes the faces and combines the multiple inputs. We then introduce a new 3D registration method based on the unified coordinate system (UCS) to compensate for pose and scale variations and normalize the probe and gallery face. To perform 3D face recognition, we train a Support Vector Machine (SVM) with both 2D color and 3D geometric features. Experimental results on a RGB-D dataset show that our method can achieve a high recognition rate and is robust in the presence of pose and expression variations.",1
Content aware image pre-compensation,"Jinwei Ye, Yu Ji, Mingyuan Zhou, Sing Bing Kang, Jingyi Yu",2018/5/23,Journal IEEE Transactions on Pattern Analysis and Machine Intelligence,"The goal of image pre-compensation is to process an image such that after being convolved with a known kernel, will appear close to the sharp reference image. In a practical setting, the pre-compensated image has significantly higher dynamic range than the latent image. As a result, some form of tone mapping is needed. In this paper, we show how global tone mapping functions affect contrast and ringing in image pre-compensation. We further enhance contrast and reduce ringing by considering the visual saliency. Specifically, we prioritize contrast preservation in salient regions while tolerating more blurriness elsewhere. For quantitative analysis, we design new metrics to measure the contrast of an image with ringing. Specifically, we set out to find its “equivalent ringing-free” image that matches its intensity histogram and uses its contrast as the measure. We illustrate our approach on projector defocus …",1
Depth value measurement,"Siu-Kei Tin, Jinwei Ye",2018/5/1,Patent office US,"A depth value of an object is measured. The object is illuminated with a luminaire comprising at least three or more pixel-layers including a first pixel-layer, a second pixel-layer and a third pixel-layer, each pixel-layer including a rectangular array of pixels. One or more images are captured of the object illuminated by the pixel-layers of the luminaire. The depth value of a point on the surface of the object is determined based on the one or more captured images. The spaced-apart pixel-layers of the luminaire are grouped into at least a front group and a back group, and the front group is separated from the back group by a distance that is relatively large as compared to a distance by which the spaced-apart pixel-layers within any one group are separated.",1
Depth reconstruction from the defocus effect of an XSlit camera,"Yu Ji, Jinwei Ye, Jingyi Yu",2015/6/7,Conference Computational Optical Sensing and Imaging,"We introduce an physical implementation of the XSlit camera by using a pair of cylindrical lenses. By analyzing the XSlit defocus effect and strategically endcoding each slit aperture, a depth reconstruction algorithm is developed.",1
Full-Volume 3D Fluid Flow Reconstruction With Light Field PIV,"Yuqi Ding, Zhong Li, Zhang Chen, Yu Ji, Jingyi Yu, Jinwei Ye",2023/1/11,Journal IEEE Transactions on Pattern Analysis and Machine Intelligence,"Particle Imaging Velocimetry (PIV) is a classical method that estimates fluid flow by analyzing the motion of injected particles. To reconstruct and track the swirling particles is a difficult computer vision problem, as the particles are dense in the fluid volume and have similar appearances. Further, tracking a large number of particles is particularly challenging due to heavy occlusion. Here we present a low-cost PIV solution that uses compact lenslet-based light field cameras as imaging device. We develop novel optimization algorithms for dense particle 3D reconstruction and tracking. As a single light field camera has limited capacity in resolving depth (z-dimension measurement), the resolution of 3D reconstruction on the x-y plane is much higher than along the z-axis. To compensate for the imbalanced resolution in 3D, we use two light field cameras positioned at an orthogonal angle to capture particle images. In this …",
Polar-Photometric Stereo Under Natural Illumination,"Yuqi Ding, Yu Ji, Jinwei Ye",2022/9/12,Conference 2022 International Conference on 3D Vision (3DV),"We present a 3D shape reconstruction method that leverages both photometric and polarimetric cues. Unlike many active methods that require controlled lighting condition, our method can be used under unknown and uncontrolled natural illumination (both indoor and outdoor). We use two circularly polarized spotlights to boost the polarization cues corrupted by the environment lighting, as well as to provide photometric cues. We solve surface normals with two polarization images by combining the polarimetric and photometric constraints. To mitigate the effect of uncontrolled environment light in photometric constraints, we es-timate a lighting proxy map and iteratively refine the normal and lighting estimation. We perform experiments under various natural illumination conditions and compare our results with state-of-the-arts photometric stereo and shape from polarization methods. Our method achieves good …",
Light Field-Based Underwater 3D Reconstruction Via Angular Resampling,"Yuqi Ding, Zhang Chen, Yu Ji, Jingyi Yu, Jinwei Ye",2021/9/5,Journal arXiv preprint arXiv:2109.02116,"Recovering 3D geometry of underwater scenes is challenging because of non-linear refraction of light at the water-air interface caused by the camera housing. We present a light field-based approach that leverages properties of angular samples for high-quality underwater 3D reconstruction from a single viewpoint. Specifically, we resample the light field image to angular patches. As underwater scenes exhibit weak view-dependent specularity, an angular patch tends to have uniform intensity when sampled at the correct depth. We thus impose this angular uniformity as a constraint for depth estimation. For efficient angular resampling, we design a fast approximation algorithm based on multivariate polynomial regression to approximate nonlinear refraction paths. We further develop a light field calibration algorithm that estimates the water-air interface geometry along with the camera parameters. Comprehensive experiments on synthetic and real data show our method produces state-of-the-art reconstruction on static and dynamic underwater scenes.",
Next-generation perception system for automated defects detection in composite laminates via polarized computational imaging,"Yuqi Ding, Jinwei Ye, Corina Barbalata, James Oubre, Chandler Lemoine, Jacob Agostinho, Genevieve Palardy",2021/8/24,Journal arXiv preprint arXiv:2108.10819,"Finishing operations on large-scale composite components like wind turbine blades, including trimming and sanding, often require multiple workers and part repositioning. In the composites manufacturing industry, automation of such processes is challenging, as manufactured part geometry may be inconsistent and task completion is based on human judgment and experience. Implementing a mobile, collaborative robotic system capable of performing finishing tasks in dynamic and uncertain environments would improve quality and lower manufacturing costs. To complete the given tasks, the collaborative robotic team must properly understand the environment and detect irregularities in the manufactured parts. In this paper, we describe the initial implementation and demonstration of a polarized computational imaging system to identify defects in composite laminates. As the polarimetric images are highly relevant to the surface micro-geometry, they can be used to detect surface defects that are not visible in conventional color images. The proposed vision system successfully identifies defect types and surface characteristics (e.g., pinholes, voids, scratches, resin flash) for different glass fiber and carbon fiber laminates.",
Novel Indoor Device-Free Human Tracking Using Learning Systems with Hidden Markov Models,"Guannan Liu, Prasanga Neupane, Hsiao-Chun Wu, Weidong Xiang, Jinwei Ye, Limeng Pu, Shih Yu Chang, Yiyan Wu, Kun Yan",2021/8/4,Conference 2021 IEEE International Symposium on Broadband Multimedia Systems and Broadcasting (BMSB),"This paper proposes a novel indoor device-free localization and tracking approach using the received signal-strength indicators (RSSIs) of WiFi signals. The RSSI feature-vectors simulated by a channel-propagation emulator software are adopted as the training data for our proposed scheme. Prevalent discriminative machine-learning methods are used to predict the locations of a moving human-object. Hidden Markov models (HMMs) are also incorporated with such machine-learning techniques for robust and reliable indoor tracking. In this work, we partition the given indoor geometry into several equi-sized zones and then convert the underlying localization/tracking problem to the classical multi-classification problem. Simulation results demonstrate that the gradient boosting decision-tree (GBDT) classifier in conjunction with the Viterbi algorithm over hidden Markov models leads to the highest localization …",
3D Fluid Flow Reconstruction Using A Compact Light Field Camera,"Yuqi Ding, Zhong Li, Yu Ji, Jingyi Yu, Jinwei Ye",2021/7/19,Conference Computational Optical Sensing and Imaging,We present a flexible and low-cost 3D PIV solution that uses a compact light field camera as acquisition device. Real experiments demonstrate that our method can volumetric 3D fluid flows of various types.,
Multi-Spectral Reflectance And Shape Reconstruction Using A Concentric Light Field,"Yuqi Ding, Mingyuan Zhou, Yu Ji, Jingyi Yu, S Susan Young, Jinwei Ye",2021/7/19,Conference Imaging Systems and Applications,We introduce a concentric light field system that can recover the shape and multi-spectral reflectance of surfaces with diverse materials. Experimental results show that our method has robust performance in recovering non-Lambertian surfaces.,
Blur-free low-light imaging with color and event cameras,"Nianyi Li, Jinwei Ye, Qifan Zhang, S Susan Young",2021/4/12,Conference Computational Imaging VI,"Imaging under low-light conditions is a challenging but important problem due to low dynamic range, image noise, and blurriness. In this work, we propose blur-free low-light imaging techniques by combining a conventional color camera with an event camera. The event camera complements the color camera by measuring brightness changes asynchronously at high speed with high dynamic range. We synchronize the two sensors with external trigger cable. We align the viewpoints of the event and color using a beamsplitter. We co-calibrate the two cameras geometrically. We derive an image formation model and use the inverted model to reduce the blurriness in color images. Experimental results demonstrate the effectiveness of our method.",
Underwater 3D Reconstruction Using Light Fields.,"Yuqi Ding, Yu Ji, Jingyi Yu, Jinwei Ye",2021,Journal CoRR,,
Co-Calibration and Registration of Color and Event Cameras,"Qifan Zhang, Jinwei Ye, Philip Osteen, S Susan Young",2020/11/1,Publisher CCDC Army Research Laboratory Adelphi United States,"The event camera uses a novel bio-inspired sensor that works radically different from the traditional optical sensor. Registration between color and event images is challenging due to the data heterogeneity. This technical report presents a method for event and color image registration through camera calibration. Specifically, an event and a color camera are co-calibrated with a common calibration target. As the event camera cannot capture static scenes, we change the intensity of the light source around the calibration board and generate the accumulated frame. At the same time, we press the color camera shutter to obtain a color image. According to this method, multiple pairs of calibration board images are obtained. Next, we combine the event camera with the color camera to perform stereo calibration. The intrinsic and extrinsic parameters of each camera can be obtained respectively, and the relative position between the two cameras determined. Based on the two cameras homography matrix, we can deduce the corresponding position of the pixel in the color image, which is the registration of color and event cameras. The results of experiments performed with a color camera and an event camera pair show the method is effective.",
Challenges and solutions in 3D object capture: High-precision multi-view camera calibration using a rotating state; and 3D reconstruction of mirror-like objects using efficient …,"Evan Levine, Can Chen, Manuel Martinello, Mahdi Nezamabadi, Siu-Kei Tin, Jinwei Ye, Francisco Imai",2017/6/26,"Conference 3D Image Acquisition and Display: Technology, Perception and Applications",Two challenges in 3D capture are illustrated: highly precise multi-view camera calibration for precise pose estimation of cameras using a rotating stage and capture of mirror-type specular objects that lack own appearance and whose reflections from environment are view-dependent.,
Measuring surface geometry using illumination direction coding,"Siu-Kei Tin, Jinwei Ye",2017/5/11,Patent office US,Measuring a surface geometry of an object involves capturing one or more images of the object illuminated by a light field produced by one or more luminaires having multiple pixel-layers with overlapping fields of illumination. Each pixel-layer simultaneously and in synchronization with each other displays multiple coded patterns such that combinations of the multiple coded patterns uniquely identify directions of light rays originating from the multiple pixel-layers. A unique incident light ray direction for each pixel of the captured one or more images is determined by decoding the combinations of the multiple coded patterns. The surface geometry of the object is recovered using the determined unique incident light ray direction for each pixel of the one or more captured images.,
Ray geometry in multi-perspective cameras: A case study of XSlit imaging and its applications,Jinwei Ye,2013,Institution University of Delaware,"A pinhole camera collects rays passing through a common 3D point and its image resembles what would be seen by human eyes. In contrast, a multi-perspective camera combines rays collected by different viewpoints. Such capabilities can potentially benefit a broad class of imaging applications, ranging from scene understanding, to high quality imaging, and to 3D reconstructions. In this dissertation, I thoroughly discuss designing, modeling, and constructing general multi-perspective cameras. The unique approach I adopt is a ray geometry analysis that uniformly models arbitrary multi-perspective cameras as manifolds of rays and ray constraints.",
Polarimetric Helmholtz Stereopsis Supplementary Material,"Yuqi Ding, Yu Ji, Mingyuan Zhou, Sing Bing Kang, Jinwei Ye","Table 1 compares our method with classical 3D reconstruction methods. Readers can find more about these methods in the following references: multiview stereopsis (MVS)[5], photometric stereopsis (PS)[8], structured light (SL)[3], shape-frompolarization (SfP)[6], and Helmholtz stereopsis (HS)[10]. In this table, we assume all classical methods use a traditional camera, whereas our method (polar-HS) uses a polarization camera. It is worth noting that the minimum number of input for SfP is 1 when a polarization camera is used. However, the reconstruction accuracy is low. In our method, the minimum number of input is 4 (one reciprocal pair) because we need to capture two images under 0◦ and 90◦ linearly polarized lighting at each camera position in the pair. In addition, note that the accuracy of MVS is only moderate when the number of input images is small. The method is capable of high accuracy when given a large number of images (more than 10) captured from different viewpoints.We are aware that in recent years, many learning-based methods are proposed for 3D reconstruction from a single and/or multiple images [7, 9]. These methods are usually augmented with some form of data prior by training on a labeled dataset. In contrast, our method relies exclusively on the captured data and uses physical constraints for reconstruction. Therefore, we only compare with the physics-based methods that are more relevant to our approach in Table 1. In Equation 9, Sa, Sb, and S g","Scholar articles Polarimetric Helmholtz Stereopsis Supplementary MaterialY Ding, Y Ji, M Zhou, SB Kang, J YeRelated articles ","Table 1 compares our method with classical 3D reconstruction methods. Readers can find more about these methods in the following references: multiview stereopsis (MVS)[5], photometric stereopsis (PS)[8], structured light (SL)[3], shape-frompolarization (SfP)[6], and Helmholtz stereopsis (HS)[10]. In this table, we assume all classical methods use a traditional camera, whereas our method (polar-HS) uses a polarization camera. It is worth noting that the minimum number of input for SfP is 1 when a polarization camera is used. However, the reconstruction accuracy is low. In our method, the minimum number of input is 4 (one reciprocal pair) because we need to capture two images under 0◦ and 90◦ linearly polarized lighting at each camera position in the pair. In addition, note that the accuracy of MVS is only moderate when the number of input images is small. The method is capable of high accuracy when given a large number of images (more than 10) captured from different viewpoints.",
Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation Supplementary Material,"Nianyi Li, Simron Thapa, Cameron Whyte, Albert Reed, Suren Jayasuriya, Jinwei Ye","In this supplementary material, we provide details on our network architecture (Section 1), and synthetic air turbulence simulation (Section 2). We show more ablation study results on different numbers of inputs and the Fourier feature mapping parameters (Section 3). Lastly, we include more visual comparison results with the state-of-the-arts on the synthetic air and water turbulence (Section 4).","Scholar articles Unsupervised Non-Rigid Image Distortion Removal via Grid Deformation Supplementary MaterialN Li, S Thapa, C Whyte, A Reed, S Jayasuriya, J YeRelated articles All 2 versions ","In this supplementary material, we provide details on our network architecture (Section 1), and synthetic air turbulence simulation (Section 2). We show more ablation study results on different numbers of inputs and the Fourier feature mapping parameters (Section 3). Lastly, we include more visual comparison results with the state-of-the-arts on the synthetic air and water turbulence (Section 4).",
CVPR 2020,"Shangzhe Wu, Christian Rupprecht, Andrea Vedaldi, Jamie Watson, Michael Firman, Aron Monszpart, Simron Thapa, Nianyi Li, Jinwei Ye, Boyang Deng, Kyle Genova, Soroosh Yazdani, Sofien Bouaziz, Geoffrey Hinton, Andrea Tagliasacchi","Table of Contents Page 1 2020 IEEE/CVF Conference on Computer Vision and Pattern 
Recognition (CVPR) CVPR 2020 Table of Contents Organizers ccix Area Chairs ccxi 
Reviewers ccxvi Oral 1-1A: 3D From a Single Image and Shape-From-X (1) Unsupervised 
Learning of Probably Symmetric Deformable 3D Objects From Images in the Wild 1 
Shangzhe Wu (Visual Geometry Group, University of Oxford), Christian Rupprecht (Visual 
Geometry Group, University of Oxford), and Andrea Vedaldi (Visual Geometry Group, 
University of Oxford) Footprints and Free Space From a Single Color Image 11 Jamie 
Watson (Niantic), Michael Firman (Niantic), Aron Monszpart (Niantic), and Gabriel J. 
Brostow (Niantic) Dynamic Fluid Surface Reconstruction Using Deep Neural Network 21 
Simron Thapa (Louisiana State University, Baton Rouge, LA, USA), Nianyi Li (Louisiana 
State University, Baton Rouge, LA, USA), and Jinwei Ye …","Scholar articles CVPR 2020S Wu, C Rupprecht, A Vedaldi, J Watson, M Firman…All 3 versions ",,
Dynamic Fluid Surface Reconstruction Using Deep Neural Network Supplementary Material,"Simron Thapa, Nianyi Li, Jinwei Ye",Our fluid surface reconstruction network (FSRN) consists of two sub-nets: 1) an encoder-decoder based convolutional neural network (FSRN-CNN) for per-frame depth and normal estimation and 2) a recurrent neural network for enforcing the temporal consistency across multiple frames (FSRN-RNN). Table 1 and Table 2 provide detailed network architecture of the two subnets.,"Scholar articles Dynamic Fluid Surface Reconstruction Using Deep Neural Network Supplementary MaterialS Thapa, N Li, J YeRelated articles All 2 versions ",Our fluid surface reconstruction network (FSRN) consists of two sub-nets: 1) an encoder-decoder based convolutional neural network (FSRN-CNN) for per-frame depth and normal estimation and 2) a recurrent neural network for enforcing the temporal consistency across multiple frames (FSRN-RNN). Table 1 and Table 2 provide detailed network architecture of the two subnets.,
