titles,authors,date,source,descriptions,citations
Introduction to software testing,"Paul Ammann, Jeff Offutt",2016/12/13,Publisher Cambridge University Press,"This extensively classroom-tested text takes an innovative approach to explaining software testing that defines it as the process of applying a few precise, general-purpose criteria to a structure or model of the software. The book incorporates cutting-edge developments, including techniques to test modern types of software such as OO, web applications, and embedded software. This revised second edition significantly expands coverage of the basics, thoroughly discussing test automaton frameworks, and it adds new, improved examples and numerous exercises. The theory of coverage criteria is carefully and cleanly explained to help students understand concepts before delving into practical applications, while extensive use of the JUnit test framework gives students practical experience in a test framework popular in the industry. Exercises, meanwhile, feature specifically tailored tools that allow students to check their own work. The book's website also offers an instructor's manual, PowerPoint slides, testing tools for students, and example software programs in Java.",2472
"Scalable, graph-based network vulnerability analysis","Paul Ammann, Duminda Wijesekera, Saket Kaushik",2002/11/18,Book Proceedings of the 9th ACM Conference on Computer and Communications Security,"Even well administered networks are vulnerable to attack. Recent work in network security has focused on the fact that combinations of exploits are the typical means by which an attacker breaks into a network. Researchers have proposed a variety of graph-based algorithms to generate attack trees (or graphs). Either structure represents all possible sequences of exploits, where any given exploit can take advantage of the penetration achieved by prior exploits in its chain, and the final exploit in the chain achieves the attacker's goal. The most recent approach in this line of work uses a modified version of the model checker NuSMV as a powerful inference engine for chaining together network exploits, compactly representing attack graphs, and identifying minimal sets of exploits. However, it is also well known that model checkers suffer from scalability problems, and there is good reason to doubt whether a model …",1138
Using model checking to analyze network vulnerabilities,"Ronald W Ritchey, Paul Ammann",2000/5/14,Conference Proceeding 2000 IEEE Symposium on Security and Privacy. S&P 2000,"Even well administered networks are vulnerable to attacks due to the security ramifications of offering a variety of combined services. That is, services that are secure when offered in isolation nonetheless provide an attacker with a vulnerability to exploit when offered simultaneously. Many current tools address vulnerabilities in the context of a single host. We address vulnerabilities due to the configuration of various hosts in a network. In a different line of research, formal methods are often useful for generating test cases, and model checkers are particularly adept at this task due to their ability to generate counterexamples. We address the network vulnerabilities problem with test cases, which amount to attack scenarios, generated by a model checker. We encode the vulnerabilities in a state machine description suitable for a model checker and then assert that an attacker cannot acquire a given privilege on a given …",673
Data diversity: An approach to software fault tolerance,"Paul Eric Ammann, John C Knight",1988/4,Journal Ieee transactions on computers,"Data diversity is described, and the results of a pilot study are presented. The regions of the input space that cause failure for certain experimental programs are discussed, and data reexpression, the way in which alternate input data sets can be obtained, is examined. A description is given of the retry block which is the data-diverse equivalent of the recovery block, and a model of the retry block, together with some empirical results is presented. N-copy programming which is the data-diverse equivalent of N-version programming is considered, and a simple model and some empirical results are also given.< >",527
Using model checking to generate tests from specifications,"Paul E Ammann, Paul E Black, William Majurski",1998/12/9,Conference Proceedings second international conference on formal engineering methods (Cat. No. 98EX241),"We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker generates countersamples which distinguish the variations from the original specification. The countersamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case generation is automatic; each countersample is a complete test case. Second, in sharp contrast to program-based mutation analysis …",504
Generating test data from state‐based specifications,"Jeff Offutt, Shaoying Liu, Aynur Abdurazik, Paul Ammann",2003/1,"Journal Software testing, verification and reliability","Although the majority of software testing in industry is conducted at the system level, most formal research has focused on the unit level. As a result, most system‐level testing techniques are only described informally. This paper presents formal testing criteria for system level testing that are based on formal specifications of the software. Software testing can only be formalized and quantified when a solid basis for test generation can be defined. Formal specifications represent a significant opportunity for testing because they precisely describe what functions the software is supposed to provide in a form that can be automatically manipulated.",490
Testing with model checkers: a survey,"Gordon Fraser, Franz Wotawa, Paul E Ammann",2009/9,"Source Software Testing, Verification and Reliability","About a decade after the initial proposal to use model checkers for the generation of test cases we take a look at the results in this field of research. Model checkers are formal verification tools, capable of providing counterexamples to violated properties. Normally, these counterexamples are meant to guide an analyst when searching for the root cause of a property violation. They are, however, also very useful as test cases. Many different approaches have been presented, many problems have been solved, yet many issues remain. This survey paper reviews the state of the art in testing with model checkers. Copyright © 2008 John Wiley & Sons, Ltd.",290
Recovery from malicious transactions,"Paul Ammann, Sushil Jajodia, Peng Liu",2002/11/7,Journal IEEE transactions on knowledge and data engineering,"Preventive measures sometimes fail to deflect malicious attacks. We adopt an information warfare perspective, which assumes success by the attacker in achieving partial, but not complete, damage. In particular, we work in the database context and consider recovery from malicious but committed transactions. Traditional recovery mechanisms do not address this problem, except for complete rollbacks, which undo the work of benign transactions as well as malicious ones, and compensating transactions, whose utility depends on application semantics. Recovery is complicated by the presence of benign transactions that depend, directly or indirectly, on the malicious transactions. We present algorithms to restore only the damaged part of the database. We identify the information that needs to be maintained for such algorithms. The initial algorithms repair damage to quiescent databases; subsequent algorithms …",229
A weakest-adversary security metric for network configuration security analysis,"Joseph Pamula, Sushil Jajodia, Paul Ammann, Vipin Swarup",2006/10/30,Book Proceedings of the 2nd ACM workshop on Quality of protection,"A security metric measures or assesses the extent to which a system meets its security objectives. Since meaningful quantitative security metrics are largely unavailable, the security community primarily uses qualitative metrics for security. In this paper, we present a novel quantitative metric for the security of computer networks that is based on an analysis of attack graphs. The metric measures the security strength of a network in terms of the strength of the weakest adversary who can successfully penetrate the network. We present an algorithm that computes the minimal sets of required initial attributes for the weakest adversary to possess in order to successfully compromise a network; given a specific network configuration, set of known exploits, a specific goal state, and an attacker class (represented by a set of all initial attacker attributes). We also demonstrate, by example, that diverse network configurations are …",226
Establishing theoretical minimal sets of mutants,"Paul Ammann, Marcio Eduardo Delamaro, Jeff Offutt",2014/3/31,"Conference 2014 IEEE seventh international conference on software testing, verification and validation","Mutation analysis generates tests that distinguish variations, or mutants, of an artifact from the original. Mutation analysis is widely considered to be a powerful approach to testing, and hence is often used to evaluate other test criteria in terms of mutation score, which is the fraction of mutants that are killed by a test set. But mutation analysis is also known to provide large numbers of redundant mutants, and these mutants can inflate the mutation score. While mutation approaches broadly characterized as reduced mutation try to eliminate redundant mutants, the literature lacks a theoretical result that articulates just how many mutants are needed in any given situation. Hence, there is, at present, no way to characterize the contribution of, for example, a particular approach to reduced mutation with respect to any theoretical minimal set of mutants. This paper's contribution is to provide such a theoretical foundation for …",197
Using formal methods to derive test frames in category-partition testing,"Paul Ammann, Jeff Offutt",1994/6/27,Conference Proceedings of COMPASS'94-1994 IEEE 9th Annual Conference on Computer Assurance,"Testing is a standard method of assuring that software performs as intended. We extend the category-partition method, which is a specification-based testing method. An important aspect of category-partition testing is the construction of test specifications as an intermediate between functional specifications and actual tests. We define a minimal coverage criterion for category-partition test specifications identify a mechanical process to produce a test specification that satisfies the criterion, and discuss the problem of resolving infeasible combinations of choices for categories. Our method uses formal schema-based functional specifications and is shown to be feasible with an example study of a simple file system.< >",192
A specification-based coverage metric to evaluate test sets,"Paul E Ammann, Paul E Black",2001/12,"Journal International Journal of Reliability, Quality and Safety Engineering","Software developers use a variety of formal and informal methods, including testing, to argue that their systems are suitable for building high assurance applications. In this paper, we develop another connection between formal methods and testing by defining a specification-based coverage metric to evaluate test sets. Formal methods in the form of a model checker supply the necessary automation to make the metric practical. The metric gives the software developer assurance that a given test set is sufficiently sensitive to the structure of an application's specification. We also develop the necessary foundation for the metric and then illustrate the metric on an example.",183
A host-based approach to network attack chaining analysis,"Paul Ammann, Joseph Pamula, Ronald Ritchey, Julie Street",2005/12/5,Conference 21st Annual Computer Security Applications Conference (ACSAC'05),"The typical means by which an attacker breaks into a network is through a chain of exploits, where each exploit in the chain lays the groundwork for subsequent exploits. Such a chain is called an attack path, and the set of all possible attack paths form an attack graph. Researchers have proposed a variety of methods to generate attack graphs. In this paper, we provide a novel alternative approach to network vulnerability analysis by utilizing a penetration tester's perspective of maximal level of penetration possible on a host. Our approach has the following benefits: it provides a more intuitive model in which an analyst can work, and its algorithmic complexity is polynomial in the size of the network, and so has the potential of scaling well to practical networks. The drawback is that we track only ""good"" attack paths, as opposed to all possible attack paths. Hence, an analyst may make suboptimal choices when repairing …",154
Surviving information warfare attacks on databases,"Paul Ammann, Sushil Jajodia, Catherine D McCollum, Barbara T Blaustein",1997/5/4,Conference Proceedings. 1997 IEEE Symposium on Security and Privacy (Cat. No. 97CB36097),"We consider the problem of surviving information warfare attacks on databases. We adopt a fault tolerance approach to the different phases of an attack. To maintain precise information about the attack, we mark data to reflect the severity of detected damage as well as the degree to which the damaged data has been repaired. In the case of partially repaired data, integrity constraints might be violated, but data is nonetheless available to support mission objectives. We define a notion of consistency suitable for databases in which some information is known to be damaged, and other information is known to be only partially repaired. We present a protocol for normal transactions with respect to the damage markings and show that consistency preserving normal transactions maintain database consistency in the presence of damage. We present an algorithm for taking consistent snapshots of databases under attack …",150
Coverage criteria for logical expressions,"Paul Ammann, Jeff Offutt, Hong Huang",2003/11/17,"Conference 14th International Symposium on Software Reliability Engineering, 2003. ISSRE 2003.","A large number of coverage criteria to generate tests from logical expressions have been proposed. Although there have been large variations in the terminology, the articulation of the criteria and the original source of the expressions, many of these criteria are fundamentally the same. The most commonly known and widely used criterion is that of modified condition decision coverage (MCDC), but some articulations of MCDC have had some ambiguities. This has led to confusion on the part of testers, students, and tool developers on how best to implement these test criteria. This paper presents a complete comprehensive set of criteria that incorporate all the existing criteria, and eliminates the ambiguities by introducing precise definitions of the various possibilities.",140
Rewriting histories: Recovering from malicious transactions,"Peng Liu, Paul Ammann, Sushil Jajodia",2000/1,Journal Distributed and Parallel databases,"We consider recovery from malicious but committed transactions. Traditional recovery mechanisms do not address this problem, except for complete rollbacks, which undo the work of good transactions as well as malicious ones, and compensating transactions, whose utility depends on application semantics. We develop an algorithm that rewrites execution histories for the purpose of backing out malicious transactions. Good transactions that are affected, directly or indirectly, by malicious transactions complicate the process of backing out undesirable transactions. We show that the prefix of a rewritten history produced by the algorithm serializes exactly the set of unaffected good transactions. The suffix of the rewritten history includes special state information to describe affected good transactions as well as malicious transactions. We describe techniques that can extract additional good transactions from this …",128
Mutation operators for testing Android apps,"Lin Deng, Jeff Offutt, Paul Ammann, Nariman Mirzaei",2017/1/1,Journal Information and Software Technology,"Context: Due to the widespread use of Android devices, Android applications (apps) have more releases, purchases, and downloads than apps for any other mobile devices. The sheer volume of code in these apps creates significant concerns about the quality of the software. However, testing Android apps is different from testing traditional Java programs due to the unique program structure and new features of apps. Simple testing coverage criteria such as statement coverage are insufficient to assure high quality of Android apps. While researchers show significant interest in finding better Android testing approaches, there is still a lack of effective and usable techniques to evaluate their proposed test selection strategies, and to ensure a reasonable number of effective tests.",97
Analyzing the validity of selective mutation with dominator mutants,"Bob Kurtz, Paul Ammann, Jeff Offutt, Márcio E Delamaro, Mariet Kurtz, Nida Gökçe",2016/11/1,Book Proceedings of the 2016 24th ACM SIGSOFT International Symposium on Foundations of Software Engineering,"Various forms of selective mutation testing have long been accepted as valid approximations to full mutation testing. This paper presents counterevidence to traditional selective mutation. The recent development of dominator mutants and minimal mutation analysis lets us analyze selective mutation without the noise introduced by the redundancy inherent in traditional mutation. We then exhaustively evaluate all small sets of mutation operators for the Proteum mutation system and determine dominator mutation scores and required work for each of these sets on an empirical test bed. The results show that all possible selective mutation approaches have poor dominator mutation scores on at least some of these programs. This suggests that to achieve high performance with respect to full mutation analysis, selective approaches will have to become more sophisticated, possibly by choosing mutants based on the …",92
Mutant subsumption graphs,"Bob Kurtz, Paul Ammann, Marcio E Delamaro, Jeff Offutt, Lin Deng",2014/3/31,"Conference 2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","Mutation testing researchers have long known that many generated mutants are not needed. This paper develops a graph model to describe redundancy among mutations. We define ""true"" subsumption, a relation that practicing test engineers would like to have, but cannot due to issues of computability. We also define dynamic subsumption and static subsumption as approximations of ""true"" subsumption. We explore the properties of the approximate subsumption relations in the context of a small example. We suggest possible uses for subsumption graphs.",92
Ecodroid: An approach for energy-based ranking of android apps,"Reyhaneh Jabbarvand Behrouz, Alireza Sadeghi, Joshua Garcia, Sam Malek, Paul Ammann",2015/5/18,Conference 2015 IEEE/ACM 4th International Workshop on Green and Sustainable Software,"The ever increasing complexity of mobile apps comes with a higher energy cost, creating an inconvenience for users on batter-constrained mobile devices. At the same time, due to the meteoric rise of the numbers apps provisioned on app repositories, there are often multiple apps from the same category (e.g., Weather, dictionary) that provide similar features. In spite of similar functionality, the apps may present very different energy costs, due to the choices made in their design and construction. Given apps with similar features, users would prefer an app with the least energy cost. However, app repositories are currently lacking information about relative energy cost of apps in a given category, forcing the users to blindly choose an app for installation without a clear understanding of its energy implications. To address this issue, we have developed Eco Droid, an approach that ranks apps from the same category …",87
Improving logic-based testing,"Gary Kaminski, Paul Ammann, Jeff Offutt",2013/8/1,Journal Journal of Systems and Software,"Logic-based testers design tests from logical expressions that appear in software artifacts such as source code, design models, and requirements specifications. This paper presents three improvements to logic-based test design. First, in the context of mutation testing, we present fault hierarchies for the six relational operators. Applying the ROR mutation operator causes each relational operator to generate seven mutants per clause. The fault hierarchies show that only three of these seven mutants are needed. Second, we show how to bring the power of the ROR operator to logic-based test criteria such as the widely used Multiple Condition-Decision Coverage (MCDC) test criterion. Third, we present theoretical results supported by empirical data that show that the more recent coverage criterion of minimal-MUMCUT can find significantly more faults than MCDC. The paper has three specific recommendations: (1 …",85
Using a model checker to test safety properties,"Paul Ammann, Wei Ding, Daling Xu",2001/6/11,Conference Proceedings Seventh IEEE International Conference on Engineering of Complex Computer Systems,"In addition to providing a sound basis for analysis, formal methods can support other development activities; in our case the target is specification-based testing at the system level. We use the formal method of model checking to either generate new test sets or analyze existing test sets with respect to safety properties expressed in a temporal logic. We consider two types of tests: failing tests, in which a system must reject (fail) a specific dangerous action, and passing tests, in which a system must accept (pass) a safe action in a context that also includes a plausible dangerous action. We formalize our notion of dangerous actions with a mutation model for model checking specifications, and we develop coverage criteria to assess test sets. The coverage criteria are based on the logic operators from the Computation Tree Logic (CTL) and encompass the idea of scenarios where a dangerous action is either inevitable (A …",79
Using Z Specifications in Category Partitioning Testing,"N Amla, PE Ammann",1992/6,Journal National Aeronautics and Space Administration,"Measurement of software reliability by life testing involves executing the software on large numbers of test cases and recording the results. The number of failures observed is used to bound the failure probability even if the number of failures observed is zero. Most analyses assume that all failures will be observed but in practice this will rarely be the case. In this paper we examine the effect of imperfect error detection, ie, the situation in which a failure of the software may not be observed. If the conventional analysis associated with life testing is used, the confidence in the bound on the failure probability is optimistic. Our results show that imperfect error detection does not necessarily limit the ability of life testing to bound the probability of failure to the very low values required in critical systems. However, we show that the confidence level associated with a bound on failure probability cannot necessarily be made as high as desired unless very strong assumptions are made about the error detection mechanism. Such assumptions are unlikely to be met in practice, and so life testing is likely to be useful only for situations where very high confidence levels are not required.",70
Trusted recovery,"Sushil Jajodia, Catherine D McCollum, Paul Ammann",1999/7/1,Journal Communications of the ACM,"72 July 1999/Vol. 42, No. 7 COMMUNICATIONS OF THE ACM neglected recovery phase. The goal of defense is to keep available as many of the critical system elements as possible in the face of information warfare attacks. It is undesirable to use recovery techniques that require halting system operations for repair, for denial of service may be the attacker’s objective, especially if it occurs at a critical time. Once a bad system element has been detected, it is essential to proceed quickly with repairs while allowing applications to continue operating even if some of the elements have been damaged by an attack.",69
The effect of imperfect error detection on reliability assessment via life testing,"Paul E Ammann, Susan S.  Brilliant, John C.  Knight",1994/2,Journal IEEE Transactions on Software Engineering,"Measurement of software reliability by life testing involves executing the software on large numbers of test cases and recording the results. The number of failures observed is used to bound the failure probability even if the number of failures observed is zero. Typical analyses assume that all failures that occur are observed, but, in practice, failures occur without being observed. In this paper, we examine the effect of imperfect error detection, i.e. the situation in which a failure of the software may not be observed. If a conventional analysis associated with life testing is used, the confidence in the bound on the failure probability is optimistic. Our results show that imperfect error detection does not necessarily limit the ability of life testing to bound the probability of failure to the very low values required in critical systems. However, we show that the confidence level associated with a bound on failure probability cannot …",67
Evaluation of three specification-based testing criteria,"Aynur Abdurazik, Paul Ammann, Wei Ding, Jeff Offutt",2000/9/11,Conference Proceedings sixth IEEE international conference on engineering of complex computer systems. ICECCS 2000,"This paper compares three specification-based testing criteria using Mathur and Wong's PROBSUBSUMES measure. The three criteria are specification-mutation coverage, full predicate coverage, and transition-pair coverage. A novel aspect of the work is that each criterion is encoded in a model checker, and the model checker is used first to generate test sets for each criterion and then to evaluate test sets against alternate criteria. Significantly, the use of the model checker for generation of test sets eliminates human bias from this phase of the experiment. The strengths and weaknesses of the criteria are discussed.",66
Model checkers in software testing,"Paul E Black, P E Ammann, Wei Ding",2002/2/1,"Publisher Paul E. Black, P E. Ammann, W Ding","The primary focus of formal methods is static analysis of specifications and code, but there is also a long tradition of exploiting formal methods for testing. This paper continues this model by exploring the role of model checkers in software testing. Model checkers were originally developed to check that state machines conformed to specifications expressed in a temporal logic. We show how to apply these powerful computation engines to the problems of test generation and test evaluation for a variety of test coverage criteria defined on model-based specifications.",65
Abstracting formal specifications to generate software tests via model checking,"Paul Ammann, Paul E Black",1999/10/24,Conference Gateway to the New Millennium. 18th Digital Avionics Systems Conference. Proceedings (Cat. No. 99CH37033),"A recent method combines model checkers with specification-based mutation analysis to generate test cases from formal software specifications. However high-level software specifications usually must be reduced to make analysis with a model checker feasible. We propose a new reduction, parts of which can be applied mechanically, to soundly reduce some large, even infinite, state machines to manageable pieces. Our work differs from other work in that we use the reduction for generating test sets, as opposed to the typical goal of analyzing for properties. Consequently, we have different criteria, and we prove a different soundness rule. Informally the rule is that counterexamples from the model checker are test cases for the original specification. The reduction changes the state machine and temporal logic constraints the model checking specification to avoid generating unsound test cases. We use a Java virtual …",65
Designing deletion mutation operators,"Marcio Eduardo Delamaro, Jeff Offutt, Paul Ammann",2014/3/31,"Conference 2014 IEEE Seventh International Conference on Software Testing, Verification and Validation","As a test criterion, mutation analysis is known for yielding very effective tests. It is also known for creating many test requirements, each of which is represented by a ""mutant"" that must be ""killed."" In recent years, researchers have found that these test requirements have a lot of duplication, in that many test requirements yield the same tests. Put another way, hundreds of mutants can usually be killed by only a few dozen tests. If we could reduce this duplication without reducing mutation's effectiveness, mutation testing could become more cost-effective. One avenue of this research has been to use only one type of mutant, the statement deletion mutation operator. Researchers have found that statement deletion mutation has relatively few mutants, but yields tests that are almost as effective as using all mutants, with the significant benefit that fewer equivalent mutants are generated. This paper extends this idea by …",63
Towards mutation analysis of android apps,"Lin Deng, Nariman Mirzaei, Paul Ammann, Jeff Offutt",2015/4/13,"Conference 2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","Android applications (apps) have the highest number of releases, purchases, and downloads among mobile apps. However, quality is a known problem, and hence there is significant research interest in better methods for testing Android apps. We identify three reasons to extend mutation testing to Android apps. First, current testing approaches for Android apps use simple coverage criteria such as statement coverage; extending mutation coverage to Android apps promises more sophisticated testing. Second, testing researchers inventing other test methods for Android apps need to evaluate the quality of their test selection strategies, which mutation excels at. Finally, some approaches to test generation for Android apps, specifically combinatorial testing approaches, generate very large numbers of tests. This is particularly problematic because running Android tests is slow. For these reasons, this paper proposes …",62
Barriers to usable security? Three organizational case studies,"Deanna D Caputo, Shari Lawrence Pfleeger, M Angela Sasse, Paul Ammann, Jeff Offutt, Lin Deng",2016/10/25,Journal IEEE Security & Privacy,"Usable security assumes that when security functions are more usable, people are more likely to use them, leading to an improvement in overall security. Existing software design and engineering processes provide little guidance for leveraging this in the development of applications. Three case studies explore organizational attempts to provide usable security products.",61
Static analysis of mutant subsumption,"Bob Kurtz, Paul Ammann, Jeff Offutt",2015/4/13,"Conference 2015 IEEE Eighth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","Mutation analysis generates a large set of variants, or mutants, and then demands a test set that distinguishes each variant from the original artifact. It has long been apparent that many mutants contribute little, if anything, to the subsequent test set. Researchers have developed various approaches to separate valuable mutants from redundant mutants. The notion of subsumption underlies several such approaches. Informally, one mutant subsumes another if tests that kill the first also kill the second. Computing subsumption relations is, not surprisingly, undecidable. Recent work formalized the notion of a mutant subsumption graph (MSG) and showed that root nodes in the MSG precisely identify mutants that are not redundant. To address the decidability issue, we first defined the dynamic subsumption graph as an approximation to the MSG. This paper continues by showing how symbolic execution can be used to …",60
Safety Analysis for the Extended Schematic Protection Model.,"Paul Ammann, Ravi S Sandhu",1991/5/20,Conference IEEE Symposium on Security and Privacy,"Access control models provide a formalism and framework for specifying control over access to information and other resources in multi-user computer systems. Useful access control models must balance expressive power with the decidability and complexity of safety analysis, ie the determination of whether or not a given subject can ever acquire access to a given object. The access matrix model of Harrison, Ruzzo, and Ullman (HRU) has very broad expressive power. Unfortunately, HRU also has extremely weak safety properties; safety analysis is undecidable for most policies of practical interest. In this paper we show the remarkable result that an alternate formulation of HRU gives us strong safety properties. This alternate formulation is called the Extended Schematic Protection Model (ESPM). ESPM is derived from the Schematic Protection Model (SPM) by extending the creation operation to allow multiple …",60
Applying formal methods to semantic-based decomposition of transactions,"Paul Ammann, Sushil Jajodia, Indrakshi Ray",1997/6/1,Journal ACM Transactions on Database Systems (TODS),"In some database applications the traditional approach of seerializability, in which transactions appear to execute atomically and in isolation on a consistent database state, fails to satisfy performance requirements. Although many researchers have investigated the process of decomposing transactions into steps to increase concurrency, such research typically focuses on providing algorithms necessary to implement a decomposition supplied by the database application developer and pays relatively little attention to what constitutess a desirable decomposition or how the developer should obtain one. We focus onthe decomposition itself. A decomposition generates proof obligations whose descharge ensures desirable properties with respect to the original collection of transactions. We introduce the notion of semantic histories to formulate and prove the necessary properties, and the notion of successor sets to …",57
Surviving information warfare attacks,"Sushil Jajodia, Paul Ammann, Catherine D McCollum",1999/4,Journal Computer,"The past few years have seen governmental, military, and commercial organizations widely adopt Web-based commercial technologies because of their convenience, ease of use, and ability to take advantage of rapid advances in the commercial market. With this increasing reliance on internetworked computer resources comes an increasing vulnerability to information warfare. In today's heavily networked environment, safety demands protection from both obvious and subtle intrusions that can delete or corrupt vital data. Traditionally, information systems security focuses primarily on prevention: putting controls and mechanisms in place that protect confidentiality, integrity, and availability by stopping users from doing bad things. Moreover, most mechanisms are powerless against misbehavior by legitimate users who perform functions for which they are authorized. The paper discusses traditional approaches and …",56
Distributed timestamp generation in planar lattice networks,"Paul Ammann, Sushil Jajodia",1993/8/1,Journal ACM Transactions on Computer Systems (TOCS),"Timestamps are considered for distributed environments in which information flow is restricted to one direction through a planar lattice imposed on a network. For applications in such networks, existing timestamping algorithms require extension and modification. For example, in secure environments, typical timestamps provide a potential signaling channel between incomparable levels. In hierarchical databases, typical timestamps cause peripheral sites to unnecessarily affect the behavior at main sites.",56
Inferring mutant utility from program context,"René Just, Bob Kurtz, Paul Ammann",2017/7/10,Book Proceedings of the 26th ACM SIGSOFT International Symposium on Software Testing and Analysis," Existing mutation techniques produce vast numbers of equivalent, trivial, and redundant mutants. Selective mutation strategies aim to reduce the inherent redundancy of full mutation analysis to obtain most of its benefit for a fraction of the cost. Unfortunately, recent research has shown that there is no fixed selective mutation strategy that is effective across a broad range of programs; the utility (i.e., usefulness) of a mutant produced by a given mutation operator varies greatly across programs. ",55
Mutation testing implements grammar-based testing,"Jeff Offutt, Paul Ammann, Lisa Liu",2006/11/7,Conference Second Workshop on Mutation Analysis (Mutation 2006-ISSRE Workshops 2006),"This paper presents an abstract view of mutation analysis. Mutation was originally thought of as making changes to program source, but similar kinds of changes have been applied to other artifacts, including program specifications, XML, and input languages. This paper argues that mutation analysis is actually a way to modify any software artifact based on its syntactic description, and is in the same family of test generation methods that create inputs from syntactic descriptions. The essential characteristic of mutation is that a syntactic description such as a grammar is used to create tests. We call this abstract view grammar-based testing, and view it as an interface, which mutation analysis implements. This shift in view allows mutation to be defined in a general way, yielding three benefits. First, it provides a simpler way to understand mutation. Second, it makes it easier to develop future applications of mutation …",54
"A timestamp ordering algorithm for secure, single-version, multi-level databases",Paul Ammann,1991,Journal Proc. IFIP WG11. 3 Working Group on Database Security,,54
Policy-based dissemination of partial web-ontologies,"Saket Kaushik, Duminda Wijesekera, Paul Ammann",2005/11/11,Book Proceedings of the 2005 workshop on Secure web services,"Traditional discretionary access control, without data alteration operators, applied directly on ontologies can result in revealing unintended information because ontologies contain meta-information about objects. As an alternative we provide a constraint logic programming based policy language that can extract, rove or desensitize sensitive concepts in ontologies prior to requested disclosures. Our policies are stratified Horn clauses with constructive negation, and our constraint syst uses a finitary syst of ZF sets developed by Dovier et al. - and consequently, admits a three-valued Kripke-Kleene santics. Consequently, it is suitable for safeguarding meta-information stored on the Santic web using OWL. We show how our three-valued santics faithfully represents traditional OWL santics.",50
"An industrial application of mutation testing: Lessons, challenges, and research directions","Goran Petrovic, Marko Ivankovic, Bob Kurtz, Paul Ammann, René Just",2018/4/9,"Source 2018 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","Mutation analysis evaluates a testing or debugging technique by measuring how well it detects mutants, which are systematically seeded, artificial faults. Mutation analysis is inherently expensive due to the large number of mutants it generates and due to the fact that many of these generated mutants are not effective; they are redundant, equivalent, or simply uninteresting and waste computational resources. A large body of research has focused on improving the scalability of mutation analysis and proposed numerous optimizations to, e.g., select effective mutants or efficiently execute a large number of tests against a large number of mutants. However, comparatively little research has focused on the costs and benefits of mutation testing, in which mutants are presented as testing goals to a developer, in the context of an industrial-scale software development process. This paper draws on an industrial application of …",49
Issues in using model checkers for test case generation,"Gordon Fraser, Franz Wotawa, Paul Ammann",2009/9/1,Journal Journal of Systems and Software,"The use of model checkers for automated software testing has received some attention in the literature: It is convenient because it allows fully automated generation of test suites for many different test objectives. On the other hand, model checkers were not originally meant to be used this way but for formal verification, so using model checkers for testing is sometimes perceived as a “hack”. Indeed, several drawbacks result from the use of model checkers for test case generation. If model checkers were designed or adapted to take into account the needs that result from the application to software testing, this could lead to significant improvements with regard to test suite quality and performance. In this paper we identify the drawbacks of current model checkers when used for testing. We illustrate techniques to overcome these problems, and show how they could be integrated into the model checking process. In …",48
The extended schematic protection model,"Paul E Ammann, Ravi S Sandhu",1992/1/1,Journal Journal of Computer Security,"Access control models provide a formalism and framework for specifying control over access to information and other resources in multi-user computer systems. Useful access control models must balance expressive power with the decidability and complexity of safety analysis (ie the determination of whether or not a given subject can ever acquire access to a given object). The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has very broad expressive power. Unfortunately, HRU also has extremely weak safety properties. Safety is undecidable for most policies of practical interest, even in the monotonic version of HRU (which only allows revocation which is itself reversible). Remarkably, an alternate formulation of monotonic HRU yields strong safety properties. This alternate formulation is called the Extended Schematic Protection Model (ESPM). ESPM is derived from the Schematic …",46
Better predicate testing,"Gary Kaminski, Paul Ammann, Jeff Offutt",2011/5/23,Book Proceedings of the 6th International Workshop on Automation of Software Test,"Mutation testing is widely recognized as being extremely powerful, but is considered difficult to automate enough for practical use. This paper theoretically addresses two possible reasons for this: the generation of redundant mutants and the lack of integration of mutation analysis with other test criteria. By addressing these two issues, this paper brings an important mutation operator, relational-operator-replacement (ROR), closer to practical use. First, we develop fault hierarchies for the six relational operators, each of which generates seven mutants per clause. These hierarchies show that, for any given clause, only three mutants are necessary. This theoretical result can be integrated easily into mutation analysis tools, thereby eliminating generation of 57% of the ROR mutants. Second, we show how to bring the power of the ROR operator to the widely used Multiple Condition-Decision Coverage (MCDC) test …",43
An experimental evaluation of simple methods for seeding program errors,"John C Knight, Paul E Ammann",1985/8/1,Book Proceedings of the 8th international conference on Software engineering,"This paper describes an experiment in which simple syntactic alterations were introduced into program text in order to evaluate the testing strategy known as error seeding. The experiment’s goal was to determine if randomly placed syntactic manipulations can produce failure characteristics similar to those of indigenous errors found within unseeded programs. As a result of a separate experiment, several programs were available, all of which were written to the same specifications and thus were intended to be functionally equivalent. The use of functionally equivalent programs allowed the influence of individual programmer styles to be removed as a variable from the error seeding experiment. Each of six different syntactic manipulations were introduced into each program and the mean times to failure for the seeded errors were observed. The seeded errors were found to have a broad spectrum of mean times to failure independent of the syntactic alteration used. We conclude that it is possible to seed errors using only simple syntactic techniques that are arbitrarily di5culty to locate. In addition, several unexpected results indicate that some issues involved in error seeding have not been addressed previously.",39
Using a fault hierarchy to improve the efficiency of DNF logic mutation testing,"Garrett Kent Kaminski, Paul Ammann",2009/4/1,Conference 2009 International Conference on Software Testing Verification and Validation,"Mutation testing is a technique for generating high quality test data. However, logic mutation testing is currently inefficient for three reasons. One, the same mutant is generated more than once. Two, mutants are generated that are guaranteed to be killed by a test that kills some other generated mutant. Three, mutants that when killed are guaranteed to kill many other mutants are not generated as valuable mutation operators are missing. This paper improves logic mutation testing by 1) extending a logic fault hierarchy to include existing logic mutation operators, 2) introducing new logic mutation operators based on existing faults in the hierarchy, 3) introducing new logic mutation operators having no corresponding faults in the hierarchy and extending the hierarchy to include them, and 4) addressing the precise effects of equivalent mutants on the fault hierarchy. An empirical study using minimal DNF predicates in …",38
Using logic criterion feasibility to reduce test set size while guaranteeing fault detection,"Garrett Kent Kaminski, Paul Ammann",2009/4/1,Conference 2009 International Conference on Software Testing Verification and Validation,"Some software testing logic coverage criteria demand inputs that guarantee detection of a large set of fault types. One powerful such criterion, MUMCUT, is composed of three criteria, where each constituent criterion ensures the detection of specific fault types. In practice, the criteria may overlap in terms of fault types detected, thereby leading to numerous redundant tests, but due to the unfortunate fact that infeasible test requirements don't result in tests, all the constituent criteria are needed. The key insight of this paper is that analysis of the feasibility of the constituent criteria can be used to reduce test set size without sacrificing fault detection. In other words, expensive criteria can be reserved for use only when they are actually necessary. This paper introduces a new logic criterion, Minimal-MUMCUT, based on this insight. Given a predicate in minimal DNF, a determination is made of which constituent criteria are …",37
Are we there yet? How redundant and equivalent mutants affect determination of test completeness,"Bob Kurtz, Paul Ammann, Jeff Offutt, Mariet Kurtz",2016/4/11,"Source 2016 IEEE Ninth International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","Mutation score has long been used in research as a metric to measure the effectiveness of testing strategies. This paper presents evidence that mutation score lacks the desired accuracy to determine the completeness of a test suite due to noise introduced by the redundancy inherent in traditional mutation, and that dominator mutation score is a superior metric for this purpose. We evaluate the impact of different levels of redundant and equivalent mutants on mutation score and the ability to determine completeness in developing a mutation-adequate test suite. We conclude that, in the context of our model, redundant mutants make it very difficult to accurately assess test completeness. Equivalent mutants, on the other hand, have little effect on determining completeness. Based on this information, we suggest limits to redundancy and equivalency that mutation tools must achieve to be practical for general use in …",36
The expressive power of multi-parent creation in monotonic access control models,"Paul Ammann, Ravi S Sandhu, Richard Lipton",1996/1/1,Journal Journal of Computer Security,"Formal demonstration of equivalence or nonequivalence of different security models helps identify the fundamental constructs and principles in such models. In this paper, we demonstrate the nonequivalence of two monotonic access control models that differ only in the creation operation for new subjects and/or objects; in particular, we show that single-parent creation is less expressive than multi-parent creation. The nature of the proof indicates that this result will apply to any monotonic access control model. The nonequivalence proof is carried out on an abstract access control model, following which the results are interpreted in standard formulations. In particular, we apply the results to demonstrate nonequivalence of the Schematic Protection Model (SPM) and the Extended Schematic Protection Model (ESPM). We also show how the results apply to the typed access matrix model (TAM), which is an extension of …",36
On-the-fly reading of entire databases,"Paul Ammann, Sushil Jajodia, Padmaja Mavuluri",1995/10,Journal IEEE Transactions on Knowledge and Data Engineering,"A common database need is to obtain a global-read, which is a consistent read of an entire database. To avoid terminating normal system activity, and thus improve availability, we propose an on-the-fly algorithm that reads database entities incrementally and allows normal transactions to proceed concurrently. The algorithm assigns each entity a color based on whether the entity has been globally read, and a shade based on how normal transactions have accessed the entity. Serializability of execution histories is ensured by requiring normal transactions to pass both a color test and a shade test before being allowed to commit. Our algorithm improves on a color-only-based scheme from the literature; the color-only scheme does not guarantee serializability.< >",35
A two snapshot algorithm for concurrency control in multi-level secure databases,"Paul Ammann, Frank Jacckle, Sushi Jajodia",1992/5/1,Conference Proceedings 1992 IEEE Computer Society Symposium on Research in Security and Privacy,"In the course of semiconductor manufacturing, various e-test measurements (also known as inline or kerf measurements) are collected to monitor the health-of-line and to make wafer scrap decisions preceding final test. These measurements are typically sampled spatially across the surface of the wafer from between-die scribe line sites, and include a variety of measurements that characterize the wafer's position in the process distribution. However, these measurements are often only used for wafer-level characterization by process and test teams, as the sampling can be quite sparse across the surface of the wafer. In this work, we introduce a novel methodology for extrapolating sparsely sampled e-test measurements to every die location on a wafer using Gaussian process models. Moreover, we introduce radial variation modeling to address variation along the wafer center-to-edge radius. The proposed …",34
Maintaining replicated authorizations in distributed database systems,"Pierangela Samarati, Paul Ammann, Sushil Jajodia",1996/2/1,Journal Data & knowledge engineering,"We consider the propagation of authorizations in distributed database systems. We present an optimistic replica control algorithm that ensures that the authorization table at any given site evolves consistently with respect to other sites. The motivation for using optimistic replica control to maintain authorizations is that site and communication failures do not needlessly delay authorization changes. In addition, the semantics of the authorization operations we employ can be exploited to resolve transient inconsistencies without the expense of an undo-redo mechanism. Instead, we give efficient, direct algorithms whereby a site scans its log of authorization requests and updates its authorization table correspondingly. From the system perspective, any inconsistencies in the authorization table replicas maintained at different sites are transient and are eliminated by further communication. We show how a site can prune its …",33
"Revisiting the relationship between fault detection, test adequacy criteria, and test set size","Yiqun T Chen, Rahul Gopinath, Anita Tadakamalla, Michael D Ernst, Reid Holmes, Gordon Fraser, Paul Ammann, René Just",2020/12/21,Book Proceedings of the 35th IEEE/ACM International Conference on Automated Software Engineering,"The research community has long recognized a complex interrelationship between fault detection, test adequacy criteria, and test set size. However, there is substantial confusion about whether and how to experimentally control for test set size when assessing how well an adequacy criterion is correlated with fault detection and when comparing test adequacy criteria. Resolving the confusion, this paper makes the following contributions: (1) A review of contradictory analyses of the relationships between fault detection, test adequacy criteria, and test set size. Specifically, this paper addresses the supposed contradiction of prior work and explains why test set size is neither a confounding variable, as previously suggested, nor an independent variable that should be experimentally manipulated. (2) An explication and discussion of the experimental designs of prior work, together with a discussion of conceptual and …",31
A logic mutation approach to selective mutation for programs and queries,"Garrett Kaminski, Upsorn Praphamontripong, Paul Ammann, Jeff Offutt",2011/10/1,Journal Information and Software Technology,"Program mutation testing is a technique for measuring and generating high quality test data. However, traditional mutation operators are not necessarily efficient or effective. We address three specific issues. One, test data that kills all mutants generated by current mutation tools can still miss detection of some common logic faults because such tools lack appropriate logic mutation operators. Two, the number of mutants generated is often unnecessarily large. Three, many equivalent mutants can be generated and these can be difficult to eliminate.",29
Extending the creation operation in the schematic protection model,"PE Ammann, Ravi S Sandhu",1990/12/3,Conference [1990] Proceedings of the Sixth Annual Computer Security Applications Conference,"Protection models provide a formalism for specifying control over access to information and other resources in a multi-user computer system. Useful protection models must balance expressive power with the complexity of safety analysis i.e. the determination of whether or not a given subject can ever acquire access to a given resource. The authors argue that, in terms of expressive power, a joint creation operation is a natural candidate for inclusion in an access control model, particularly in the context of integrity considerations. They extend the Schematic Protection Model (SPM) to allow for groups of subjects to jointly create other subjects and objects. They discuss the safety properties of ESPM. Despite the increase in expressive power, ESPM retains tractable safety analysis for many cases of practical interest.< >",28
An industrial case study of structural testing applied to safety-critical embedded software,"Jing Guan, Jeff Offutt, Paul Ammann",2006/9/21,Book Proceedings of the 2006 ACM/IEEE international symposium on Empirical software engineering,"Effective testing of safety-critical real-time embedded software is difficult and expensive. Many companies are hesitant about the cost of formalized criteria-based testing and are not convinced of the benefits. This paper presents the results of an industrial case study that compared the normal testing at a company (manual functional testing) with testing based on the logic-based criterion of correlated active clause coverage (CACC). The evaluation was performed during the testing of embedded, real-time control software that has been deployed in a safety-critical application in the transportation industry. We found in our study that the test cases generated to satisfy the CACC criterion detected major safety-critical faults that were not detected by functional testing. We also found that the cost required for CACC testing was not necessarily higher than the cost of functional testing. There were also several faults that were …",27
Coverage criteria for state based specifications,"Paul Ammann, Jeff Offutt, Wuzhi Xu",2008,"Journal Formal Methods and Testing: An Outcome of the FORTEST Network, Revised Selected Papers","Test engineers often face the task of developing a set of test cases that are appropriate for a given software artefact. The software testing literature is replete with testing methods tailored to the various specification, design, and implementation methods used in software engineering. This chapter takes a novel inverted view. Instead of starting with the specific artefact at hand, we identify two general sets of coverage criteria – one based on graphs and the other based on predicates. We then ask two questions with respect to the specific artefact under test: (1) What graphs are suitable abstractions of the artefact for the purpose of testing? (2) What predicates should be extracted from this artefact for the purpose of testing? Combining the answers to these two questions with the standard graph-based and logic-based coverage criteria yields test requirements. The test engineer can then proceed to identify test …",22
A policy driven approach to email services,"Saket Kaushik, Paul Ammann, Duminda Wijesekera, William Winsborough, Ronald Ritchey",2004/6/9,"Conference Proceedings. Fifth IEEE International Workshop on Policies for Distributed Systems and Networks, 2004. POLICY 2004.","The primary original design goal for email was to provide best-effort message delivery. Unfortunately, as the ever increasing uproar over SPAM demonstrates, the existing email infrastructure is no longer well suited to the worldwide set of email users - particularly email receivers. Rather than propose yet another band-aid solution to SPAM, this paper rethinks email from the requirements perspective, albeit with the constraint of designing a system suitable for incremental adoption in the current environment. Our result to this exercise is a policy driven email service in which the interests of each principal can be articulated and accommodated. Our scheme rewards faithful senders with better quality of service and discourages misbehavior. Our scheme provides receivers with policy-driven control over whether and how a given message appears in the recipients mailbox.",22
On the performance of software testing using multiple versions,"Susan S Brilliant, John C Knight, PE Ammann",1990/1/1,Conference Digest of Papers. Fault-Tolerant Computing: 20th International Symposium,"The authors present analytic models of the performance of comparison checking (also called back-to-back testing and automatic testing), and they use these models to investigate its effectiveness. A Markov model is used to analyze the observation time required for a test system to uncover a fault using comparison checking. A basis for evaluation is provided by developing a similar Markov model for the analysis of ideal checking, ie using a perfect (through unrealizable) oracle. Also presented is a model of the effect of comparison checking on a version's failure probability as testing proceeds. Again, comparison checking is evaluated against ideal checking. The analyses show that comparison checking is a powerful and effective technique.<>",22
Reconciling perspectives of software logic testing,"Garrett Kaminski, Gregory Williams, Paul Ammann",2008/9,"Journal Software Testing, Verification and Reliability","Many software logic test coverage criteria have emerged over the past several years; however, they are scattered throughout the literature. The goal of this paper is to describe the various logic tests and explain their rationale in a centralized location in order to aid software testers in their decisions about which to implement. Each logic test is examined in terms of its minimum and maximum test sizes, subsumption relationship to other logic tests, and its fault detection capability. It is shown that although semantic tests generally have a smaller test size, syntactic tests are better in terms of fault detection capability. Furthermore, it is shown that the syntactic tests subsume their semantic counterparts. Two new software faults are introduced to Kuhn's fault hierarchy, namely the term insertion fault and the term negation fault, and the fault detection capability of the logic tests with respect to these faults is examined. A new …",21
Relating counterexamples to test cases in CTL model checking specifications,"Duminda Wijesekera, Paul Ammann, Lingya Sun, Gordon Fraser",2007/7/9,Book Proceedings of the 3rd international workshop on Advances in model-based testing,"Counterexamples produced by model checkers are frequently exploited for the purpose of testing. Counterexamples and test cases are generally treated as essentially the same thing, while in fact they can differ significantly. For example, it might take more than one test case to ""cover"" a given counterexample, because not all property violations can be illustrated with linear counterexamples. This paper presents a formal relationship between counterexamples and test cases in the context of the Computation Tree Logic (CTL), the logic of the popular model checker SMV. Given a test requirement as a CTL formula, we define what it means for a set of test cases to cover a counterexample associated with that requirement. This result can not only be used in the generation of a test set that satisfies a given test coverage criterion, but also in the determination of whether an extant test set satisfies the criterion. Our results …",21
An algebra for composing ontologies,"Saket Kaushik, Csilla Farkas, Duminda Wijesekera, Paul Ammann",2006/11,Journal FOIS,"Ontologies are used as a means of expressing agreements to a vocabulary shared by a community in a coherent and consistent manner. As it happens in the Internet, ontologies are created by community members in a decentralized manner, requiring that they be merged before being used by the community. We develop an algabra to do so in the Resource Discription Framework (RDF). To provide formal semantics of the proposed algebraic operators, we type a fragment of the RDF syntax.",21
Incorporating transaction semantics to reduce reprocessing overhead in replicated mobile data applications,"Peng Liu, Paul Ammann, Sushil Jajodia",1999/6/5,Conference Proceedings. 19th IEEE International Conference on Distributed Computing Systems (Cat. No. 99CB37003),"Update anywhere-anytime-anyway transactional replication has unstable behavior as the workload scales up. To reduce this problem, a two-tier replication algorithm is proposed in (Gray et al., 1996) that allows mobile applications to propose tentative transactions that are later applied to a master copy. However it can suffer from heavy reprocessing overhead in many circumstances. We present the method of merging histories instead of reprocessing to reduce the overhead of two-tier replication. The basic idea is when a mobile node connects to the base nodes merging the tentative history into the base history so that substantial work of tentative transactions could be saved. As a result, a set of undesirable transactions (denoted B) have to be backed out to resolve the conflicts between the two histories. Desirable transactions that are affected directly or indirectly, by the transactions in B complicate the process of …",21
Implementing transaction control expressions by checking for absence of access rights.,"Paul Ammann, Ravi S Sandhu",1992/12,Conference ACSAC,"Separation of duties is an important, real-world requirement that access control models should support. In/13), Sandhu introduced the transaction control ez-pression (TCE) for specifying dynamic separation of duties. In this paper we consider the implementation of TCEs in the typed access matriz model (TAM) re-cently proposed by Sandhu/16). We show that TAM requires eztension for satisfactory handling of dynamic separation of duties. In particular, dynamic separation requires the capability to ezplicitly test for the absence of rights in cells of the access matriz. We illustrate how TAM, eztended to incorporate such tests, can implement TCEs. We also discuss the impact of checks for absence of rights on safety analysis (ie, the determination of whether or not a given subject can acquire a given right to a given object).",20
Design fault tolerance,"JC Knight, PE Ammann",1991/1/1,Journal Reliability Engineering & System Safety,Typical software fault tolerance techniques are modeled on successful hardware fault tolerance techniques. The software fault tolerance techniques rely on design redundancy to tolerate residual design faults in the software; the hardware fault tolerance techniques rely on component redundancy to tolerate physical degradation in the hardware. Investigations of design redundant software have revealed difficulties in adapting the hardware strategy to software.,20
Globally consistent event ordering in one-directional distributed environments,"Paul Ammann, Sushil Jajodia, Phyllis G Frankl",1996/6,Journal IEEE transactions on parallel and distributed systems,"We consider communication structures for event ordering algorithms in distributed environments where information flows only in one direction. Example applications are multilevel security and hierarchically decomposed databases. Although the most general one directional communication structure is a partial order, partial orders do not enjoy the property of being consistently ordered, a formalization of the notion that local ordering decisions are ensured to be globally consistent. Our main result is that the crown free property is necessary and sufficient for a communication structure to be consistently ordered. We discuss the computational complexity of detecting crowns and sketch typical applications.",18
Test generation and recognition with formal methods,"Paul E Ammann, PE Black",2000/6,"Journal The First International Workshop on Automated Program Analysis, Testing and Verification",,17
Ensuring atomicity of multilevel transactions,"Paul Ammann, Sushil Jajodia, Indrakshi Ray",1996/5/6,Conference Proceedings 1996 IEEE Symposium on Security and Privacy,"Ensuring atomicity is a major outstanding problem with present methods of handling multilevel transactions. The chief difficulty is that a high section of a transaction may be unable to complete due to violations of the integrity constraints, and a rollback of sections can be exploited to implement a covert channel. We define a notion of semantic atomicity which guarantees that either all or none of the sections of a transaction are present in any history. The notion of correct executions in our model is based on semantic correctness-that is, maintenance of integrity constraints-rather than serializability. We give a method whereby the application developer can statically analyze the set of transactions in the application and determine if the set ensures semantic atomicity and other desirable properties.",17
Using formal methods to reason about semantics-based decompositions of transactions,"Paul Ammann, Sushil Jajodia, Indrakshi Ray",1995/9/11,Conference VLDB,"Many researchers have investigated the process of decomposing transactions into smaller pieces to increase concurrency. The research typically focuses on implementing a decomposition supplied by the database application developer, with relatively little attention to what constitutes a desirable decomposition and how the developer should obtain such a decomposition. In this paper, we argue that the decomposition process itself warrants attention. A decomposition generates a set of proof obligations that must be satisfied to show that a particular decomposition correctly models the original collection of transactions. We introduce the notion of semantic histories to formulate and prove the necessary properties. Since the decomposition impacts not only the atomicity of transactions, but isolation and consistency as well, we present a technique based on formal methods that allows these properties to be surrendered in a carefully controlled manner.",16
An efficient multiversion algorithm for secure servicing of transaction reads,"Paul Ammann, Sushil Jajodia",1994/11/2,Book Proceedings of the 2nd ACM Conference on Computer and communications security,"We propose an efficient multiversion algorithm for servicing read requests in secure multilevel databases. Rather than keep an arbitrary number of versions of a datum, as standard multiversion algorithms do, the algorithm presented here maintains only a small fixed number of versions—up to three—for a modified datum. Each version corresponds to the state of the datum at the end of an externally defined version period. The algorithm avoids both covert channels and starvation of high transactions, and applies to security structures that are arbitrary partial orders. The algorithm also offers long-read transactions at any security level conflict-free access to a consistent, though slightly dated, view of any authorized portion of the database. We derive constraints sufficient to  guarantee one-copy serializability of executions histories, and then exhibit an algorithm that satisfies these constraints.",16
Issues Influencing the Use of N-Version Programming.,"John C Knight, Paul Ammann",1989/8,Conference IFIP Congress,"N-version programming is a software fault tolerance technique that is modeled on the hardware fault tolerance technique of N-modular redundancy. N-version programming relies on design redundancy to tolerate residual design faults in the software; N-module redundancy relies on component redundancy to tolerate physical degradation in the hardware. Investigations of N-version programming have revealed difficulties in adapting the hardware strategy to software. The difficulties are: 1) modeling-the independence model suitable for hardware reliability assessment has been shown to be invalid and naive for software, and more appropriate models have been developed, 2) effect of testing-testing the versions against one another with comparison checking causes them to compute progressively more similar functions, thereby reducing the opportunity for the redundant designs to tolerate remaining faults, 3) decision algorithms-small differences in computed internal values can cause versions to diverge, thereby producing outputs that are individually correct but defeat the decision algorithm, and 4) economicseconomic models of N-version software have not considered all phases of development, particularly the maintenance phase. This paper summarizes the technical and economic aspects of N-version software that must be considered to justify use of the technique.",15
Planar lattice security structures for multilevel replicated databases,"Paul Ammann, Sushil Jajodia",1993/9/12,Book Proceedings of the IFIP WG11. 3 Working Conference on Database Security VII,,14
Transforming mutation testing from the technology of the future into the technology of the present,Paul Ammann,2015/4/13,"Journal International conference on software testing, verification and validation workshops. IEEE",,13
Adding criteria-based tests to test driven development,"William Shelton, Nan Li, Paul Ammann, Jeff Offutt",2012/4/17,"Conference 2012 IEEE Fifth International Conference on Software Testing, Verification and Validation","Test driven development (TDD) is the practice of writing unit tests before writing the source. TDD practitioners typically start with example-based unit tests to verify an understanding of the software's intended functionality and to drive software design decisions. Hence, the typical role of test cases in TDD leans more towards specifying and documenting expected behavior, and less towards detecting faults. Conversely, traditional criteria-based test coverage ignores functionality in favor of tests that thoroughly exercise the software. This paper examines whether it is possible to combine both approaches. Specifically, can additional criteria based tests improve the quality of TDD test suites without disrupting the TDD development process? This paper presents the results of an observational study that generated additional criteria-based tests as part of a TDD exercise. The criterion was mutation analysis and the additional …",13
"Using semantic correctness in multidatabases to achieve local autonomy, distribute coordination, and maintain global integrity","Indrakshi Ray, Paul Ammann, Sushil Jajodia",2000/11/1,Journal Information Sciences,"A multidatabase poses the following four, often contradictory, requirements. First, local databases require both design autonomy to accommodate the diverse legacy nature of the local databases, and execution autonomy to ensure that local transactions are not unduly blocked by global transactions. Second, management of global transactions must be distributed to avoid bottlenecks and to tolerate failure in the global database. Third, both local and global integrity constraints must be maintained. Finally, concurrent processing of transactions requires that execution histories be correct, an objective traditionally achieved with serializability. Although alternate forms of correctness have been proposed, none of the solutions advanced to date has simultaneously achieved all four requirements. We propose a transaction processing model that uses a semantics-based notion of correctness to achieve all four requirements …",13
Reducing logic test set size while preserving fault detection,"Garrett Kaminski, Paul Ammann",2011/9,"Journal Software Testing, Verification and Reliability","Logic criteria demand inputs that guarantee detection of certain faults. One such criterion, MUMCUT, is composed of three criteria, where each constituent criterion ensures the detection of specific faults. In practice, the criteria may overlap in terms of faults detected, leading to redundant tests, but due to the fact that infeasible requirements do not result in tests, all the constituent criteria are needed. The key insight of this paper is that analysis of the feasibility of the constituent criteria can reduce test set size without sacrificing fault detection for specific faults. This paper introduces a new logic criterion, Minimal‐MUMCUT, and shows how it can apply to minimal DNF, minimal CNF, and general form Boolean expressions. With Minimal‐MUMCUT, a determination is made of which constituent criteria are feasible, and hence necessary, at the level of individual literals and terms. An empirical study found that Minimal …",12
A fault tolerance approach to survivability,"Paul Ammann, Sushil Jajodia, Peng Liu",1998/7/7,"Conference Proceedings Computer Security, Dependability, and Assurance: From Needs to Solutions (Cat. No. 98EX358)","Attacks on computer systems have received a great deal of press attention; however, most of the focus has been on how an attacker can disrupt an organization's operations. Although attack prevention is clearly preferred, preventive measures do fail, and some attacks inevitably succeed in compromising some or all of particular systems, i.e., databases. We propose research into a fault-tolerance approach that addresses all phases of survivability: attack detection, damage confinement, damage assessment and repair, and attack avoidance. We focus attention on continued service and recovery issue. A promising area of research for continued service addresses relaxed notions of consistency. Expanding on the notion of self stabilization, the idea is to formalize the degree of damage under which useful services is still possible. A complementary research area for recovery is the engineering of suitable mechanisms …",12
A novel self-paced model for teaching programming,"Jeff Offutt, Paul Ammann, Kinga Dobolyi, Chris Kauffmann, Jaime Lester, Upsorn Praphamontripong, Huzefa Rangwala, Sanjeev Setia, Pearl Wang, Liz White",2017/4/12,Book Proceedings of the Fourth (2017) ACM Conference on Learning@ Scale,"The Self-Paced Learning Increases Retention and Capacity (SPARC) project is responding to the well-documented surge in CS enrollment by creating a self-paced learning environment that blends online learning, automated assessment, collaborative practice, and peer-supported learning. SPARC delivers educational material online, encourages students to practice programming in groups, frees them to learn material at their own pace, and allows them to demonstrate proficiency at any time. This model contrasts with traditional course offerings, which impose a single schedule of due dates and exams for all students. SPARC allows students to complete courses faster or slower at a pace tailored to the individual, thereby allowing universities to teach more students with the same or fewer resources. This paper describes the goals and elements of the SPARC model as applied to CS1. We present results so far and …",11
BPEL orchestration of secure webmail,"Saket Kaushik, Duminda Wijesekera, Paul Ammann",2006/11/3,Book Proceedings of the 3rd ACM workshop on Secure web services,"WebMail proposes to migrate existing SMTP-based mail systems to Web-Services. We show how a verifiably-correct, generic mail service that enables extensions of SMTP-based standard mail use cases that avoids known misuse cases can be specified using WSDL and orchestrated using BPEL.",11
Email feedback: a policy-based approach to overcoming false positives,"Saket Kaushik, William Winsborough, Duminda Wijesekera, Paul Ammann",2005/11/11,Book Proceedings of the 2005 ACM workshop on Formal methods in security engineering,"Current email-control mechanisms, though highly effective, are pro-ne to dropping desirable messages. This can be attributed to their coarseness in filtering out undesirable messages from desirable ones. As a result policies to control undesirable messages are often overly permissive. To allow policies to be more restrictive, the transmission mechanism must be made aware of the ways to document a message so that it is acceptable downstream, thus giving the senders a chance of meeting those requirements. In this work, we design a scheme to enable rejected, but desirable messages to be upgraded in a way that they meet downstream requirements. We call this process 'message refinement'. This in turn allows downstream principals to express and enforce precise requirements as the risk of losing desirable messages is minimized. To apply this scheme uniformly to any email-control mechanism, we provide a …",11
"A framework for establishing, assessing, and managing trust in inter-organizational relationships","Joseph Pamula, Paul Ammann, Sushil Jajodia, Ronald Ritchey",2006/11/3,Book Proceedings of the 3rd ACM workshop on Secure web services,"In this paper, we present an efficient, novel framework for establishing, assessing, and managing trust in inter-organizational relationships, in terms of allowable network sharing, that is based on analyzing an invariance property of a computer network environment. Our goal is to answer the following two questions: (1) From any given host in one network, what level of access, direct or indirect, is implied to each host in another network? This addresses the consequences of connecting two networks on access levels between networks. (2) What are the effects, in terms of access internal to a given network, of connecting to another network? This addresses the consequences of connecting two networks on access levels internal to a given network. Answers to these questions allow an informed business decision to be made as to whether the proposed network sharing should be allowed, and, if so, what the …",10
A safety kernel for traffic light control,Paul Ammann,1995/6/25,"Conference COMPASS'95 Proceedings of the Tenth Annual Conference on Computer Assurance Systems Integrity, Software Safety and Process Security'","The success of kernels for enforcing security has led to proposals to use kernels for enforcing safety. This paper presents a feasibility demonstration of one particular proposal for a safety kernel via the application of traffic light control. The paper begins with the safety properties for traffic light control and specifies a kernel that maintains the safety properties. An implementation sketch of the kernel in Ada is given and use of the kernel is discussed. The contribution of the paper is a demonstration that a kernel is a feasible and desirable technique for software in a realistic, safety-critical application. The paper also illustrates how formal methods aid the software engineer in constructing and reasoning about such software.",10
A distributed implementation of the extended schematic protection model.,"Paul Ammann, Ravi S Sandhu, Gurpreet S Suri",1991/12/2,Conference ACSAC,"Protection models provide a formalism for specifying control over access to information and other resources in a multi-user computer system. One such model, the Extended Schematic Protection Model (ESPM), has expressive power equivalent to the monotonic access matrix model of Harrison, Ruzzo, and Ullman [7]. Yet ESPM retains tractable safety analysis for many cases of practical interest. Thus ESPM is a very general model, and it is of interest whether ESPM can be implemented in a reasonable manner. In this paper, we outline a distributed implementation for ESPM. Our implementation is capability-based, with an architecture where servers act as mediators to all subject and object access. Capabilities are made nontransferable by burying the identity of subjects in them, and unforgeable by using a public key encryption algorithm. Timestamps and public keys are used as mechanisms for revocation.",10
Introduction to Software Testing Edition 2,"Paul Ammann, Jeff Offutt",2017,"Publisher Cambridge University Press, New York, NY","This document contains the work-in-progress solutions for the second edition of the text. The goal is to keep the solution manuals synchronized with the textbook so that there are no “TBD” solutions, as persisted in the first edition for many years. We distinguish between “student solutions” and “instructor only” for the convenience of both. Students can work homeworks then check their own answers. Instructors can assign homeworks with some confidence that students will do their own work instead of looking up the answer in the manual.",9
One-representative safety analysis in the non-monotonic transform model,"Paul E Ammann, Ravi S Sandhu",1994/6/14,Conference Proceedings The Computer Security Foundations Workshop VII,"We analyze the safety question for the Non-Monotonic Transform (NMT) model, an access control model that encompasses a wide variety of practical access control mechanisms. In general, safety analysis, i.e. whether it is possible for a specified subject to obtain a given access right for a certain object, is computationally intractable, even for many monotonic models. We identify one-representable NMT schemes and argue that they have tractable safety analysis. Safety analysis of one-representable schemes considers exactly one representative of each type of subject in the initial state, and thus the complexity of safety analysis is independent of the total number of subjects in the system. We demonstrate by example that one-representable schemes admit applications of practical interest, and that safety analysis guides the construction of such schemes.< >",9
Using abstraction and Web applications to teach criteria-based test design,"Jeff Offutt, Nan Li, Paul Ammann, Wuzhi Xu",2011/5/22,Conference 2011 24th IEEE-CS Conference on Software Engineering Education and Training (CSEE&T),"The need for better software continues to rise, as do expectations. This, in turn, puts more emphasis on finding problems before software is released. Industry is responding by testing more, but many test engineers in industry lack a practical, yet theoretically sound, understanding of testing. Software engineering educators must respond by teaching students to test better. An essential testing skill is designing tests, and an efficient way to design high quality tests is to use an engineering approach: test criteria. To achieve the maximum benefit, criteria should be used during unit (developer) testing, as well as integration and system testing. This paper presents an in-depth teaching experience report on how we successfully teach criteria-based test design using abstraction and publicly accessible web applications. Our teaching materials are freely available online or upon request.",8
Implementing semantic-based decomposition of transactions,"Sushil Jajodia, Indrakshi Ray, Paul Ammann",1997/6/16,Conference CAiSE,"In some database applications, performance requirements are not satisfied by the traditional approach of serializability, in which transactions appear to execute atomically and in isolation on a consistent database state. Although many researchers have investigated the process of decomposing transactions into steps to increase concurrency, the focus of the research is typically on implementing a decomposition supplied by the database application developer, with relatively little attention to what constitutes a desirable decomposition and how the developer should obtain such a decomposition. In our research, we focus on the decomposition process itself.",8
Semantics-based transaction processing: satisfying conflicting objectives,"Paul Ammann, Sushil Jajodia",1997/4,Journal IEEE Concurrency,"The traditional correctness criteria of serializability forces database designers to make design trade-offs. For example, in multidatabases, the designer balances the objectives of local design and execution autonomy, decentralized management of global transactions, maintenance of global integrity constraints, and execution-history correctness. The last objective is typically assessed with respect to some variant of conflict serializability. Switching to a semantics-based perspective of correctness can greatly reduce the conflict between the remaining objectives. In the case of multidatabases, the conflict can be entirely avoided for certain applications. We describe the success of the semantics-based perspective in three distinct application areas: multidatabases, secure multilevel databases, and long-duration transactions. Others have developed a fault tolerance approach with a closely related formal basis. Moreover …",8
Concurrency control in a secure multilevel database via a two-snapshot algorithm,"Paul Ammann, Frank Jaeckle, Sushil Jajodia",1995/1/1,Journal Journal of Computer Security,"We offer a concurrency conirol algorithm for replicated, secure, multilevel databases. We compare the algorithm with a multiversion approach and with the typical full-replication approach. In the full-replication approach, each security level maintains a container that holds a complete copy of data at lower security levels. In the approach described here, access to data at lower security levels is through shared, read-only snapshots, where a constant number of snapshots at each level–two, as it turns out–is sufficient. We derive necessary properties for snapshots, give a switching algorithm to assign read-downs to snapshots, specify a snapshot creation algorithm, demonstrate that the approach is free of indirect channels and starvation, and prove one-copy serializability on execution histories. In contrast to some comparable algorithms, our algorithm is correct for any security structure that is a partial order.",8
Reachability and propagation for LTL requirements testing,"Gordon Fraser, Paul Ammann",2008/8/12,Conference 2008 The Eighth International Conference on Quality Software,"It is important to test software with respect to user requirements, especially when adhering to safety standards, which require traceability from requirements to test cases. While research has resulted in many different model based testing techniques, only a few consider requirement properties; this paper helps fill this gap. We identify two fundamental characteristics of a test case intended to evaluate a given requirement property. The two characteristics are adapted from the venerable Reachability, Infection, and Propagation (RIP) model for faults and failures in ordinary code. In the context of requirements testing, we propose the reachability property amounts to the property not being vacuously true on a given test case, and the propagation property amounts to a potential violation of the property on the test case being observable. In particular, we formalize these notions in the context of requirement properties …",7
A fault tolerance approach to survivability,"Paul Ammann, Sushil Jajodia, Peng Liu",1998/7/7,"Conference Proceedings Computer Security, Dependability, and Assurance: From Needs to Solutions (Cat. No. 98EX358)","Attacks on computer systems have received a great deal of press attention; however, most of the focus has been on how an attacker can disrupt an organization's operations. Although attack prevention is clearly preferred, preventive measures do fail, and some attacks inevitably succeed in compromising some or all of particular systems, i.e., databases. We propose research into a fault-tolerance approach that addresses all phases of survivability: attack detection, damage confinement, damage assessment and repair, and attack avoidance. We focus attention on continued service and recovery issue. A promising area of research for continued service addresses relaxed notions of consistency. Expanding on the notion of self stabilization, the idea is to formalize the degree of damage under which useful services is still possible. A complementary research area for recovery is the engineering of suitable mechanisms …",12
"Computer security, fault tolerance, and software assurance","Sushil Jajodia, Paul Ammann",1999/1/1,Journal IEEE Concurrency (out of print),"Although we could cite many smaller ideas as success stories in computer security, here are some major ones. The Trusted Computer System Evaluation Criteria is a collection of criteria used to grade or rate the security offered by a computersystem product.(The TCSEC is sometimes called “the Orange Book” because of its orange cover.) Evaluation process based on the Orange Book has resulted in high-assurance (B3 and A1) operating systems of high quality and security, despite their limitation. Private-key cryptography, such as DES, is a commercial success. Although the 56-bit key size makes it vulnerable to brute-force attacks, DES has held up remarkably well against cryptoanalysis. Public-key cryptography is widely studied and well-understood, although still based on an unproven assumption. The Computer Emergency Response Team (CERT) provides important public services, alerting the public to new …",7
Propagation of authorizations in distributed database systems,"Pierangela Samarati, Paul Ammann, Sushil Jajodia",1994/11/2,Book Proceedings of the 2nd ACM Conference on Computer and communications security,"We consider the propagation of authorizations in distributed database systems. If no constraints are imposed on the propagation of authorization changes, then the authorization states at different sites may evolve inconsistently. A standard solution is to suppress the distributed aspect and make all changes appear as if they had occurred in some serial order at a single site, perhaps via an atomic commit protocol. However, rigid insistence on consistency may result in authorization changes being needlessly delayed, a problem exacerbated in the context of site or communication failures. We propose an optimistic authorization propagation algorithm. We specify an authorization table and a set of operations for altering the authorization table. Each site maintains a log of authorization  operations. We exploit the semantics of authorization operations to avoid relying on an undo-redo mechanism for processing out of …",7
Applications of optimization to logic testing,"Garrett Kaminski, Paul Ammann",2010/4/6,"Conference 2010 Third International Conference on Software Testing, Verification, and Validation Workshops","A tradeoff exists in software logic testing between test set size and fault detection. Testers may want to minimize test set size subject to guaranteeing fault detection or they may want to maximize faults detection subject to a test set size. One way to guarantee fault detection is to use heuristics to produce tests that satisfy logic criteria. Some logic criteria have the property that they are satisfied by a test set if detection of certain faults is guaranteed by that test set. An empirical study is conducted to compare test set size and computation time for heuristics and optimization for various faults and criteria. The results show that optimization is a better choice for applications where each test has significant cost, because for a small difference in computation time, optimization reduces test set size. A second empirical study examined the percentage of faults detected in a best, random, and worst case, first for a test set size of one …",6
Parameter validation using constraint optimization for modeling and simulation,"Guodong Shao, A Brodsky, P Ammann, C McLean",2009/6,Journal Proceedings of the industrial simulation conference 2009,"Modeling and simulation (M&S) techniques are increasingly being used to solve problems and aid decision making in many different fields. Results of simulations are expected to provide reliable information for decision makers. But potential errors may be introduced during the M&S development lifecycle. It is critical to ensure to build the right model and the model is built right. M&S community has had intensive Verification and Validation (V&V) research. But V&V activities are often not formally performed in most of the cases. For those who perform V&V activities, they normally wait until development of the simulation modeling is finished. Practical and solid validation techniques are hence needed. In this paper, the authors propose a validation methodology that allows parallel simulation development and model parameter validation, ie first the simulation model can be built with unknown parameters included; and then, those parameters can be estimated using a built-in constraint optimizer. Finally the initially unknown parameters are replaced with the found optimal values. The model is then ready for future output prediction. As an example application, a simple supply chain cost simulation model was discussed using the proposed methodology.",6
Using logic criterion feasibility to reduce test set size while guaranteeing double fault detection,"Garrett Kaminski, Paul Ammann",2009/4/1,"Conference 2009 International Conference on Software Testing, Verification, and Validation Workshops","Logic criteria are used in software testing to find inputs that guarantee detecting certain faults. Thus, satisfying a logic criterion guarantees killing certain mutants. Some logic criteria are composed of other criteria. Determining component criterion feasibility can be used as a means to reduce test set size without sacrificing fault detection. This paper introduces a new logic criterion based on component criterion feasibility. Given a predicate in minimal DNF, a determination is made of which component criteria are feasible for individual literals and terms. This in turn provides determination of which criteria are necessary to detect double faults and kill second-order mutants. A test set satisfying this new criterion guarantees detecting the same double faults as a larger test set satisfying another criterion. An empirical study using predicates in avionics software showed that tests sets satisfying the new criterion detected all but …",6
A semantic-based transaction processing model for multilevel transactions 1,"Indrakshi Ray, Paul Ammann, Sushil Jajodia",1998/1/1,Journal Journal of Computer Security,"Multilevel transactions have been proposed for multilevel secure databases; in contrast to most proposals, such transactions allow users to read and write across multiple security levels. The security requirement that no high level operation influence a low level operation often conflicts with the atomicity requirement of the standard transaction processing model. In particular, others have shown that no concurrency control algorithm based on the standard transaction processing model can guarantee both atomicity and security. This conflict motivates us to propose an alternative semantic-based transaction processing model for multilevel transactions. Our model uses the semantics of the application to analyze an application and reason about its behavior. Our notion of correctness is based on semantic correctness instead of serializability as in the standard transaction processing model. Semantic correctness ensures …",6
Using formal methods to mechanize category-partition testing,"Paul Ammann, Jeff Offutt",1993/9,Book Technical Report ISSE-TR-93-105,,6
A two snapshot algorithm for concurrency control in secure multilevel databases,"P Amman, F Jaeckle, S Jajodia",1992,Journal Proc. of IEEE Symp. on Security and Privacy,,6
Using the b-toolkit to ensure safety in scr specifications,"Indrakshi Ray, Paul Ammann",1997/6/16,Conference Proceedings of COMPASS'97: 12th Annual Conference on Computer Assurance,"SCR (Software Cost Reduction) specifications are useful for specifying event-driven systems. To use SCR effectively for critical applications, automated verification of safety properties is important. The fact that model checking approaches are sometimes problematic motivates us to further examine the alternative approach of theorem proving. Theorem proving, in general, is a difficult task; however the regular structure of the proof obligations generated from SCR specifications suggests that relatively unsophisticated theorem provers can discharge many of these obligations. As a feasibility study, we use the B-Toolkit to detect safety violations in an example SCR specification. The B-Toolkit is a good choice because it is commercially available and Supports verified refinement to executables in a commercial programming language (C). We convert the mode transition table in the example SCR specification to an AMN …",5
Evaluating a test automation decision support tool,"Kesina Baral, Rasika Mohod, Jennifer Flamm, Seth Goldrich, Paul Ammann",2019/4/22,"Conference 2019 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","Goldrich and Flamm developed the MITRE Automated Test Decision Framework (ATDF) to help MITRE government sponsors (and, via sharing on GitHub, development organizations in general) move from manually tested legacy software towards automated test, continuous integration, continuous deployment, and, ultimately, DevOps. Often such legacy systems comprise multiple components with manual test procedures. The objective of the empirical study described in this paper is to determine whether ATDF usefully ranks components with respect to Return on Investment (ROI) when introducing automated tests. ROI is simply the ratio of profit to cost. When adding automated tests, what will be the profit that these tests will carry? What is the cost or level of effort to engineer a sufficient set of automated tests? Our evaluation approach models ROI using static defect counts identified by SonarLint and estimated cost to …",4
Prioritizing mutants to guide mutation testing,"Samuel J Kaufman, Ryan Featherman, Justin Alvin, Bob Kurtz, Paul Ammann, René Just",2022/5/21,Book Proceedings of the 44th International Conference on Software Engineering,"Mutation testing offers concrete test goals (mutants) and a rigorous test efficacy criterion, but it is expensive due to vast numbers of mutants, many of which are neither useful nor actionable. Prior work has focused on selecting representative and sufficient mutant subsets, measuring whether a test set that is mutation-adequate for the subset is equally adequate for the entire set. However, no known industrial application of mutation testing uses or even computes mutation adequacy, instead focusing on iteratively presenting very few mutants as concrete test goals for developers to write tests.",3
Confetti: Amplifying concolic guidance for fuzzers,"James Kukucka, Luís Pina, Paul Ammann, Jonathan Bell",2022/5/21,Book Proceedings of the 44th International Conference on Software Engineering,"Fuzz testing (fuzzing) allows developers to detect bugs and vulnerabilities in code by automatically generating defect-revealing inputs. Most fuzzers operate by generating inputs for applications and mutating the bytes of those inputs, guiding the fuzzing process with branch coverage feedback via instrumentation. Whitebox guidance (e.g., taint tracking or concolic execution) is sometimes integrated with coverage-guided fuzzing to help cover tricky-to-reach branches that are guarded by complex conditions (so-called ""magic values""). This integration typically takes the form of a targeted input mutation, e.g., placing particular byte values at a specific offset of some input in order to cover a branch. However, these dynamic analysis techniques are not perfect in practice, which can result in the loss of important relationships between input bytes and branch predicates, thus reducing the effective power of the technique. We …",3
Practice makes better: quiz retake software to increase student learning,"Kesina Baral, Jeff Offutt, Paul Ammann, Rasika Mohod",2021/8/23,Book Proceedings of the 3rd International Workshop on Education through Advanced Software Engineering and Artificial Intelligence,"In the past few years, we have made several pedagogical changes to the way we teach and assess student knowledge in our courses.These courses are undergraduate software engineering courses taken in the third or fourth years, and graduate (non-research) courses taken as part of a master’s degree. They are taken by software engineering majors and computer science majors. This paper focuses on a specific technique–allowing students to retake weekly quizzes.We use weekly quizzes to offer more frequent, yet lower stakes,assessments than the traditional midterm exam. Quizzes are usually given at the beginning of class meetings. We offer students who under-performed or who missed a quiz the chance to try again. A major contribution of this paper is a description of scheduling soft-ware we developed to facilitate the retake process. Retake quizzes are different from the original quizzes, but cover the …",3
Generating test data to distinguish conjunctive queries with equalities,"Preetham Vemasani, Alexander Brodsky, Paul Ammann",2014/3/31,"Conference 2014 IEEE Seventh International Conference on Software Testing, Verification and Validation Workshops","The widespread use of databases in software systems has increased the importance of unit testing the queries that form the interface to these databases. Mutation analysis is a powerful testing technique that has been adapted to test database queries. But each of the existing mutation approaches to testing database queries has one or more of the following shortcomings: inability to recognize equivalent mutants, inability to generate test databases automatically, or inability to mutate all aspects of a query. In this paper we address all three of these challenges by adapting results from the rich literature on query rewriting. We restrict attention to the class of conjunctive queries with equalities. In return for this restriction, we give an algorithm that recognizes equivalent mutants, generates a test database that distinguishes each nonequivalent mutant, and applies to arbitrary mutations, as long at the mutation is also a …",3
Can-follow concurrency control,"Peng Liu, Jie Li, Sushil Jajodia, Paul Ammann",2007/9/17,Journal IEEE Transactions on Computers,"Can-follow concurrency control permits a transactionto read (write) an item write-locked (read-locked) by anothertransaction with almost no delays. By combining the merits of2PL and 2V2PL, this approach mitigates the lock contention notonly between update and read-only transactions, but also betweenupdate and update transactions.",3
System testing via mutation analysis of model checking specifications,Paul Ammann,2000/1/1,Journal ACM SIGSOFT Software Engineering Notes,"System level testing consumes considerable resources in commercial projects, and much of the activity is ad-hoc and relatively unformalized. Extensive research has been devoted to software testing, but the bulk has focused on the unit level instead of the system level. This research seeks to formalize certain aspects of system level testing. In particular, the research provides methods to generate test cases with respect to a given model, and also the means to evaluate test cases from other sources, such as regression test sets.",3
Semantic-Based Decomposition of Transactions,"Paul Ammann, Sushil Jajodia, Indrakshi Ray",1997,Journal Advanced Transaction Models and Architectures,"Sometimes transactions must be decomposed into steps. The need for decomposition arises in a variety of different domains. For example, long duration transactions may be decomposed to improve performance, global transactions in multidatabases may be decomposed to preserve local database autonomy, and multilevel secure transactions may be decomposed to avoid leaking sensitive information. To achieve these various objectives, a decomposition sacrifices those desirable properties, namely atomicity, consistency, and isolation, that form the foundation of syntactically based correctness approaches such as conflict serializability. We remedy this loss by defining a semantic view of correctness organized around a new set of desirable properties that are specifically designed for reasoning about decompositions. The exact details of the semantic correctness properties depend on the domain being …",3
The partitioned synchronization rule for planar extendible partial orders,"Paul Ammann, Vijayalakshmi Atluri, Sushil Jajodia",1995/10,Journal IEEE transactions on knowledge and data engineering,"The partitioned synchronization rule is a technique for proving the correctness of concurrency control algorithms. Prior work has shown the applicability of the partitioned synchronization rule to hierarchically decomposed databases whose structure is restricted to semitrees. The principal contribution of the paper is a demonstration that the partitioned synchronization rule also applies to more general structures than semitrees, specifically, to any planar extendible partial order, a partial order which when extended with a least and a greatest element still remains planar. To demonstrate utility, the paper presents two applications of the partitioned synchronization rule. The first application shows correctness of a component based timestamp generation algorithm suitable for implementing a timestamp ordering concurrency control algorithm. The second application shows correctness of a snapshot algorithm for …",3
A five year perspective on software engineering graduate programs at george mason university,"Paul Ammann, Hassan Gomaa, Jeff Offutt, David Rine, Bo Sanden",1994,"Conference Software Engineering Education: 7th SEI CSEE Conference San Antonio, Texas, USA, January 5–7, 1994 Proceedings 7","This paper describes the experience obtained at George Mason University while developing a Master of Science program in software engineering. To date, the program has graduated over 45 students, with a current production rate of 10 to 15 a year. The paper also describes experience with a certificate program in software engineering, which is a software engineering specialization taken by Masters students in related disciplines, and the software engineering specialization within the PhD program in Information Technology. We discuss our courses, students, the successes that we have had, and the problems that we have faced.",3
An Evaluation of the Minimal-MUMCUT Logic Criterion and Prime Path Coverage.,"Garrett Kent Kaminski, Upsorn Praphamontripong, Paul Ammann, Jeff Offutt",2010,Conference Software Engineering Research and Practice,"This paper presents comparisons of the Minimal-MUMCUT logic criterion and prime path coverage. A theoretical comparison of the two criteria is performed in terms of (1) how well tests satisfying one criterion satisfy the other and (2) fault detection. We then compare the criteria experimentally. For 22 programs, we develop tests to satisfy Minimal-MUMCUT and prime path coverage. We use these tests in two separate experiments. First we measure the effectiveness of the tests developed for one criterion in terms of the other. Next we investigate the ability of the test sets to find actual faults. Faults are seeded via a mutation tool and then supplemented with mutants created by DNF logic mutation operators. We then measure the number of non-equivalent mutants killed by each test set. Results indicate that while prime path-adequate test sets are closer to satisfying Minimal-MUMCUT than vice versa, the criteria had similar fault detection and Minimal-MUMCUT required fewer tests.",2
Policy transformations for preventing leakage of sensitive information in email systems,"Saket Kaushik, William Winsborough, Duminda Wijesekera, Paul Ammann",2006,"Conference Data and Applications Security XX: 20th Annual IFIP WG 11.3 Working Conference on Data and Applications Security, Sophia Antipolis, France, July 31-August 2, 2006. Proceedings 20","In this paper we identify an undesirable side-effect of combining different email-control mechanisms for protection from unwanted messages, namely, leakage of recipients’ private information to message senders. The problem arises because some email-control mechanisms like bonds, graph-turing tests, etc., inherently leak information, and without discontinuing their use, leakage channels cannot be closed. We formalize the capabilities of an attacker and show how she can launch guessing attacks on recipient’s mail acceptance policy that utilizes leaky mechanism in an effort to avoid unwanted mail.",2
Teaching a Testing Concept (JUnit) with Active Learning,"Kesina Baral, Paul Ammann",2020/10/24,"Conference 2020 IEEE International Conference on Software Testing, Verification and Validation Workshops (ICSTW)","Active learning is an approach that facilitates learning through collaboration, communication and discussion in the classroom. Flipped classroom is a variant of active learning where lecture materials are recorded for viewing outside the classroom thereby freeing up class time for problem solving and discussions. In this demo, we show how we use active learning in the context of teaching the testing concept of property-based testing.",1
Customized Program Mutation: Inferring Mutant Utility from Program Context,"René Just, Bob Kurtz, Paul Ammann",2017,"Publisher Technical Report UM-CS-2017-004. University of Massachuse s, Amherst, Amherst, MA, USA","The purpose of selective mutation strategies is to reduce the inherent redundancy of full mutation analysis and hence obtain most of its bene t for a fraction of the cost. Unfortunately, recent research has shown that there is no xed selective mutation strategy that is e ective across a broad range of programs. In other words, for any given mutation operator, the utility (ie, usefulness) of a mutant produced by that operator varies greatly across programs. Hence, selective mutation, as currently de ned, is a dead end despite the fact that existing mutation systems are known to produce highly redundant mutants.",1
Applying MCDC to large DNF logic expressions,"Garrett Kaminski, Paul Ammann",2010,"Conference SERP 2010: proceedings of the 2010 international conference on software engineering research & practice (Las Vegas NV, July 12-15, 2010)","SERP 2010: proceedings of the 2010 international conference on software engineering research & practice (Las Vegas NV, July 12-15, 2010). 2010, pp 411-417, 7 p; ref: 15 ref",1
Distributed CLP clusters as a security policy framework for email,"Saket Kaushik, Duminda Wijesekera, William Winsborough, Paul Ammann",2005,Journal Applications of Constraint Satisfaction and Programming to Computer Security Problems,"The simple mail transfer protocol (SMTP) used to transmit e-mail was designed with no security related control, resulting in it being exploited as a means to send “unwanted” email (aka “spam”). Consequently, recent extensions have concentrated on policy based management of e-mail pipeline that begins with the sender and ends with the intended receiver. As a theoretical framework to address this problem, we propose a distributed constraint logic programming (CLP) based framework in which policies are CLP modules that control message flow, and transmit messages acceptable to downstream principals. Our syntax is based on using stratified Horn clauses with constructive negation. The distributed aspect is used to enable the system to move enforcement points upstream in the message flows, as well as to enable message senders and their service providers to add headers useful for downstream actors. It is facilitated by importing and exporting predicates (cf. Maher [19]) among legal participants of the email pipeline. Accordingly our semantics consists of an appropriately tailored three-valued semantics, where stratification is used to ensure the finite termination of policy goals. For efficient implementation, we propose to materialize the policies and show that the materialization structure” faithfully models” our distributed policy.",1
The integrity challenge,"Paul Ammann, Sushil Jajodia",2000,Journal Integrity and Internal Control in Information Systems: Strategic Views on the Need for Control,"Traditionally, system designers view integrity as Boolean: a system either has it or it doesn’t. In practice, of course, no real database enjoys complete integrity: invariably, a variety of factors conspire to make some information stale, missing, or just plain wrong. System design and analysis should recognize that real systems lack integrity, to some degree, most, if not all, of the time. Significant benefits flow from such recognition. Risk management techniques can identify the severity of different integrity loss scenarios, thereby focusing scarce resources on critical areas. A designer can deliberately sacrifice nonessential integrity under carefully controlled conditions to achieve other design objectives, such as performance, autonomy, availability, or security. Designers can achieve these objectives and still assure the preservation of essential aspects of integrity. Advanced applications can use explicit integrity …",1
Maintaining knowledge currency in the 21st century,"Paul Ammann, Jeff Offutt",1997/4/13,Conference Proceedings Tenth Conference on Software Engineering Education and Training,"Software engineering is a rapidly changing discipline, and will continue to be so for the foreseeable future. This pace of change brings both problems and opportunities to universities that teach software engineering. Engineers ore no longer satisfied with one or two initial university education experiences, but by necessity are becoming lifetime learners, with frequent trips back to educational providers. This recurring education is needed to update engineers' knowledge with new ideas and concepts, and to update engineers' skills. In this paper, we take the position that universities can and should respond to this situation with a new model for graduate software engineering education, which we call professional currency certificates. These courses should offer the depth of knowledge and university academic credit that traditional academic courses offer, but with the convenience and practical nature of corporate …",1
Applying data redundancy to differential equation solvers,"Paul Ammann, Dahlard L Lukes, John C Knight",1997/1,Journal Annals of Software Engineering,Data redundancy methods evaluate the output of a program on a given input by examining the outputs produced by the same program on additional inputs. This papers explores the use of data redundancy to detect and/or tolerate failures in differential equation solvers. Our first goal is to show that data redundancy techniques are applicable to a wide class of differential equations. Our second task is to identify circumstances in which an independence model of the sort used in program checking can be exploited to build highly reliable solvers from moderately reliable components. We conclude with illustrative examples of applying various data redundancy techniques to a standard differential equation solver. The method has potential for critical systems in which the application’s control laws are specified as sets of differential equations.,1
Data Redundancy for the Detection and Tolerance of Software Faults,PE Ammann,1992,"Conference Computing Science and Statistics: Statistics of Many Parameters: Curves, Images, Spatial Models","In crucial computer applications, such as avionics systems and automated life support systems, great confidence must be placed in the correct, safe, and reliable operation of the software. For a typical system, current development, analysis, and fault tolerance techniques cannot guarantee either the absence of software faults or adequate levels of confidence in proper operation. Such systems are nonetheless being built, however, and it is desirable to enlarge the set of techniques available for improving the software for critical applications. One such set is based upon the use of design redundancy to provide fault tolerance during operation and to aid testing during development The most general of these techniques, N-version programming and recovery blocks, have been the subjects of widespread study.",1
An experimental evaluation of error seeding as a program validation technique,"John C Knight, Paul E Ammann",1985/12/1,Journal NASA. Goddard Space Flight Center Proceedings of Tenth Annual Software Engineering Workshop,"A previously reported experiment in error seeding as a program validation technique is summarized. The experiment was designed to test the validity of three assumptions on which the alleged effectiveness of error seeding is based. Errors were seeded into 17 functionally identical but independently programmed Pascal programs in such a way as to produce 408 programs, each with one seeded error. Using mean time to failure as a metric, results indicated that it is possible to generate seeded errors that are arbitrarily but not equally difficult to locate. Examination of indigenous errors demonstrated that these are also arbitrarily difficult to locate. These two results support the assumption that seeded and indigenous errors are approximately equally difficult to locate. However, the assumption that, for each type of error, all errors are equally difficult to locate was not borne out. Finally, since a seeded error occasionally corrected an indigenous error, the assumption that errors do not interfere with each other was proven wrong. Error seeding can be made useful by taking these results into account in modifying the underlying model.",1
An experimental evaluation of simple methods for seeding program errors,"John C Knight, Paul E Ammann",1985/8/1,Book Proceedings of the 8th international conference on Software engineering,"This paper describes an experiment in which simple syntactic alterations were introduced into program text in order to evaluate the testing strategy known as error seeding. The experiment’s goal was to determine if randomly placed syntactic manipulations can produce failure characteristics similar to those of indigenous errors found within unseeded programs. As a result of a separate experiment, several programs were available, all of which were written to the same specifications and thus were intended to be functionally equivalent. The use of functionally equivalent programs allowed the influence of individual programmer styles to be removed as a variable from the error seeding experiment. Each of six different syntactic manipulations were introduced into each program and the mean times to failure for the seeded errors were observed. The seeded errors were found to have a broad spectrum of mean times to failure independent of the syntactic alteration used. We conclude that it is possible to seed errors using only simple syntactic techniques that are arbitrarily di5culty to locate. In addition, several unexpected results indicate that some issues involved in error seeding have not been addressed previously.",39
ON DEMAND: STEM Teaching with Active Learning: A Model Extracted from Software Engineering Classes (5 mins),"Kesina Baral, Jeff Offut, Paul Ammann",2020/7/31,Journal Innovations in Teaching & Learning Conference Proceedings,"Lecture-based class meetings don’t leave much time for interaction between the students and the instructor or collaboration among students to increase learning.“Active learning” is a broad term that encompasses teaching approaches where students talk, write, collaborate, move, etc. inside the classroom. We address the challenge of adapting an active learning approach to software engineering classes and propose that our adaptation might apply more broadly to other STEM classes. Our approach has five components: reading assignments, short videos, in-class exercises, homeworks, and quizzes. This sequence of components repeats for different topics throughout the course on a weekly basis. Reading assignments: Active classroom exercises are more effective if students prepare. Hence, students should be motivated to preview the topic before class. To facilitate this, we not only assign readings but also check for basic reading comprehension. Short videos: Students view short videos outside of class that introduce a topic, set up the problem, and discuss distractors en route to a correct solution. In-class exercises: Students apply the material they are learning through in-class exercises (the “active learning” part). Students work in small groups. We focus on having students explain their solutions to us (and the class) instead of simply telling them what they should do. Homework: Students practice materials learned in class through homework. We encourage collaboration among students. Quizzes: To assess learning, we quiz students on the topic they learned the previous week. We post quiz solutions on a discussion board (Piazza) so that …",
Guiding testing effort using mutant utility,"Justin Alvin, Bob Kurtz, Paul Ammann, Huzefa Rangwala, René Just",2019/5/25,Conference 2019 IEEE/ACM 41st International Conference on Software Engineering: Companion Proceedings (ICSE-Companion),"This paper addresses two long-standing goals in software testing: making mutation-based testing practical and software testing overall more effective, predictable, and consistent. To that end, this paper proposes a novel mutation-based approach that guides testing effort based on test goal utility.",
Guest Editorial for the Special Issue on Model‐Based Testing,"Paul Ammann, Gordon Fraser, Franz Wotawa",2012/8/1,Journal Software Testing Verification and Reliability,"Decades of research on model-based testing have spawned a large number of different techniques and tools to derive test cases from models. One particular advantage embraced by researchers is the fact that model-based testing makes it possible to reuse many of the techniques developed in the formal methods and verification domain. This is exemplified in three very different approaches in this second volume of the Special Issue on Model-Based Testing. The first paper,“Formal Passive Testing of Timed Systems: Theory and Tools” by Cesar Andres, Mercedes Merayo, and Manuel Nunez, concerns passive testing, in which a system is tested by either observing it while it is running or by inspecting traces after a sufficiently long run, which is in contrast to active testing where inputs are sent to the system under test. The paper describes a suite encompassing both formal methodologies, algorithms, and implemented tools for testing systems where timing aspects are central. The properties to be tested are specified as invariants, which can be checked against a specification, if one is available. The PASTE tool then checks these invariants against logs, and mutation analysis is used to estimate the quality of a set of invariants. The second paper,“Scenario-Based Testing using Symbolic Animation of B Models” by Frederic Dadeau, Kalou Cabrera Castillos, and Regis Tissot, considers active testing of systems based on a formal model specified in terms of B machines. This approach is based on scenarios specified in a specialized language, which are used to drive the symbolic execution of the models. The execution step is based on a constraint …",
A Decision-Guided Energy Management Framework for Sustainable Manufacturing,"Guodong Shao, Alexander Brodsky, Jorge Arinez, Daniel Menasce, Paul Ammann",2011/1/1,Conference International Design Engineering Technical Conferences and Computers and Information in Engineering Conference,"A growing number of manufacturing industries are initiating efforts to address sustainability issues. According to the National Association of Manufacturers, the manufacturing sector currently accounts for about one third of all energy consumed in the United States [1]. Reducing energy costs and pollution emissions involves many areas within an industrial facility. Peak electric demands are a significant component in the cost of electricity. Electric demand management relates to electric tariff rates, new power generation, and incentives to curtail peak usages. Shifting some equipment/machine usage to the periods of lower cost or using stand-by local generators during the peak demand period can yield important savings. Analysis of these options is important to decision makers to avoid unnecessary high cost of energy and equipment. This paper proposes a Decision-Guided energy management in manufacturing (DG …",
"1997 Ph. D., School of Information Technology, George Mason University, Fairfax, Virginia. 1991 Master of Computer Science and Engineering, Jadavpur University, Calcutta, India …","Ph D Advisors, Sushil Jajodia, Paul Ammann",2006/7,Journal Databases,,
Trusted Recovery from Information Attacks,"Sushil Jajodia, Paul Ammann, Peng Liu",2001/8/1,Publisher GEORGE MASON UNIV FAIRFAX VA,"Preventive measures sometimes fail to deflect malicious attacks. In this work, we adopt an information warfare perspective which assumes success by the attacker in achieving partial, but not complete damage. in particular, we work in the database context and consider recovery form malicious but committed transactions. Traditional recovery mechanisms do not address this problem, except for complete rollbacks, which undo the work of benign transactions as well as malicious ones, and compensating transactions, whose utility depends on application semantics. recovery is complicated by the presence of benign transactions that depend, directly or indirectly, on the malicious transactions. We present recovery models to restore only the damaged part of the database. Two families of new repair algorithms are developed one is a set of dependency-graph based algorithms, the other is a set of algorithms that do repair via rewriting histories.",
Using Formal Methods To Reason About Semantics-Based Decompositions Of Transactions,"Sushil Jajodia, Indrakshi Ray, Paul Ammann",1995,"Journal VLDB'95: Proceedings of the 21st International Conference on Very Large Data Bases, Zurich, Switzerland, Sept. 11-15, 1995","Many researchers have investigated the process of decomposing transactions into smaller pieces to increase concurrency. The research typically focuses on implementing a decomposition supplied by the database application developer, with relatively little attention. to what constitutes a desirable decomposition and how the developer should obtain such a decomposition. In this paper, we argue that the decomposition process itself warrants attention. A decomposition generates a set of proof obligations that must be satisfied to show that a particular decomposition correctly models the original collection of transactions. We introduce the notion of semantic histories to formulate and prove the necessary properties. Since the decomposition impacts not only the atomicity of transactions, but isolation and consistency as well, we present a technique based on formal methods that allows these properties to be surrendered in a carefully controlled manner.",
A Safety Kernel For Tra c Light Control,Paul Ammann,1994/7,"Description The success of kernels for enforcing security has led to proposals to use kernels for enforcing safety. This paper presents a feasibility demonstration of one particular proposal for a safety kernel via the application of tra c light control. The paper begins with the safety properties for tra c light control and speci es a kernel that maintains the safety properties. An implementation sketch of the kernel in Ada is given and use of the kernel is discussed. The contribution of the paper is a demonstration that a kernel is a feasible and desirable technique for software in a realistic, safety-critical application. The paper also illustrates how formal methods aid the software engineer in constructing and reasoning about such software.","The success of kernels for enforcing security has led to proposals to use kernels for enforcing safety. This paper presents a feasibility demonstration of one particular proposal for a safety kernel via the application of tra c light control. The paper begins with the safety properties for tra c light control and speci es a kernel that maintains the safety properties. An implementation sketch of the kernel in Ada is given and use of the kernel is discussed. The contribution of the paper is a demonstration that a kernel is a feasible and desirable technique for software in a realistic, safety-critical application. The paper also illustrates how formal methods aid the software engineer in constructing and reasoning about such software.",
The Effectiveness of Data Diversity as an Error Detection Mechanism,"John C Knight, Paul E Ammann, Steven Santos",1991,"Publisher Department of Computer Science, School of Engineering and Applied Science, University of Virginia",,
The Failure Characteristics of Unbiased Faults and Their Relation to Multi-version Software,"Paul E Ammann, John C Knight",1987,"Publisher Department of Computer Science, School of Engineering and Applied Science, University of Virginia",,
Data diversity: An approach to fault-tolerant software,"Paul E Ammann, John C Knight",1986/12/1,"Journal NASA. Goddard Space Flight Center, Proceedings of the Eleventh Annual Software Engineering Workshop",,
Program Committee Chairs,"Laurie Williams, Claes Wohlin, Andrea Arcuri, Paul Ammann, Anneliese Andrews, Giuliano Antoniol, Thomas Ball, Fevzi Belli, Tomas Berling, Kirill Bogdanov, Fabrice Bouquet, Tevfik Bultan, Cristian Cadar, Ana Cavalli, James Clause, Ian Craggs, Christoph Csallner, Marcio Eduardo Delamaro, Massimiliano Di Penta, Rachida Dssouli, Lydie Du Bousquet, Stephen Edwards, Sigrid Eldh, Robert Feldt, Phyllis Frankl, Gordon Fraser, Sudipto Ghosh, Arnaud Gotlieb, Jens Grabowski, Mark Harman, Robert Hierons, Florentin Ipate, Natalia Juristo, Gail Kaiser, Aditya Kanade, Johannes Kinder, Yu Lei","Program Committee Page 1 Program Committee Program Committee Chairs Laurie Williams, 
North Carolina State University, USA Claes Wohlin, Blekinge Institute of Technology, Sweden 
Program Committee Members Andrea Arcuri, Schlumberger & Simula Research Laboratory, 
Norway Paul Ammann, George Mason University, USA Anneliese Andrews, University of 
Denver, USA Giuliano Antoniol, Ecole Polytechnique de Montréal, Canada Thomas Ball, 
Microsoft Research, USA Fevzi Belli, University of Paderborn, Germany Tomas Berling, Saab 
AB, EDS, Sweden Kirill Bogdanov, University of Sheffield, UK Fabrice Bouquet, Université de 
Franche Comté, France Tevfik Bultan, University of California at Santa Barbara, USA Cristian 
Cadar, Imperial College London, UK Ana Cavalli, Institut National des Telecommunications, 
France James Clause, University of Delaware, USA Ian Craggs, IBM United Kingdom, UK …","Scholar articles Program Committee ChairsL Williams, C Wohlin, A Arcuri, P Ammann, A Andrews…",,
"Delamaro, Marcio Eduardo 304 Deng, Lin 412 De Oliveira Neto, Francisco Gomes 254 De Souza, Kathiani Elisa 284","Simone Do RS De Souza, Ana Almeida, Khaled Al-Sabbagh, Fabrice Ambert, Paul Ammann, Paulo Antunes, Francisco Araújo, Paolo Arcaini, Andrea Arcuri, Luca Ardito, Aitor Arrieta, Cyrille Artho, Thomas Auer, Naylor G Bachiega, Michael Baer, Shizuka Ban, Kesina Baral, Elodie Bernard, Andrea Bombarda, Josip Bozic, Claudinei Brito Junior, Sarita M Bruschi, Miroslav Bures, Alexandre Canny, Ján Čegiň, Suranjan Chakraborty, Nour Chetouane, Antoine Chevrot, Riccardo Coppola, Aymeric Cretin, Fabiano Cutigi Ferrari, Lucas Dallilo, Josh Dehlinger, Mike De Vries, Sahin Dirim, Felix Dobslaw, Natia Doliashvili, Alejandra Duque-Torres, Rafael S Durelli, Vinicius HS Durelli, Sigrid Eldh, Eduard Paul Enoiu, Isabel Evans, Peureux Fabien, Hermann Felbinger, Michael Felderer, Robert Feldt, Fabiano Ferrari, Ana Fidalgo, Mark Floryan, Konrad Fögen, Alexy Fouillade, Gordon Fraser, Dadeau Frédéric, Torben Friedrichs, Alessio Gambi, Angelo Gargantini, Ralf Gerlich",Presents an index of the authors whose articles are published in the conference proceedings record.,"Scholar articles Delamaro, Marcio Eduardo 304 Deng, Lin 412 De Oliveira Neto, Francisco Gomes 254 De Souza, Kathiani Elisa 284SDRS De Souza, A Almeida, K Al-Sabbagh, F Ambert…",Presents an index of the authors whose articles are published in the conference proceedings record.,
Message from MUTATION 2020 Chairs,"Jie M Zhang, Mike Papadakis, Jinhan Kim, Lin Deng, Birgitta Lindström, Sina Shamshiri, José Miguel Rojas, Sudipto Ghosh, Goran Petrovic, Alex Denisov, Giovani Guizzo, Donghwan Shin, Thierry Titcheu Chekam, Qianqian Zhu, Andy Zaidman, Gordon Fraser, Mark Harman, Lingming Zhang, René Just, Marinos Kintis Othera, Paul Ammann","It is our pleasure to welcome you to the 15th edition of the International Workshop on Mutation Analysis (MUTATION 2020), collocated with the 13th IEEE International Conference on Software Testing, Verification and Validation (ICST 2020) in Porto, Portugal. Since its first edition, the MUTATION workshop has provided a forum to bring together researchers and practitioners, enabling them to exchange ideas, address fundamental challenges in mutation analysis and testing and discuss new applications of mutation. This year, MUTATION features a keynote from Dr. Mike Papadakis from the Interdisciplinary Centre for Security, Reliability and Trust (SnT) Luxembourg University. It is also our pleasure to accept seven high quality full papers, after extensive reviews and discussion among the program committee members.","Scholar articles Message from MUTATION 2020 ChairsJM Zhang, M Papadakis, J Kim, L Deng, B Lindström…All 2 versions ","It is our pleasure to welcome you to the 15th edition of the International Workshop on Mutation Analysis (MUTATION 2020), collocated with the 13th IEEE International Conference on Software Testing, Verification and Validation (ICST 2020) in Porto, Portugal. Since its first edition, the MUTATION workshop has provided a forum to bring together researchers and practitioners, enabling them to exchange ideas, address fundamental challenges in mutation analysis and testing and discuss new applications of mutation. This year, MUTATION features a keynote from Dr. Mike Papadakis from the Interdisciplinary Centre for Security, Reliability and Trust (SnT) Luxembourg University. It is also our pleasure to accept seven high quality full papers, after extensive reviews and discussion among the program committee members.",
A Distributed Implementation of the,"Paul Ammann, Ravi S Sandhu, Gurpreet S Suri","Protection models provide a formalism for specifying control over access to information and other resources in a multi-user computer system. One such model, the Eztended Schematic Protection Model (ESPM), has ezpressive power equivalent to the monotonic access matriz model of Harrison, Ruzzo, and Ullman/7). Yet ESPM retains tractable safety analy-sis for mainy cases of practical interest. Thus ESPM is a very general model, and it is of interest whether ESPM can be implemented in a reasonable manner. In this paper, we outline a distributed implementation for ESPM. Our implementation is capability-based, with an architecture where servers act as mediators to all subject and object access. Capabilities are made nontransferable by burying the identity of subjects in them, and unforgeable by using a public key encryption algorithm. Timestamps and public keys are used as mechanisms for revocation.","Scholar articles A Distributed Implementation of theP Ammann, RS Sandhu, GS SuriRelated articles All 2 versions ","Protection models provide a formalism for specifying control over access to information and other resources in a multi-user computer system. One such model, the Eztended Schematic Protection Model (ESPM), has ezpressive power equivalent to the monotonic access matriz model of Harrison, Ruzzo, and Ullman/7). Yet ESPM retains tractable safety analy-sis for mainy cases of practical interest. Thus ESPM is a very general model, and it is of interest whether ESPM can be implemented in a reasonable manner. In this paper, we outline a distributed implementation for ESPM. Our implementation is capability-based, with an architecture where servers act as mediators to all subject and object access. Capabilities are made nontransferable by burying the identity of subjects in them, and unforgeable by using a public key encryption algorithm. Timestamps and public keys are used as mechanisms for revocation.",
"Department of Information and Software Systems Engineering George Mason University Fairfax, VA 22030","Paul E Ammann, Ravi S Sandhu","Access control models provide a formalism and framework for specifying control over access to information and other resources in multi-user computer systems. Useful access control models must balance expressive power with the decidability and com-plexity of safety analysis (ie the determination of whether or not a given subject can ever acquire access to a given object). The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has very broad expressive power. Unfortunately, HRU also has extremely weak safety properties. Safety is undecidable for most policies of practical interest, even in the monotonic version of HRU (which only allows revocation which is itself reversible). Remarkably, an alternate formulation of monotonic HRU yields strong safety properties. This alternate formulation is called the Extended Schematic Protection Model (ESPM). ESPM is derived from the Schematic …","Scholar articles Department of Information and Software Systems Engineering George Mason University Fairfax, VA 22030PE Ammann, RS SandhuRelated articles All 2 versions ","Access control models provide a formalism and framework for specifying control over access to information and other resources in multi-user computer systems. Useful access control models must balance expressive power with the decidability and com-plexity of safety analysis (ie the determination of whether or not a given subject can ever acquire access to a given object). The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has very broad expressive power. Unfortunately, HRU also has extremely weak safety properties. Safety is undecidable for most policies of practical interest, even in the monotonic version of HRU (which only allows revocation which is itself reversible). Remarkably, an alternate formulation of monotonic HRU yields strong safety properties. This alternate formulation is called the Extended Schematic Protection Model (ESPM). ESPM is derived from the Schematic …",
Using Model Checking to Generate,Paul E Ammann,"We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker gener-ates counterexamples which distinguish the variations from the original specification. The counterexamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case gener-ation is automatic; each counterexample is a complete test case. Second, in sharp contrast to program-based mutation analysis, equivalent mutant identification is also automatic. We apply our method to an example specification and evaluate the resulting test sets with coverage metrics on a Java implementation.",Scholar articles Using Model Checking to GeneratePE AmmannRelated articles All 3 versions ,"We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker gener-ates counterexamples which distinguish the variations from the original specification. The counterexamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case gener-ation is automatic; each counterexample is a complete test case. Second, in sharp contrast to program-based mutation analysis, equivalent mutant identification is also automatic. We apply our method to an example specification and evaluate the resulting test sets with coverage metrics on a Java implementation.",
Mutation 2018 Committees,"Marinos Kintis, Nan Li, José Miguel Rojas, Birgitta Lindström, Goran Petrovic, Gordon Fraser, Gregory Kapfhammer, Jens Krinke, Jie Zhang, Lech Madeyski, Lin Deng, Mike Papadakis, Milos Gligoric, Nicos Malevris, Paul Ammann, Rahul Gopinath, Rene Just, Sina Shamshiri, Sudipto Ghosh, Vinicius Durelli, Yue Jia, Yves Le Traon, Thierry Titcheu Chekam","Mutation 2018 Committees Toggle navigation IEEE Computer Society Digital Library Jobs Tech 
News Resource Center Press Room Browse By Date Advertising About Us IEEE IEEE 
Computer Society IEEE Computer Society Digital Library My Subscriptions Magazines Journals 
Conference Proceedings Institutional Subscriptions IEEE IEEE Computer Society More Jobs 
Tech News Resource Center Press Room Browse By Date Advertising About Us Cart All 
Advanced Search Conference Cover Image Download 1.Home 2.Proceedings 3.icstw 2018 
Mutation 2018 Committees 2018, pp. 14-14, DOI Bookmark: 10.1109/ICSTW.2018.00007 
Keywords Authors Abstract Provides a listing of current committee members and society officers. 
Mutation 2018 Committees , ,Program Chairs , ,Marinos Kintis, ,University of Luxembourg, ,Nan 
Li, ,Medidata Solutions, ,José Miguel Rojas, ,University of Leicester, , ,Program Committee , …","Scholar articles Mutation 2018 CommitteesM Kintis, N Li, JM Rojas, B Lindström, G Petrovic…All 2 versions ",,
sing Model Checking to Generate Tests Arom SpeciHcations Paul E. Ammann Paul E. Black William Majurski George Mason University National Institute of Standards and Technology …,Paul E Ammann,"We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker generates counterexamples which distinguish the variations from the original specification. The counterexamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case generation is automatic; each counterexample is a complete test case. Second, in sharp contrast to program-based mutation analysis, equivalent mutant identification is also automatic. We apply our method to an example specification and evaluate the resulting test sets with coverage metrics on a Java implementation.","Scholar articles sing Model Checking to Generate Tests Arom SpeciHcations Paul E. Ammann Paul E. Black William Majurski George Mason University National Institute of Standards and Technology Information & Software Eng. Dept. North Building, Room 562PE Ammann","We apply a model checker to the problem of test generation using a new application of mutation analysis. We define syntactic operators, each of which produces a slight variation on a given model. The operators define a form of mutation analysis at the level of the model checker specification. A model checker generates counterexamples which distinguish the variations from the original specification. The counterexamples can easily be turned into complete test cases, that is, with inputs and expected results. We define two classes of operators: those that produce test cases from which a correct implementation must differ, and those that produce test cases with which it must agree. There are substantial advantages to combining a model checker with mutation analysis. First, test case generation is automatic; each counterexample is a complete test case. Second, in sharp contrast to program-based mutation analysis, equivalent mutant identification is also automatic. We apply our method to an example specification and evaluate the resulting test sets with coverage metrics on a Java implementation.",
"Introduction to Software Testing Chapter 2.1, 2.2 Overview Graph Coverage Criteria","Paul Ammann, Jeff Offutt","• Satisfaction: Given a set TR of test requirements for a criterion C, a set of tests T satisfies C on a graph if and only if for every test requirement in TR, there is a test path in path (T) that meets the test requirement tr","Scholar articles Introduction to Software Testing Chapter 2.1, 2.2 Overview Graph Coverage CriteriaP Ammann, J OffuttAll 3 versions ","• Satisfaction: Given a set TR of test requirements for a criterion C, a set of tests T satisfies C on a graph if and only if for every test requirement in TR, there is a test path in path (T) that meets the test requirement tr",
Message from the Chairs–MUTATION 2016,"René Just, Jens Krinke, Christopher Henard, Paul Ammann, Gordon Fraser, Sudipto Ghosh, Milos Gligoric, Rahul Gopinath, Yue Jia, Gregory Kapfhammer",Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,"Scholar articles Message from the Chairs–MUTATION 2016R Just, J Krinke, C Henard, P Ammann, G Fraser…",Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,
INDRAKSHI RAY,,,,,
Message from the Chairs–MUTATION 2015,"Mike Papadakis, René Just, Paul Ammann, Leonardo Bottaci, Jeremy Bradbury, Byoungju Choi, Gordon Fraser, Milos Gligoric, Mark Harman, Rob Hierons, Bill Howden, Yue Jia, Gregory Kapfhammer",Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,"Scholar articles Message from the Chairs–MUTATION 2015M Papadakis, R Just, P Ammann, L Bottaci, J Bradbury…All 2 versions ",Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,
Rewriting Histories: Recovering from Malicious,"Peng Liu, Paul Ammann, Sushil Jajodia","We consider recovery from malicious but committed transactions. Traditional recovery mechanisms do not address this problem, except for complete rollbacks, which undo the work of good transactions as well as malicious ones, and compensating transactions, whose utility depends on application semantics. We develop an algorithm that rewrites execution histories for the purpose of backing out malicious transactions. Good transactions that are affected, directly or indirectly, by malicious transactions complicate the process of backing out undesirable transactions. We show that the prefix of a rewritten history produced by the algorithm serializes exactly the set of unaffected good transactions. The suffix of the rewritten history includes special state information to describe affected good transactions as well as malicious transactions. We describe techniques that can extract additional good transactions from this latter …","Scholar articles Rewriting Histories: Recovering from MaliciousP Liu, P Ammann, S JajodiaRelated articles ","We consider recovery from malicious but committed transactions. Traditional recovery mechanisms do not address this problem, except for complete rollbacks, which undo the work of good transactions as well as malicious ones, and compensating transactions, whose utility depends on application semantics. We develop an algorithm that rewrites execution histories for the purpose of backing out malicious transactions. Good transactions that are affected, directly or indirectly, by malicious transactions complicate the process of backing out undesirable transactions. We show that the prefix of a rewritten history produced by the algorithm serializes exactly the set of unaffected good transactions. The suffix of the rewritten history includes special state information to describe affected good transactions as well as malicious transactions. We describe techniques that can extract additional good transactions from this latter …",
