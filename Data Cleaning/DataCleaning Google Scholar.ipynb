{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading python require packages\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import display\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use to remove stop words from sentences\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#apply lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "#apply stemming on words\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to clean documents\n",
    "def cleanData(doc):\n",
    "    tokens = doc.split() #split document into words\n",
    "    table = str.maketrans('', '', punctuation) #remove punctuation\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    tokens = [word for word in tokens if word.isalpha()] #check only for alphabets\n",
    "    tokens = [w for w in tokens if not w in stop_words] #remove stop words \n",
    "    tokens = [word for word in tokens if len(word) > 1] #apply on words whose length > 1\n",
    "    tokens = [ps.stem(token) for token in tokens] #apply stemming on words\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] #apply lemmatization on words\n",
    "    tokens = ' '.join(tokens) #join all words as single document\n",
    "    return tokens #return cleaned document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\munna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  \n",
      "c:\\users\\munna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:9: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if __name__ == \"__main__\":\n",
      "c:\\users\\munna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "c:\\users\\munna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:11: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "c:\\users\\munna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:12: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  if sys.path[0] == \"\":\n",
      "c:\\users\\munna\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:13: FutureWarning: get_value is deprecated and will be removed in a future release. Please use .at[] or .iat[] accessors instead\n",
      "  del sys.path[0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Document Name : Abolfazl Safikhani.csv (44, 6) 44\n",
      "Cleaned Document Name : Aditya Johri.csv (209, 6) 253\n",
      "Cleaned Document Name : Ahmed Bin Zaman.csv (13, 6) 266\n",
      "Cleaned Document Name : Alexander Brodsky.csv (199, 6) 465\n",
      "Cleaned Document Name : Alexander Levis.csv (376, 6) 841\n",
      "Cleaned Document Name : Ali Beheshti.csv (41, 6) 882\n",
      "Cleaned Document Name : Amarda Shehu.csv (253, 6) 1135\n",
      "Cleaned Document Name : Ariela Sofer.csv (52, 6) 1187\n",
      "Cleaned Document Name : Ben Seiyon Lee.csv (17, 6) 1204\n",
      "Cleaned Document Name : Bernd-Peter Paris.csv (48, 6) 1252\n",
      "Cleaned Document Name : Bijan Jabbari.csv (227, 6) 1479\n",
      "Cleaned Document Name : Bo Han.csv (230, 6) 1709\n",
      "Cleaned Document Name : Brian L. Mark.csv (183, 6) 1892\n",
      "Cleaned Document Name : Burak Tanyu.csv (82, 6) 1974\n",
      "Cleaned Document Name : Cameron Nowzari.csv (65, 6) 2039\n",
      "Cleaned Document Name : Carlotta Domeniconi.csv (239, 6) 2278\n",
      "Cleaned Document Name : Caroline D Hoemann.csv (140, 6) 2418\n",
      "Cleaned Document Name : Colin Reagle.csv (10, 6) 2428\n",
      "Cleaned Document Name : Daigo Shishika.csv (37, 6) 2465\n",
      "Cleaned Document Name : Doaa Bondok.csv (9, 6) 2474\n",
      "Cleaned Document Name : Duminda Wijesekera.csv (352, 6) 2826\n",
      "Cleaned Document Name : Edward Huang.csv (78, 6) 2904\n",
      "Cleaned Document Name : Elise Miller-Hooks.csv (143, 6) 3047\n",
      "Cleaned Document Name : Emanuela Marasco.csv (50, 6) 3097\n",
      "Cleaned Document Name : Eric Osterweil.csv (113, 6) 3210\n",
      "Cleaned Document Name : Eugene Kim.csv (5, 6) 3215\n",
      "Cleaned Document Name : Evgenios M. Kornaropoulos.csv (23, 6) 3238\n",
      "Cleaned Document Name : Fei Li.csv (67, 6) 3305\n",
      "Cleaned Document Name : Foteini Baldimtsi.csv (51, 6) 3356\n",
      "Cleaned Document Name : George Hazelrigg.csv (116, 6) 3472\n",
      "Cleaned Document Name : Gheorghe Tecuci.csv (209, 6) 3681\n",
      "Cleaned Document Name : Giorgio A. Ascoli.csv (308, 6) 3989\n",
      "Cleaned Document Name : Giuseppe Ateniese.csv (157, 6) 4146\n",
      "Cleaned Document Name : Hadi El-Amine.csv (17, 6) 4163\n",
      "Cleaned Document Name : Hakan Aydin.csv (94, 6) 4257\n",
      "Cleaned Document Name : Harry Foxwell.csv (7, 6) 4264\n",
      "Cleaned Document Name : Hemant Purohit.csv (113, 6) 4377\n",
      "Cleaned Document Name : Holger Dannenberg.csv (16, 6) 4393\n",
      "Cleaned Document Name : Irina Hashmi.csv (20, 6) 4413\n",
      "Cleaned Document Name : Jan Allbeck.csv (101, 6) 4514\n",
      "Cleaned Document Name : Janis Terpenny.csv (203, 6) 4717\n",
      "Cleaned Document Name : Jeff Offutt.csv (360, 6) 5077\n",
      "Cleaned Document Name : Jeffrey Moran.csv (46, 6) 5123\n",
      "Cleaned Document Name : Jens-Peter Kaps.csv (83, 6) 5206\n",
      "Cleaned Document Name : Jessica Lin.csv (124, 6) 5330\n",
      "Cleaned Document Name : Jianli Pan.csv (73, 6) 5403\n",
      "Cleaned Document Name : Jie Xu.csv (76, 6) 5479\n",
      "Cleaned Document Name : Jim Chen.csv (940, 6) 6419\n",
      "Cleaned Document Name : Jinwei Ye.csv (50, 6) 6469\n",
      "Cleaned Document Name : John Shortle.csv (88, 6) 6557\n",
      "Cleaned Document Name : Juan Cebral.csv (291, 6) 6848\n",
      "Cleaned Document Name : Kai Zeng.csv (160, 6) 7008\n",
      "Cleaned Document Name : Karla Hoffman.csv (108, 6) 7116\n",
      "Cleaned Document Name : Katherine (Raven) Russell.csv (10, 6) 7126\n",
      "Cleaned Document Name : Kenneth S. Ball.csv (115, 6) 7241\n",
      "Cleaned Document Name : Kevin Andrea.csv (7, 6) 7248\n",
      "Cleaned Document Name : Kevin Lybarger.csv (26, 6) 7274\n",
      "Cleaned Document Name : Kevin Moran.csv (49, 6) 7323\n",
      "Cleaned Document Name : Khaled N. Khasawneh.csv (39, 6) 7362\n",
      "Cleaned Document Name : Khondkar Islam.csv (26, 6) 7388\n",
      "Cleaned Document Name : Kim T. Blackwell.csv (152, 6) 7540\n",
      "Cleaned Document Name : Kirin Emlet Furst.csv (11, 6) 7551\n",
      "Cleaned Document Name : Kun Sun.csv (142, 6) 7693\n",
      "Cleaned Document Name : Kuo Tian.csv (27, 6) 7720\n",
      "Cleaned Document Name : Lance Sherry.csv (296, 6) 8016\n",
      "Cleaned Document Name : Lei Yang.csv (61, 6) 8077\n",
      "Cleaned Document Name : Lily Wang.csv (52, 6) 8129\n",
      "Cleaned Document Name : Linghan Zhang.csv (13, 6) 8142\n",
      "Cleaned Document Name : Lishan Yang.csv (15, 6) 8157\n",
      "Cleaned Document Name : Liza Wilson Durant.csv (16, 6) 8173\n",
      "Cleaned Document Name : Lucas RF Henneman.csv (47, 6) 8220\n",
      "Cleaned Document Name : Mahdi Hashemi.csv (40, 6) 8260\n",
      "Cleaned Document Name : Marcos Zampieri.csv (153, 6) 8413\n",
      "Cleaned Document Name : Mark Snyder.csv (9, 6) 8422\n",
      "Cleaned Document Name : Massimiliano Albanese.csv (126, 6) 8548\n",
      "Cleaned Document Name : Md Tanvir Arafin.csv (37, 6) 8585\n",
      "Cleaned Document Name : Michael Eagle.csv (60, 6) 8645\n",
      "Cleaned Document Name : Michael Lyons.csv (3, 6) 8648\n",
      "Cleaned Document Name : Mihai Boicu.csv (132, 6) 8780\n",
      "Cleaned Document Name : Mingkui Wei.csv (26, 6) 8806\n",
      "Cleaned Document Name : Mingrui Liu.csv (37, 6) 8843\n",
      "Cleaned Document Name : Mohan Venigalla.csv (121, 6) 8964\n",
      "Cleaned Document Name : Monson Hayes.csv (264, 6) 9228\n",
      "Cleaned Document Name : Myeong Lee.csv (33, 6) 9261\n",
      "Cleaned Document Name : Ningshi Yao.csv (15, 6) 9276\n",
      "Cleaned Document Name : Omoche Cheche Agada.csv (2, 6) 9278\n",
      "Cleaned Document Name : Parag V. Chitnis.csv (123, 6) 9401\n",
      "Cleaned Document Name : Parth Pathak.csv (76, 6) 9477\n",
      "Cleaned Document Name : Paul Ammann.csv (148, 6) 9625\n",
      "Cleaned Document Name : Pouyan Ahmadi.csv (18, 6) 9643\n",
      "Cleaned Document Name : Pramita Bagchi.csv (24, 6) 9667\n",
      "Cleaned Document Name : Qi Wei.csv (64, 6) 9731\n",
      "Cleaned Document Name : Qiang Zeng.csv (55, 6) 9786\n",
      "Cleaned Document Name : Qiliang Li.csv (204, 6) 9990\n",
      "Cleaned Document Name : Rajesh Ganesan.csv (86, 6) 10076\n",
      "Cleaned Document Name : Ran Ji.csv (15, 6) 10091\n",
      "Cleaned Document Name : Remi Veneziano.csv (56, 6) 10147\n",
      "Cleaned Document Name : Robert Handler.csv (161, 6) 10308\n",
      "Cleaned Document Name : Sadegh Torabi.csv (22, 6) 10330\n",
      "Cleaned Document Name : Sanjeev Setia.csv (78, 6) 10408\n",
      "Cleaned Document Name : Sanmay Das.csv (103, 6) 10511\n",
      "Cleaned Document Name : Shani E Ross.csv (15, 6) 10526\n",
      "Cleaned Document Name : Sharmin Sultana.csv (20, 6) 10546\n",
      "Cleaned Document Name : Sherif Hashem.csv (50, 6) 10596\n",
      "Cleaned Document Name : Shuochao Yao.csv (81, 6) 10677\n",
      "Cleaned Document Name : Siddhartha Sikdar.csv (180, 6) 10857\n",
      "Cleaned Document Name : Songqing Chen.csv (202, 6) 11059\n",
      "Cleaned Document Name : Thomas D. LaToza.csv (100, 6) 11159\n",
      "Cleaned Document Name : Tianshu Feng.csv (10, 6) 11169\n",
      "Cleaned Document Name : Tolga Soyata.csv (104, 6) 11273\n",
      "Cleaned Document Name : Vadim Sokolov.csv (65, 6) 11338\n",
      "Cleaned Document Name : Vasiliki Ikonomidou.csv (89, 6) 11427\n",
      "Cleaned Document Name : Vijay K. Shah.csv (37, 6) 11464\n",
      "Cleaned Document Name : Vivian Genaro Motti.csv (132, 6) 11596\n",
      "Cleaned Document Name : Viviana Maggioni.csv (185, 6) 11781\n",
      "Cleaned Document Name : Weiwen Jiang.csv (106, 6) 11887\n",
      "Cleaned Document Name : Xiang 'Shawn' Chen.csv (82, 6) 11969\n",
      "Cleaned Document Name : Xiaokuan Zhang.csv (17, 6) 11986\n",
      "Cleaned Document Name : Xuan Wang.csv (23, 6) 12009\n",
      "Cleaned Document Name : Xuesu Xiao.csv (81, 6) 12090\n",
      "Cleaned Document Name : Yariv Ephraim.csv (101, 6) 12191\n",
      "Cleaned Document Name : Yotam Gingold.csv (58, 6) 12249\n",
      "Cleaned Document Name : Zhisheng Yan.csv (45, 6) 12294\n",
      "Cleaned Document Name : Ziwei Zhu.csv (26, 6) 12320\n",
      "Cleaned Document Name : Ziyu Yao.csv (17, 6) 12337\n",
      "\n",
      "Cleaned Document\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>authors</th>\n",
       "      <th>date</th>\n",
       "      <th>source</th>\n",
       "      <th>descriptions</th>\n",
       "      <th>citations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>joint structur break detect paramet estim high...</td>\n",
       "      <td>Abolfazl Safikhani</td>\n",
       "      <td>1/2/2022</td>\n",
       "      <td>journal journal american statist associ</td>\n",
       "      <td>assum stationar unrealist mani time seri appli...</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spatiotempor model yellow taxi demand new york...</td>\n",
       "      <td>Abolfazl Safikhani</td>\n",
       "      <td>7/1/2020</td>\n",
       "      <td>journal intern journal forecast</td>\n",
       "      <td>the spatiotempor variat demand transport parti...</td>\n",
       "      <td>37.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>investig rang anxieti safeti buffer batteri el...</td>\n",
       "      <td>Abolfazl Safikhani</td>\n",
       "      <td>6/13/2018</td>\n",
       "      <td>journal journal advanc transport</td>\n",
       "      <td>driver tend rang anxieti compar drive tradit f...</td>\n",
       "      <td>34.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cyclelength predict actuat trafficsign control...</td>\n",
       "      <td>Abolfazl Safikhani</td>\n",
       "      <td>3/1/2018</td>\n",
       "      <td>journal journal comput civil engin</td>\n",
       "      <td>In urban transport system traffic signal main ...</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>predict shortterm uber demand new york citi us...</td>\n",
       "      <td>Abolfazl Safikhani</td>\n",
       "      <td>5/1/2019</td>\n",
       "      <td>journal journal comput civil engin</td>\n",
       "      <td>the demand ehail servic grow rapidli especi la...</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12332</th>\n",
       "      <td>proceed workshop structur unstructur knowledg ...</td>\n",
       "      <td>Ziyu Yao</td>\n",
       "      <td>2022/7</td>\n",
       "      <td>confer proceed workshop structur unstructur kn...</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12333</th>\n",
       "      <td>code edit few exemplar adapt multiext composit</td>\n",
       "      <td>Ziyu Yao</td>\n",
       "      <td>3/4/2022</td>\n",
       "      <td>confer deep learn code workshop</td>\n",
       "      <td>thi paper consid comput sourc code edit exempl...</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12334</th>\n",
       "      <td>proceed workshop natur languag process program</td>\n",
       "      <td>Ziyu Yao</td>\n",
       "      <td>2021/8</td>\n",
       "      <td>confer proceed workshop natur languag process ...</td>\n",
       "      <td>the prolifer programmingrel platform github st...</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12335</th>\n",
       "      <td>On advanc natur languag interfac data collect ...</td>\n",
       "      <td>Ziyu Yao</td>\n",
       "      <td>2021</td>\n",
       "      <td>institut the ohio state univers</td>\n",
       "      <td>natur languag provid univers effici way human ...</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12336</th>\n",
       "      <td>iec toward interestelicit neural convers agent</td>\n",
       "      <td>Ziyu Yao</td>\n",
       "      <td>2019</td>\n",
       "      <td>descript convers agent need suffici social ski...</td>\n",
       "      <td>convers agent need suffici social skill establ...</td>\n",
       "      <td>N/A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12337 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  titles             authors  \\\n",
       "0      joint structur break detect paramet estim high...  Abolfazl Safikhani   \n",
       "1      spatiotempor model yellow taxi demand new york...  Abolfazl Safikhani   \n",
       "2      investig rang anxieti safeti buffer batteri el...  Abolfazl Safikhani   \n",
       "3      cyclelength predict actuat trafficsign control...  Abolfazl Safikhani   \n",
       "4      predict shortterm uber demand new york citi us...  Abolfazl Safikhani   \n",
       "...                                                  ...                 ...   \n",
       "12332  proceed workshop structur unstructur knowledg ...            Ziyu Yao   \n",
       "12333     code edit few exemplar adapt multiext composit            Ziyu Yao   \n",
       "12334     proceed workshop natur languag process program            Ziyu Yao   \n",
       "12335  On advanc natur languag interfac data collect ...            Ziyu Yao   \n",
       "12336     iec toward interestelicit neural convers agent            Ziyu Yao   \n",
       "\n",
       "            date                                             source  \\\n",
       "0       1/2/2022            journal journal american statist associ   \n",
       "1       7/1/2020                    journal intern journal forecast   \n",
       "2      6/13/2018                   journal journal advanc transport   \n",
       "3       3/1/2018                 journal journal comput civil engin   \n",
       "4       5/1/2019                 journal journal comput civil engin   \n",
       "...          ...                                                ...   \n",
       "12332     2022/7  confer proceed workshop structur unstructur kn...   \n",
       "12333   3/4/2022                    confer deep learn code workshop   \n",
       "12334     2021/8  confer proceed workshop natur languag process ...   \n",
       "12335       2021                    institut the ohio state univers   \n",
       "12336       2019  descript convers agent need suffici social ski...   \n",
       "\n",
       "                                            descriptions citations  \n",
       "0      assum stationar unrealist mani time seri appli...      50.0  \n",
       "1      the spatiotempor variat demand transport parti...      37.0  \n",
       "2      driver tend rang anxieti compar drive tradit f...      34.0  \n",
       "3      In urban transport system traffic signal main ...      32.0  \n",
       "4      the demand ehail servic grow rapidli especi la...      17.0  \n",
       "...                                                  ...       ...  \n",
       "12332                                                N/A       N/A  \n",
       "12333  thi paper consid comput sourc code edit exempl...       N/A  \n",
       "12334  the prolifer programmingrel platform github st...       N/A  \n",
       "12335  natur languag provid univers effici way human ...       N/A  \n",
       "12336  convers agent need suffici social skill establ...       N/A  \n",
       "\n",
       "[12337 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = 'GoogleScholar' #here is the location\n",
    "clean = []\n",
    "for root, dirs, directory in os.walk(path):#load all documents from GoogleScholar folder\n",
    "    for j in range(len(directory)):\n",
    "        document = pd.read_csv(root+\"/\"+directory[j]) #Read contents from each document \n",
    "        document.fillna(\"N/A\", inplace = True)\n",
    "        for i in range(len(document)): #read all rows from dataset\n",
    "            titles = str(document.get_value(i, 'titles')) #read each column\n",
    "            authors = str(document.get_value(i, 'authors'))\n",
    "            date = str(document.get_value(i, 'date'))\n",
    "            source = str(document.get_value(i, 'source'))\n",
    "            desc = str(document.get_value(i, 'descriptions'))\n",
    "            citations = str(document.get_value(i, 'citations'))\n",
    "            if len(titles) > 10:\n",
    "                titles = cleanData(titles) #clean the titles\n",
    "            if len(source) > 10:    \n",
    "                source = cleanData(source) #clean source\n",
    "            if len(desc) > 10:\n",
    "                desc = cleanData(desc)\n",
    "            clean.append([titles, authors, date, source, desc, citations]) #add clean data to array    \n",
    "        print(\"Cleaned Document Name : \"+directory[j]+\" \"+str(document.shape)+\" \"+str(len(clean)))          \n",
    "#create data frame with all cleaned documents    \n",
    "df = pd.DataFrame(clean, columns = ['titles','authors','date','source','descriptions','citations']) \n",
    "#save and display cleaned document\n",
    "df.to_csv(\"Cleaned.csv\", index=False)   \n",
    "print()\n",
    "print(\"Cleaned Document\")\n",
    "print()\n",
    "display(df)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
